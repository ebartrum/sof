{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT embedding] variable.\n",
      "[INIT renderer] FC, with renderer = FC\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SRNsModel:\n\tUnexpected key(s) in state_dict: \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.0.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.0.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.1.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.1.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.0.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.0.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.1.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.1.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.2.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.2.bias\", \"ray_marcher.out_layer.weight\", \"ray_marcher.out_layer.bias\", \"ray_marcher.lstm.weight_ih\", \"ray_marcher.lstm.weight_hh\", \"ray_marcher.lstm.bias_ih\", \"ray_marcher.lstm.bias_hh\", \"pixel_generator.net.4.net.0.weight\", \"pixel_generator.net.4.net.0.bias\", \"pixel_generator.net.4.net.1.weight\", \"pixel_generator.net.4.net.1.bias\", \"pixel_generator.net.5.weight\", \"pixel_generator.net.5.bias\", \"pixel_generator.net.3.net.0.weight\", \"pixel_generator.net.3.net.0.bias\", \"pixel_generator.net.3.net.1.weight\", \"pixel_generator.net.3.net.1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b2e7b3ee5ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n\u001b[0;32m---> 39\u001b[0;31m                  overwrite_embeddings=False, overwrite_cam=True)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SRNs/util.py\u001b[0m in \u001b[0;36mcustom_load\u001b[0;34m(model, path, discriminator, overwrite_embeddings, overwrite_renderer, overwrite_cam, optimizer)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/srns/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SRNsModel:\n\tUnexpected key(s) in state_dict: \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.0.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.0.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.1.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.0.net.1.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.0.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.0.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.1.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.1.net.1.bias\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.2.weight\", \"hyper_phi.layers.3.hyper_linear.hypo_params.net.2.bias\", \"ray_marcher.out_layer.weight\", \"ray_marcher.out_layer.bias\", \"ray_marcher.lstm.weight_ih\", \"ray_marcher.lstm.weight_hh\", \"ray_marcher.lstm.bias_ih\", \"ray_marcher.lstm.bias_hh\", \"pixel_generator.net.4.net.0.weight\", \"pixel_generator.net.4.net.0.bias\", \"pixel_generator.net.4.net.1.weight\", \"pixel_generator.net.4.net.1.bias\", \"pixel_generator.net.5.weight\", \"pixel_generator.net.5.bias\", \"pixel_generator.net.3.net.0.weight\", \"pixel_generator.net.3.net.0.bias\", \"pixel_generator.net.3.net.1.weight\", \"pixel_generator.net.3.net.1.bias\". "
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from modeling import SRNsModel\n",
    "import util\n",
    "\n",
    "_RENDERER = 'FC'\n",
    "_ORTHO = False\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn import manifold\n",
    "\n",
    "_MODEL_PATH = '/home/anpei/liury/log/SRNs/072515face_seg_real/checkpoints/epoch_0073_iter_080000.pth'\n",
    "_OPT_CAM = False\n",
    "_ORTHO = True\n",
    "_TOT_NUM_INSTANCES = 1689\n",
    "\n",
    "_IMG_SIZE = 128\n",
    "_OUT_SIZE = 512\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "model = SRNsModel(num_instances=_TOT_NUM_INSTANCES,\n",
    "                  latent_dim=256,\n",
    "                  renderer=_RENDERER,\n",
    "                  tracing_steps=10,\n",
    "                  freeze_networks=True,\n",
    "                  out_channels=20,\n",
    "                  img_sidelength=_IMG_SIZE,\n",
    "                  output_sidelength=_OUT_SIZE,\n",
    "                  opt_cam=_OPT_CAM,\n",
    "                  orthogonal=_ORTHO)\n",
    "\n",
    "util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n",
    "                 overwrite_embeddings=False, overwrite_cam=True)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\n",
    "\n",
    "_CMAP = np.asarray([[0, 0, 0], [127, 212, 255], [255, 255, 127], [255, 255, 127], # 'background','skin', 'l_brow', 'r_brow'\n",
    "                    [255, 255, 170], [255, 255, 170], [240, 157, 240], [255, 212, 255], #'l_eye', 'r_eye', 'r_nose', 'l_nose',\n",
    "                    [31, 162, 230], [127, 255, 255], [127, 255, 255], #'mouth', 'u_lip', 'l_lip'\n",
    "                    [0, 255, 85], [0, 255, 85], [0, 255, 170], [255, 255, 170], #'l_ear', 'r_ear', 'ear_r', 'eye_g'\n",
    "                    [127, 170, 255], [85, 0, 255], [255, 170, 127], #'neck', 'neck_l', 'cloth'\n",
    "                    [212, 127, 255], [0, 170, 255]#, 'hair', 'hat'\n",
    "                    ])\n",
    "\n",
    "_CMAP =torch.tensor(_CMAP, dtype=torch.float32) / 255.0\n",
    "\n",
    "\n",
    "def _campos2matrix(cam_pos, cam_center=None, cam_up=None):\n",
    "    _cam_target = np.asarray([0,0.11,0.1]) if cam_center is None else cam_center\n",
    "    _cam_target = _cam_target.reshape((1, 3))\n",
    "    # print('*** cam_center = ', _cam_target.shape)\n",
    "\n",
    "    _cam_up = np.asarray([0.0, 1.0, 0.0]) if cam_up is None else cam_up\n",
    "\n",
    "    cam_dir = (_cam_target-cam_pos)\n",
    "    cam_dir = cam_dir / np.linalg.norm(cam_dir)\n",
    "    cam_right = np.cross(cam_dir, _cam_up)\n",
    "    cam_right = cam_right / np.linalg.norm(cam_right)\n",
    "    cam_up = np.cross(cam_right, cam_dir)\n",
    "\n",
    "    cam_R = np.concatenate([cam_right.T, cam_up.T, cam_dir.T], axis=1)\n",
    "\n",
    "    cam_P = np.eye(4)\n",
    "    cam_P[:3, :3] = cam_R\n",
    "    cam_P[:3, 3] = cam_pos\n",
    "\n",
    "    return cam_P\n",
    "\n",
    "\n",
    "_CAM_INT = np.load(\n",
    "    '/home/anpei/liury/data/facial-data/seg_face_real/intrinsics_avg.npy')\n",
    "\n",
    "_CAM_INT[0, 0] *= (_IMG_SIZE / 512.0)\n",
    "_CAM_INT[1, 1] *= (_IMG_SIZE / 512.0)\n",
    "_CAM_INT[0, 2] *= (_IMG_SIZE / 512.0)\n",
    "_CAM_INT[1, 2] *= (_IMG_SIZE / 512.0)\n",
    "\n",
    "_CAM_INT = torch.from_numpy(_CAM_INT).float().unsqueeze(0)\n",
    "\n",
    "lookat = np.asarray([-1.5, -1.5, 0.0])\n",
    "cam_center =  np.asarray([-1.5, -1.5, 1.5])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "_CAM_POSE = _campos2matrix(cam_center, lookat, cam_up)\n",
    "_CAM_POSE = torch.from_numpy(_CAM_POSE).float().unsqueeze(0)\n",
    "\n",
    "_UV = np.mgrid[0:_IMG_SIZE, 0:_IMG_SIZE].astype(np.int32)\n",
    "_UV = torch.from_numpy(np.flip(_UV, axis=0).copy()).long()\n",
    "_UV = _UV.reshape(2, -1).transpose(1, 0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def render_latent(model, z):\n",
    "    B, L = z.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pose = _CAM_POSE.expand(B, -1, -1)\n",
    "        cam_int = _CAM_INT.expand(B, -1, -1)\n",
    "        uv = _UV.expand(B, -1, -1)\n",
    "\n",
    "        predictions, depth_maps = model(pose, z, cam_int, uv) # (B, L*L, C)\n",
    "\n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "\n",
    "        out_img = util.lin2img(pred, color_map=_CMAP) # (B, C, L, L)\n",
    "        out_seg = pred.view(B, _OUT_SIZE, _OUT_SIZE, 1).cpu().numpy()\n",
    "\n",
    "        print(out_img.shape)\n",
    "        \n",
    "        out_img = make_grid(out_img*255, range=(0, 255), nrow=4, padding=10, normalize=True)\n",
    "        \n",
    "        out_seg = out_seg.squeeze().astype(np.uint8) # (B, L, L)\n",
    "\n",
    "        return out_img, out_seg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def fit_gmm(gmm, model):\n",
    "    \n",
    "    X = model.latent_codes.weight.data.cpu().numpy()\n",
    "    gmm.fit(X)\n",
    "    sample_latents = gmm.sample(8)\n",
    "    \n",
    "    print(\"*** sample_latents = \", sample_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn import manifold\n",
    "from sklearn import mixture\n",
    "\n",
    "_NUM_COMP = 16\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=_NUM_COMP, covariance_type='full', random_state=0)\n",
    "X = model.latent_codes.weight.data.cpu().numpy()\n",
    "gmm.fit(X)\n",
    "\n",
    "sample_z = gmm.sample(1)[0]\n",
    "sample_z = torch.from_numpy(gmm.sample(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 512, 512])\n",
      "torch.Size([3, 1054, 2098])\n",
      "*** sample_latents =  (8, 256)\n"
     ]
    }
   ],
   "source": [
    "out_img, out_seg = render_latent(model, sample_z.float())\n",
    "print(out_img.shape)\n",
    "\n",
    "print(\"*** sample_latents = \", sample_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "  \n",
    "    def __init__(self, in_dim=256, feat_dim=400):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim, feat_dim)\n",
    "        self.fc2_mu = nn.Linear(feat_dim, 2)\n",
    "        self.fc2_sig = nn.Linear(feat_dim, 2)\n",
    "        self.fc3 = nn.Linear(2, feat_dim)\n",
    "        self.fc4 = nn.Linear(feat_dim, in_dim)\n",
    "  \n",
    "    def encode(self, x):\n",
    "        a1 = F.relu(self.fc1(x))\n",
    "        a_mu = self.fc2_mu(a1)\n",
    "        a_logvar = self.fc2_sig(a1)\n",
    "        return a_mu, a_logvar\n",
    "  \n",
    "    def decode(self, z):\n",
    "        a3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(a3))\n",
    "  \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "  \n",
    "    def forward(self,x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.in_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    \n",
    "class LatentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, model, data_len=1000, offset=0):\n",
    "        self.model = model\n",
    "        self.data_len = data_len\n",
    "        self.offset = offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.model.get_embedding(\n",
    "            {'instance_idx': torch.LongTensor([(idx+self.offset)%self.data_len]).squeeze().cuda()})\n",
    "    \n",
    "    \n",
    "def loss_function(recon_x, x, mu, logvar):    \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , Minibatch : 0 Loss = 0.27705455 Val Loss = 4.17892319\n",
      "Epoch 1 : Loss = (4.06338394) \n",
      "Epoch : 2 , Minibatch : 0 Loss = 0.25754046 Val Loss = 3.89115897\n",
      "Epoch 2 : Loss = (3.78110790) \n",
      "Epoch : 3 , Minibatch : 0 Loss = 0.23997873 Val Loss = 3.61830616\n",
      "Epoch 3 : Loss = (3.51460207) \n",
      "Epoch : 4 , Minibatch : 0 Loss = 0.22420526 Val Loss = 3.36102918\n",
      "Epoch 4 : Loss = (3.26353517) \n",
      "Epoch : 5 , Minibatch : 0 Loss = 0.20887518 Val Loss = 3.11962029\n",
      "Epoch 5 : Loss = (3.02812868) \n",
      "Epoch : 6 , Minibatch : 0 Loss = 0.18790746 Val Loss = 2.89317873\n",
      "Epoch 6 : Loss = (2.80743748) \n",
      "Epoch : 7 , Minibatch : 0 Loss = 0.18209022 Val Loss = 2.68094242\n",
      "Epoch 7 : Loss = (2.60067192) \n",
      "Epoch : 8 , Minibatch : 0 Loss = 0.16207141 Val Loss = 2.48228276\n",
      "Epoch 8 : Loss = (2.40715930) \n",
      "Epoch : 9 , Minibatch : 0 Loss = 0.15350431 Val Loss = 2.29642770\n",
      "Epoch 9 : Loss = (2.22628245) \n",
      "Epoch : 10 , Minibatch : 0 Loss = 0.14380854 Val Loss = 2.12263829\n",
      "Epoch 10 : Loss = (2.05707976) \n",
      "Epoch : 11 , Minibatch : 0 Loss = 0.13073832 Val Loss = 1.96038681\n",
      "Epoch 11 : Loss = (1.89916468) \n",
      "Epoch : 12 , Minibatch : 0 Loss = 0.11841369 Val Loss = 1.80892432\n",
      "Epoch 12 : Loss = (1.75181282) \n",
      "Epoch : 13 , Minibatch : 0 Loss = 0.10858631 Val Loss = 1.66753513\n",
      "Epoch 13 : Loss = (1.61425692) \n",
      "Epoch : 14 , Minibatch : 0 Loss = 0.10747740 Val Loss = 1.53584957\n",
      "Epoch 14 : Loss = (1.48625362) \n",
      "Epoch : 15 , Minibatch : 0 Loss = 0.09382832 Val Loss = 1.41306815\n",
      "Epoch 15 : Loss = (1.36690626) \n",
      "Epoch : 16 , Minibatch : 0 Loss = 0.08929890 Val Loss = 1.29894838\n",
      "Epoch 16 : Loss = (1.25602379) \n",
      "Epoch : 17 , Minibatch : 0 Loss = 0.08292609 Val Loss = 1.19278958\n",
      "Epoch 17 : Loss = (1.15298244) \n",
      "Epoch : 18 , Minibatch : 0 Loss = 0.07290491 Val Loss = 1.09415317\n",
      "Epoch 18 : Loss = (1.05721527) \n",
      "Epoch : 19 , Minibatch : 0 Loss = 0.06845197 Val Loss = 1.00271162\n",
      "Epoch 19 : Loss = (0.96849209) \n",
      "Epoch : 20 , Minibatch : 0 Loss = 0.06232992 Val Loss = 0.91795301\n",
      "Epoch 20 : Loss = (0.88619062) \n",
      "Epoch : 21 , Minibatch : 0 Loss = 0.05804226 Val Loss = 0.83957723\n",
      "Epoch 21 : Loss = (0.81021938) \n",
      "Epoch : 22 , Minibatch : 0 Loss = 0.05010313 Val Loss = 0.76698551\n",
      "Epoch 22 : Loss = (0.73987323) \n",
      "Epoch : 23 , Minibatch : 0 Loss = 0.04850924 Val Loss = 0.69997448\n",
      "Epoch 23 : Loss = (0.67503324) \n",
      "Epoch : 24 , Minibatch : 0 Loss = 0.04177523 Val Loss = 0.63816819\n",
      "Epoch 24 : Loss = (0.61512563) \n",
      "Epoch : 25 , Minibatch : 0 Loss = 0.03795877 Val Loss = 0.58138150\n",
      "Epoch 25 : Loss = (0.56016454) \n",
      "Epoch : 26 , Minibatch : 0 Loss = 0.03612834 Val Loss = 0.52907643\n",
      "Epoch 26 : Loss = (0.50962493) \n",
      "Epoch : 27 , Minibatch : 0 Loss = 0.03119791 Val Loss = 0.48105162\n",
      "Epoch 27 : Loss = (0.46319130) \n",
      "Epoch : 28 , Minibatch : 0 Loss = 0.02827275 Val Loss = 0.43710080\n",
      "Epoch 28 : Loss = (0.42071763) \n",
      "Epoch : 29 , Minibatch : 0 Loss = 0.02804831 Val Loss = 0.39682904\n",
      "Epoch 29 : Loss = (0.38191313) \n",
      "Epoch : 30 , Minibatch : 0 Loss = 0.02480936 Val Loss = 0.36000234\n",
      "Epoch 30 : Loss = (0.34644544) \n",
      "Epoch : 31 , Minibatch : 0 Loss = 0.02226239 Val Loss = 0.32641611\n",
      "Epoch 31 : Loss = (0.31403783) \n",
      "Epoch : 32 , Minibatch : 0 Loss = 0.02000722 Val Loss = 0.29593766\n",
      "Epoch 32 : Loss = (0.28466779) \n",
      "Epoch : 33 , Minibatch : 0 Loss = 0.01780620 Val Loss = 0.26819381\n",
      "Epoch 33 : Loss = (0.25798017) \n",
      "Epoch : 34 , Minibatch : 0 Loss = 0.01768407 Val Loss = 0.24299929\n",
      "Epoch 34 : Loss = (0.23376292) \n",
      "Epoch : 35 , Minibatch : 0 Loss = 0.01443508 Val Loss = 0.22021505\n",
      "Epoch 35 : Loss = (0.21184614) \n",
      "Epoch : 36 , Minibatch : 0 Loss = 0.01211369 Val Loss = 0.19962490\n",
      "Epoch 36 : Loss = (0.19204804) \n",
      "Epoch : 37 , Minibatch : 0 Loss = 0.01266178 Val Loss = 0.18104097\n",
      "Epoch 37 : Loss = (0.17428845) \n",
      "Epoch : 38 , Minibatch : 0 Loss = 0.01088431 Val Loss = 0.16419920\n",
      "Epoch 38 : Loss = (0.15809110) \n",
      "Epoch : 39 , Minibatch : 0 Loss = 0.00936744 Val Loss = 0.14921269\n",
      "Epoch 39 : Loss = (0.14373195) \n",
      "Epoch : 40 , Minibatch : 0 Loss = 0.00901729 Val Loss = 0.13564599\n",
      "Epoch 40 : Loss = (0.13076156) \n",
      "Epoch : 41 , Minibatch : 0 Loss = 0.00822738 Val Loss = 0.12348965\n",
      "Epoch 41 : Loss = (0.11909351) \n",
      "Epoch : 42 , Minibatch : 0 Loss = 0.00761658 Val Loss = 0.11268494\n",
      "Epoch 42 : Loss = (0.10871080) \n",
      "Epoch : 43 , Minibatch : 0 Loss = 0.00675958 Val Loss = 0.10301134\n",
      "Epoch 43 : Loss = (0.09948048) \n",
      "Epoch : 44 , Minibatch : 0 Loss = 0.00675362 Val Loss = 0.09430835\n",
      "Epoch 44 : Loss = (0.09118006) \n",
      "Epoch : 45 , Minibatch : 0 Loss = 0.00624987 Val Loss = 0.08655512\n",
      "Epoch 45 : Loss = (0.08374435) \n",
      "Epoch : 46 , Minibatch : 0 Loss = 0.00575751 Val Loss = 0.07970327\n",
      "Epoch 46 : Loss = (0.07724956) \n",
      "Epoch : 47 , Minibatch : 0 Loss = 0.00477719 Val Loss = 0.07349977\n",
      "Epoch 47 : Loss = (0.07130778) \n",
      "Epoch : 48 , Minibatch : 0 Loss = 0.00434321 Val Loss = 0.06806967\n",
      "Epoch 48 : Loss = (0.06609690) \n",
      "Epoch : 49 , Minibatch : 0 Loss = 0.00419247 Val Loss = 0.06321719\n",
      "Epoch 49 : Loss = (0.06146398) \n",
      "Epoch : 50 , Minibatch : 0 Loss = 0.00435910 Val Loss = 0.05888221\n",
      "Epoch 50 : Loss = (0.05732483) \n",
      "Epoch : 51 , Minibatch : 0 Loss = 0.00393304 Val Loss = 0.05504423\n",
      "Epoch 51 : Loss = (0.05366176) \n",
      "Epoch : 52 , Minibatch : 0 Loss = 0.00286290 Val Loss = 0.05159605\n",
      "Epoch 52 : Loss = (0.05036867) \n",
      "Epoch : 53 , Minibatch : 0 Loss = 0.00334322 Val Loss = 0.04854512\n",
      "Epoch 53 : Loss = (0.04746693) \n",
      "Epoch : 54 , Minibatch : 0 Loss = 0.00300026 Val Loss = 0.04580522\n",
      "Epoch 54 : Loss = (0.04483306) \n",
      "Epoch : 55 , Minibatch : 0 Loss = 0.00360650 Val Loss = 0.04337093\n",
      "Epoch 55 : Loss = (0.04250589) \n",
      "Epoch : 56 , Minibatch : 0 Loss = 0.00339866 Val Loss = 0.04119220\n",
      "Epoch 56 : Loss = (0.04041862) \n",
      "Epoch : 57 , Minibatch : 0 Loss = 0.00254115 Val Loss = 0.03923607\n",
      "Epoch 57 : Loss = (0.03852525) \n",
      "Epoch : 58 , Minibatch : 0 Loss = 0.00267300 Val Loss = 0.03749126\n",
      "Epoch 58 : Loss = (0.03685033) \n",
      "Epoch : 59 , Minibatch : 0 Loss = 0.00232902 Val Loss = 0.03589350\n",
      "Epoch 59 : Loss = (0.03534207) \n",
      "Epoch : 60 , Minibatch : 0 Loss = 0.00245079 Val Loss = 0.03443170\n",
      "Epoch 60 : Loss = (0.03392321) \n",
      "Epoch : 61 , Minibatch : 0 Loss = 0.00225461 Val Loss = 0.03314245\n",
      "Epoch 61 : Loss = (0.03268105) \n",
      "Epoch : 62 , Minibatch : 0 Loss = 0.00206348 Val Loss = 0.03195810\n",
      "Epoch 62 : Loss = (0.03152373) \n",
      "Epoch : 63 , Minibatch : 0 Loss = 0.00200030 Val Loss = 0.03088659\n",
      "Epoch 63 : Loss = (0.03048912) \n",
      "Epoch : 64 , Minibatch : 0 Loss = 0.00219148 Val Loss = 0.02988008\n",
      "Epoch 64 : Loss = (0.02952671) \n",
      "Epoch : 65 , Minibatch : 0 Loss = 0.00170848 Val Loss = 0.02895793\n",
      "Epoch 65 : Loss = (0.02861464) \n",
      "Epoch : 66 , Minibatch : 0 Loss = 0.00181371 Val Loss = 0.02812687\n",
      "Epoch 66 : Loss = (0.02781585) \n",
      "Epoch : 67 , Minibatch : 0 Loss = 0.00155929 Val Loss = 0.02732956\n",
      "Epoch 67 : Loss = (0.02703282) \n",
      "Epoch : 68 , Minibatch : 0 Loss = 0.00158361 Val Loss = 0.02659842\n",
      "Epoch 68 : Loss = (0.02632469) \n",
      "Epoch : 69 , Minibatch : 0 Loss = 0.00186452 Val Loss = 0.02590591\n",
      "Epoch 69 : Loss = (0.02565342) \n",
      "Epoch : 70 , Minibatch : 0 Loss = 0.00167453 Val Loss = 0.02525565\n",
      "Epoch 70 : Loss = (0.02502725) \n",
      "Epoch : 71 , Minibatch : 0 Loss = 0.00180122 Val Loss = 0.02464366\n",
      "Epoch 71 : Loss = (0.02442864) \n",
      "Epoch : 72 , Minibatch : 0 Loss = 0.00176182 Val Loss = 0.02406898\n",
      "Epoch 72 : Loss = (0.02386436) \n",
      "Epoch : 73 , Minibatch : 0 Loss = 0.00157130 Val Loss = 0.02352682\n",
      "Epoch 73 : Loss = (0.02332762) \n",
      "Epoch : 74 , Minibatch : 0 Loss = 0.00145671 Val Loss = 0.02301434\n",
      "Epoch 74 : Loss = (0.02282941) \n",
      "Epoch : 75 , Minibatch : 0 Loss = 0.00145110 Val Loss = 0.02251405\n",
      "Epoch 75 : Loss = (0.02233326) \n",
      "Epoch : 76 , Minibatch : 0 Loss = 0.00142688 Val Loss = 0.02204937\n",
      "Epoch 76 : Loss = (0.02187353) \n",
      "Epoch : 77 , Minibatch : 0 Loss = 0.00131989 Val Loss = 0.02160132\n",
      "Epoch 77 : Loss = (0.02143764) \n",
      "Epoch : 78 , Minibatch : 0 Loss = 0.00128484 Val Loss = 0.02116174\n",
      "Epoch 78 : Loss = (0.02100480) \n",
      "Epoch : 79 , Minibatch : 0 Loss = 0.00140297 Val Loss = 0.02074343\n",
      "Epoch 79 : Loss = (0.02059460) \n",
      "Epoch : 80 , Minibatch : 0 Loss = 0.00149199 Val Loss = 0.02034566\n",
      "Epoch 80 : Loss = (0.02020210) \n",
      "Epoch : 81 , Minibatch : 0 Loss = 0.00138301 Val Loss = 0.01995334\n",
      "Epoch 81 : Loss = (0.01981604) \n",
      "Epoch : 82 , Minibatch : 0 Loss = 0.00118899 Val Loss = 0.01958436\n",
      "Epoch 82 : Loss = (0.01944768) \n",
      "Epoch : 83 , Minibatch : 0 Loss = 0.00115609 Val Loss = 0.01922852\n",
      "Epoch 83 : Loss = (0.01908979) \n",
      "Epoch : 84 , Minibatch : 0 Loss = 0.00130403 Val Loss = 0.01887617\n",
      "Epoch 84 : Loss = (0.01874995) \n",
      "Epoch : 85 , Minibatch : 0 Loss = 0.00142071 Val Loss = 0.01853678\n",
      "Epoch 85 : Loss = (0.01841390) \n",
      "Epoch : 86 , Minibatch : 0 Loss = 0.00117293 Val Loss = 0.01820943\n",
      "Epoch 86 : Loss = (0.01808944) \n",
      "Epoch : 87 , Minibatch : 0 Loss = 0.00130862 Val Loss = 0.01788574\n",
      "Epoch 87 : Loss = (0.01777238) \n",
      "Epoch : 88 , Minibatch : 0 Loss = 0.00121677 Val Loss = 0.01758248\n",
      "Epoch 88 : Loss = (0.01746804) \n",
      "Epoch : 89 , Minibatch : 0 Loss = 0.00122058 Val Loss = 0.01728183\n",
      "Epoch 89 : Loss = (0.01717097) \n",
      "Epoch : 90 , Minibatch : 0 Loss = 0.00121143 Val Loss = 0.01698646\n",
      "Epoch 90 : Loss = (0.01688048) \n",
      "Epoch : 91 , Minibatch : 0 Loss = 0.00105047 Val Loss = 0.01669896\n",
      "Epoch 91 : Loss = (0.01659727) \n",
      "Epoch : 92 , Minibatch : 0 Loss = 0.00113150 Val Loss = 0.01642314\n",
      "Epoch 92 : Loss = (0.01632589) \n",
      "Epoch : 93 , Minibatch : 0 Loss = 0.00112283 Val Loss = 0.01615092\n",
      "Epoch 93 : Loss = (0.01605368) \n",
      "Epoch : 94 , Minibatch : 0 Loss = 0.00121874 Val Loss = 0.01588684\n",
      "Epoch 94 : Loss = (0.01579592) \n",
      "Epoch : 95 , Minibatch : 0 Loss = 0.00112221 Val Loss = 0.01562461\n",
      "Epoch 95 : Loss = (0.01553592) \n",
      "Epoch : 96 , Minibatch : 0 Loss = 0.00106737 Val Loss = 0.01537493\n",
      "Epoch 96 : Loss = (0.01528513) \n",
      "Epoch : 97 , Minibatch : 0 Loss = 0.00105202 Val Loss = 0.01513305\n",
      "Epoch 97 : Loss = (0.01504159) \n",
      "Epoch : 98 , Minibatch : 0 Loss = 0.00092962 Val Loss = 0.01489356\n",
      "Epoch 98 : Loss = (0.01480567) \n",
      "Epoch : 99 , Minibatch : 0 Loss = 0.00099993 Val Loss = 0.01465839\n",
      "Epoch 99 : Loss = (0.01457232) \n",
      "Epoch : 100 , Minibatch : 0 Loss = 0.00090125 Val Loss = 0.01442713\n",
      "Epoch 100 : Loss = (0.01434514) \n",
      "Epoch : 101 , Minibatch : 0 Loss = 0.00093541 Val Loss = 0.01420388\n",
      "Epoch 101 : Loss = (0.01412335) \n",
      "Epoch : 102 , Minibatch : 0 Loss = 0.00098452 Val Loss = 0.01398066\n",
      "Epoch 102 : Loss = (0.01390174) \n",
      "Epoch : 103 , Minibatch : 0 Loss = 0.00093737 Val Loss = 0.01376683\n",
      "Epoch 103 : Loss = (0.01369038) \n",
      "Epoch : 104 , Minibatch : 0 Loss = 0.00082308 Val Loss = 0.01355752\n",
      "Epoch 104 : Loss = (0.01348308) \n",
      "Epoch : 105 , Minibatch : 0 Loss = 0.00084853 Val Loss = 0.01334849\n",
      "Epoch 105 : Loss = (0.01327705) \n",
      "Epoch : 106 , Minibatch : 0 Loss = 0.00095204 Val Loss = 0.01314452\n",
      "Epoch 106 : Loss = (0.01307845) \n",
      "Epoch : 107 , Minibatch : 0 Loss = 0.00090501 Val Loss = 0.01294822\n",
      "Epoch 107 : Loss = (0.01288033) \n",
      "Epoch : 108 , Minibatch : 0 Loss = 0.00092414 Val Loss = 0.01275560\n",
      "Epoch 108 : Loss = (0.01268589) \n",
      "Epoch : 109 , Minibatch : 0 Loss = 0.00092012 Val Loss = 0.01256445\n",
      "Epoch 109 : Loss = (0.01249549) \n",
      "Epoch : 110 , Minibatch : 0 Loss = 0.00070667 Val Loss = 0.01238027\n",
      "Epoch 110 : Loss = (0.01230994) \n",
      "Epoch : 111 , Minibatch : 0 Loss = 0.00075307 Val Loss = 0.01219288\n",
      "Epoch 111 : Loss = (0.01212817) \n",
      "Epoch : 112 , Minibatch : 0 Loss = 0.00090572 Val Loss = 0.01201048\n",
      "Epoch 112 : Loss = (0.01195085) \n",
      "Epoch : 113 , Minibatch : 0 Loss = 0.00074685 Val Loss = 0.01183608\n",
      "Epoch 113 : Loss = (0.01177368) \n",
      "Epoch : 114 , Minibatch : 0 Loss = 0.00083885 Val Loss = 0.01166037\n",
      "Epoch 114 : Loss = (0.01160070) \n",
      "Epoch : 115 , Minibatch : 0 Loss = 0.00076085 Val Loss = 0.01148874\n",
      "Epoch 115 : Loss = (0.01143083) \n",
      "Epoch : 116 , Minibatch : 0 Loss = 0.00075045 Val Loss = 0.01132232\n",
      "Epoch 116 : Loss = (0.01126060) \n",
      "Epoch : 117 , Minibatch : 0 Loss = 0.00072220 Val Loss = 0.01115826\n",
      "Epoch 117 : Loss = (0.01109868) \n",
      "Epoch : 118 , Minibatch : 0 Loss = 0.00079840 Val Loss = 0.01099786\n",
      "Epoch 118 : Loss = (0.01093742) \n",
      "Epoch : 119 , Minibatch : 0 Loss = 0.00072369 Val Loss = 0.01083609\n",
      "Epoch 119 : Loss = (0.01077741) \n",
      "Epoch : 120 , Minibatch : 0 Loss = 0.00071126 Val Loss = 0.01068231\n",
      "Epoch 120 : Loss = (0.01062420) \n",
      "Epoch : 121 , Minibatch : 0 Loss = 0.00076151 Val Loss = 0.01052433\n",
      "Epoch 121 : Loss = (0.01047385) \n",
      "Epoch : 122 , Minibatch : 0 Loss = 0.00062987 Val Loss = 0.01037073\n",
      "Epoch 122 : Loss = (0.01031840) \n",
      "Epoch : 123 , Minibatch : 0 Loss = 0.00068748 Val Loss = 0.01022434\n",
      "Epoch 123 : Loss = (0.01017264) \n",
      "Epoch : 124 , Minibatch : 0 Loss = 0.00065592 Val Loss = 0.01007456\n",
      "Epoch 124 : Loss = (0.01002321) \n",
      "Epoch : 125 , Minibatch : 0 Loss = 0.00075290 Val Loss = 0.00993097\n",
      "Epoch 125 : Loss = (0.00988457) \n",
      "Epoch : 126 , Minibatch : 0 Loss = 0.00068527 Val Loss = 0.00978926\n",
      "Epoch 126 : Loss = (0.00973913) \n",
      "Epoch : 127 , Minibatch : 0 Loss = 0.00053841 Val Loss = 0.00964668\n",
      "Epoch 127 : Loss = (0.00959831) \n",
      "Epoch : 128 , Minibatch : 0 Loss = 0.00053674 Val Loss = 0.00951281\n",
      "Epoch 128 : Loss = (0.00946343) \n",
      "Epoch : 129 , Minibatch : 0 Loss = 0.00070569 Val Loss = 0.00937641\n",
      "Epoch 129 : Loss = (0.00932908) \n",
      "Epoch : 130 , Minibatch : 0 Loss = 0.00063008 Val Loss = 0.00924361\n",
      "Epoch 130 : Loss = (0.00920016) \n",
      "Epoch : 131 , Minibatch : 0 Loss = 0.00058481 Val Loss = 0.00910854\n",
      "Epoch 131 : Loss = (0.00906554) \n",
      "Epoch : 132 , Minibatch : 0 Loss = 0.00070211 Val Loss = 0.00898013\n",
      "Epoch 132 : Loss = (0.00893560) \n",
      "Epoch : 133 , Minibatch : 0 Loss = 0.00060183 Val Loss = 0.00885275\n",
      "Epoch 133 : Loss = (0.00881067) \n",
      "Epoch : 134 , Minibatch : 0 Loss = 0.00054571 Val Loss = 0.00872654\n",
      "Epoch 134 : Loss = (0.00868171) \n",
      "Epoch : 135 , Minibatch : 0 Loss = 0.00057137 Val Loss = 0.00860387\n",
      "Epoch 135 : Loss = (0.00856248) \n",
      "Epoch : 136 , Minibatch : 0 Loss = 0.00060335 Val Loss = 0.00848281\n",
      "Epoch 136 : Loss = (0.00844118) \n",
      "Epoch : 137 , Minibatch : 0 Loss = 0.00055963 Val Loss = 0.00835913\n",
      "Epoch 137 : Loss = (0.00832081) \n",
      "Epoch : 138 , Minibatch : 0 Loss = 0.00051397 Val Loss = 0.00824201\n",
      "Epoch 138 : Loss = (0.00820073) \n",
      "Epoch : 139 , Minibatch : 0 Loss = 0.00049505 Val Loss = 0.00812691\n",
      "Epoch 139 : Loss = (0.00808528) \n",
      "Epoch : 140 , Minibatch : 0 Loss = 0.00047839 Val Loss = 0.00801370\n",
      "Epoch 140 : Loss = (0.00797164) \n",
      "Epoch : 141 , Minibatch : 0 Loss = 0.00053295 Val Loss = 0.00789621\n",
      "Epoch 141 : Loss = (0.00785792) \n",
      "Epoch : 142 , Minibatch : 0 Loss = 0.00048390 Val Loss = 0.00778511\n",
      "Epoch 142 : Loss = (0.00774926) \n",
      "Epoch : 143 , Minibatch : 0 Loss = 0.00048581 Val Loss = 0.00767574\n",
      "Epoch 143 : Loss = (0.00763798) \n",
      "Epoch : 144 , Minibatch : 0 Loss = 0.00054064 Val Loss = 0.00756931\n",
      "Epoch 144 : Loss = (0.00753227) \n",
      "Epoch : 145 , Minibatch : 0 Loss = 0.00050324 Val Loss = 0.00745863\n",
      "Epoch 145 : Loss = (0.00742498) \n",
      "Epoch : 146 , Minibatch : 0 Loss = 0.00049424 Val Loss = 0.00735557\n",
      "Epoch 146 : Loss = (0.00731945) \n",
      "Epoch : 147 , Minibatch : 0 Loss = 0.00051051 Val Loss = 0.00725281\n",
      "Epoch 147 : Loss = (0.00721666) \n",
      "Epoch : 148 , Minibatch : 0 Loss = 0.00052881 Val Loss = 0.00714839\n",
      "Epoch 148 : Loss = (0.00711414) \n",
      "Epoch : 149 , Minibatch : 0 Loss = 0.00048465 Val Loss = 0.00704566\n",
      "Epoch 149 : Loss = (0.00701430) \n",
      "Epoch : 150 , Minibatch : 0 Loss = 0.00045872 Val Loss = 0.00695187\n",
      "Epoch 150 : Loss = (0.00691453) \n",
      "Epoch : 151 , Minibatch : 0 Loss = 0.00048673 Val Loss = 0.00685161\n",
      "Epoch 151 : Loss = (0.00681922) \n",
      "Epoch : 152 , Minibatch : 0 Loss = 0.00043556 Val Loss = 0.00675392\n",
      "Epoch 152 : Loss = (0.00672272) \n",
      "Epoch : 153 , Minibatch : 0 Loss = 0.00045744 Val Loss = 0.00665796\n",
      "Epoch 153 : Loss = (0.00662553) \n",
      "Epoch : 154 , Minibatch : 0 Loss = 0.00046030 Val Loss = 0.00656432\n",
      "Epoch 154 : Loss = (0.00653431) \n",
      "Epoch : 155 , Minibatch : 0 Loss = 0.00048932 Val Loss = 0.00646958\n",
      "Epoch 155 : Loss = (0.00644130) \n",
      "Epoch : 156 , Minibatch : 0 Loss = 0.00044346 Val Loss = 0.00637934\n",
      "Epoch 156 : Loss = (0.00635040) \n",
      "Epoch : 157 , Minibatch : 0 Loss = 0.00039318 Val Loss = 0.00628871\n",
      "Epoch 157 : Loss = (0.00626034) \n",
      "Epoch : 158 , Minibatch : 0 Loss = 0.00040877 Val Loss = 0.00620151\n",
      "Epoch 158 : Loss = (0.00617000) \n",
      "Epoch : 159 , Minibatch : 0 Loss = 0.00041658 Val Loss = 0.00611135\n",
      "Epoch 159 : Loss = (0.00608483) \n",
      "Epoch : 160 , Minibatch : 0 Loss = 0.00046116 Val Loss = 0.00602710\n",
      "Epoch 160 : Loss = (0.00599816) \n",
      "Epoch : 161 , Minibatch : 0 Loss = 0.00042018 Val Loss = 0.00594425\n",
      "Epoch 161 : Loss = (0.00591603) \n",
      "Epoch : 162 , Minibatch : 0 Loss = 0.00033641 Val Loss = 0.00585952\n",
      "Epoch 162 : Loss = (0.00583205) \n",
      "Epoch : 163 , Minibatch : 0 Loss = 0.00035346 Val Loss = 0.00577497\n",
      "Epoch 163 : Loss = (0.00574878) \n",
      "Epoch : 164 , Minibatch : 0 Loss = 0.00038990 Val Loss = 0.00569585\n",
      "Epoch 164 : Loss = (0.00566795) \n",
      "Epoch : 165 , Minibatch : 0 Loss = 0.00044096 Val Loss = 0.00561440\n",
      "Epoch 165 : Loss = (0.00558841) \n",
      "Epoch : 166 , Minibatch : 0 Loss = 0.00036740 Val Loss = 0.00553530\n",
      "Epoch 166 : Loss = (0.00550950) \n",
      "Epoch : 167 , Minibatch : 0 Loss = 0.00035626 Val Loss = 0.00545487\n",
      "Epoch 167 : Loss = (0.00543162) \n",
      "Epoch : 168 , Minibatch : 0 Loss = 0.00036776 Val Loss = 0.00537917\n",
      "Epoch 168 : Loss = (0.00535479) \n",
      "Epoch : 169 , Minibatch : 0 Loss = 0.00034061 Val Loss = 0.00530222\n",
      "Epoch 169 : Loss = (0.00527880) \n",
      "Epoch : 170 , Minibatch : 0 Loss = 0.00035295 Val Loss = 0.00523099\n",
      "Epoch 170 : Loss = (0.00520378) \n",
      "Epoch : 171 , Minibatch : 0 Loss = 0.00034222 Val Loss = 0.00515521\n",
      "Epoch 171 : Loss = (0.00513414) \n",
      "Epoch : 172 , Minibatch : 0 Loss = 0.00035167 Val Loss = 0.00508204\n",
      "Epoch 172 : Loss = (0.00505835) \n",
      "Epoch : 173 , Minibatch : 0 Loss = 0.00033763 Val Loss = 0.00501013\n",
      "Epoch 173 : Loss = (0.00498417) \n",
      "Epoch : 174 , Minibatch : 0 Loss = 0.00035796 Val Loss = 0.00493953\n",
      "Epoch 174 : Loss = (0.00491515) \n",
      "Epoch : 175 , Minibatch : 0 Loss = 0.00027606 Val Loss = 0.00487083\n",
      "Epoch 175 : Loss = (0.00485113) \n",
      "Epoch : 176 , Minibatch : 0 Loss = 0.00030309 Val Loss = 0.00480074\n",
      "Epoch 176 : Loss = (0.00477868) \n",
      "Epoch : 177 , Minibatch : 0 Loss = 0.00032559 Val Loss = 0.00473660\n",
      "Epoch 177 : Loss = (0.00471145) \n",
      "Epoch : 178 , Minibatch : 0 Loss = 0.00029048 Val Loss = 0.00466663\n",
      "Epoch 178 : Loss = (0.00464651) \n",
      "Epoch : 179 , Minibatch : 0 Loss = 0.00030333 Val Loss = 0.00460154\n",
      "Epoch 179 : Loss = (0.00457644) \n",
      "Epoch : 180 , Minibatch : 0 Loss = 0.00030053 Val Loss = 0.00453582\n",
      "Epoch 180 : Loss = (0.00451756) \n",
      "Epoch : 181 , Minibatch : 0 Loss = 0.00031075 Val Loss = 0.00447106\n",
      "Epoch 181 : Loss = (0.00445288) \n",
      "Epoch : 182 , Minibatch : 0 Loss = 0.00029066 Val Loss = 0.00441274\n",
      "Epoch 182 : Loss = (0.00439078) \n",
      "Epoch : 183 , Minibatch : 0 Loss = 0.00034040 Val Loss = 0.00434816\n",
      "Epoch 183 : Loss = (0.00432858) \n",
      "Epoch : 184 , Minibatch : 0 Loss = 0.00030446 Val Loss = 0.00428921\n",
      "Epoch 184 : Loss = (0.00426593) \n",
      "Epoch : 185 , Minibatch : 0 Loss = 0.00026760 Val Loss = 0.00422496\n",
      "Epoch 185 : Loss = (0.00420660) \n",
      "Epoch : 186 , Minibatch : 0 Loss = 0.00027823 Val Loss = 0.00416690\n",
      "Epoch 186 : Loss = (0.00414583) \n",
      "Epoch : 187 , Minibatch : 0 Loss = 0.00025180 Val Loss = 0.00410807\n",
      "Epoch 187 : Loss = (0.00408971) \n",
      "Epoch : 188 , Minibatch : 0 Loss = 0.00027373 Val Loss = 0.00405180\n",
      "Epoch 188 : Loss = (0.00403354) \n",
      "Epoch : 189 , Minibatch : 0 Loss = 0.00026581 Val Loss = 0.00399294\n",
      "Epoch 189 : Loss = (0.00397542) \n",
      "Epoch : 190 , Minibatch : 0 Loss = 0.00023136 Val Loss = 0.00393683\n",
      "Epoch 190 : Loss = (0.00391784) \n",
      "Epoch : 191 , Minibatch : 0 Loss = 0.00025380 Val Loss = 0.00388229\n",
      "Epoch 191 : Loss = (0.00386426) \n",
      "Epoch : 192 , Minibatch : 0 Loss = 0.00024047 Val Loss = 0.00382856\n",
      "Epoch 192 : Loss = (0.00380939) \n",
      "Epoch : 193 , Minibatch : 0 Loss = 0.00020945 Val Loss = 0.00377226\n",
      "Epoch 193 : Loss = (0.00375775) \n",
      "Epoch : 194 , Minibatch : 0 Loss = 0.00021848 Val Loss = 0.00372076\n",
      "Epoch 194 : Loss = (0.00370294) \n",
      "Epoch : 195 , Minibatch : 0 Loss = 0.00024194 Val Loss = 0.00366655\n",
      "Epoch 195 : Loss = (0.00365150) \n",
      "Epoch : 196 , Minibatch : 0 Loss = 0.00023454 Val Loss = 0.00361380\n",
      "Epoch 196 : Loss = (0.00360510) \n",
      "Epoch : 197 , Minibatch : 0 Loss = 0.00023291 Val Loss = 0.00356722\n",
      "Epoch 197 : Loss = (0.00355080) \n",
      "Epoch : 198 , Minibatch : 0 Loss = 0.00023904 Val Loss = 0.00351778\n",
      "Epoch 198 : Loss = (0.00349936) \n",
      "Epoch : 199 , Minibatch : 0 Loss = 0.00021434 Val Loss = 0.00346512\n",
      "Epoch 199 : Loss = (0.00345054) \n",
      "Epoch : 200 , Minibatch : 0 Loss = 0.00023994 Val Loss = 0.00341496\n",
      "Epoch 200 : Loss = (0.00340223) \n",
      "Epoch : 201 , Minibatch : 0 Loss = 0.00022519 Val Loss = 0.00337183\n",
      "Epoch 201 : Loss = (0.00335604) \n",
      "Epoch : 202 , Minibatch : 0 Loss = 0.00021678 Val Loss = 0.00332466\n",
      "Epoch 202 : Loss = (0.00331032) \n",
      "Epoch : 203 , Minibatch : 0 Loss = 0.00023341 Val Loss = 0.00327951\n",
      "Epoch 203 : Loss = (0.00326312) \n",
      "Epoch : 204 , Minibatch : 0 Loss = 0.00021979 Val Loss = 0.00323197\n",
      "Epoch 204 : Loss = (0.00321776) \n",
      "Epoch : 205 , Minibatch : 0 Loss = 0.00021672 Val Loss = 0.00318390\n",
      "Epoch 205 : Loss = (0.00317115) \n",
      "Epoch : 206 , Minibatch : 0 Loss = 0.00021502 Val Loss = 0.00314164\n",
      "Epoch 206 : Loss = (0.00312904) \n",
      "Epoch : 207 , Minibatch : 0 Loss = 0.00020206 Val Loss = 0.00309673\n",
      "Epoch 207 : Loss = (0.00308448) \n",
      "Epoch : 208 , Minibatch : 0 Loss = 0.00022188 Val Loss = 0.00305498\n",
      "Epoch 208 : Loss = (0.00304195) \n",
      "Epoch : 209 , Minibatch : 0 Loss = 0.00019136 Val Loss = 0.00301239\n",
      "Epoch 209 : Loss = (0.00300002) \n",
      "Epoch : 210 , Minibatch : 0 Loss = 0.00018942 Val Loss = 0.00297177\n",
      "Epoch 210 : Loss = (0.00295627) \n",
      "Epoch : 211 , Minibatch : 0 Loss = 0.00018159 Val Loss = 0.00293073\n",
      "Epoch 211 : Loss = (0.00291756) \n",
      "Epoch : 212 , Minibatch : 0 Loss = 0.00017968 Val Loss = 0.00288934\n",
      "Epoch 212 : Loss = (0.00287652) \n",
      "Epoch : 213 , Minibatch : 0 Loss = 0.00020227 Val Loss = 0.00284979\n",
      "Epoch 213 : Loss = (0.00283849) \n",
      "Epoch : 214 , Minibatch : 0 Loss = 0.00016722 Val Loss = 0.00280821\n",
      "Epoch 214 : Loss = (0.00279647) \n",
      "Epoch : 215 , Minibatch : 0 Loss = 0.00017905 Val Loss = 0.00277165\n",
      "Epoch 215 : Loss = (0.00275996) \n",
      "Epoch : 216 , Minibatch : 0 Loss = 0.00017241 Val Loss = 0.00273299\n",
      "Epoch 216 : Loss = (0.00272414) \n",
      "Epoch : 217 , Minibatch : 0 Loss = 0.00017279 Val Loss = 0.00269490\n",
      "Epoch 217 : Loss = (0.00268319) \n",
      "Epoch : 218 , Minibatch : 0 Loss = 0.00016764 Val Loss = 0.00265881\n",
      "Epoch 218 : Loss = (0.00264657) \n",
      "Epoch : 219 , Minibatch : 0 Loss = 0.00015977 Val Loss = 0.00262332\n",
      "Epoch 219 : Loss = (0.00261384) \n",
      "Epoch : 220 , Minibatch : 0 Loss = 0.00016955 Val Loss = 0.00258917\n",
      "Epoch 220 : Loss = (0.00257626) \n",
      "Epoch : 221 , Minibatch : 0 Loss = 0.00016958 Val Loss = 0.00255325\n",
      "Epoch 221 : Loss = (0.00254279) \n",
      "Epoch : 222 , Minibatch : 0 Loss = 0.00016606 Val Loss = 0.00251830\n",
      "Epoch 222 : Loss = (0.00250593) \n",
      "Epoch : 223 , Minibatch : 0 Loss = 0.00016126 Val Loss = 0.00248110\n",
      "Epoch 223 : Loss = (0.00247118) \n",
      "Epoch : 224 , Minibatch : 0 Loss = 0.00016755 Val Loss = 0.00244966\n",
      "Epoch 224 : Loss = (0.00243825) \n",
      "Epoch : 225 , Minibatch : 0 Loss = 0.00018856 Val Loss = 0.00241554\n",
      "Epoch 225 : Loss = (0.00240609) \n",
      "Epoch : 226 , Minibatch : 0 Loss = 0.00017381 Val Loss = 0.00238106\n",
      "Epoch 226 : Loss = (0.00236982) \n",
      "Epoch : 227 , Minibatch : 0 Loss = 0.00015512 Val Loss = 0.00234956\n",
      "Epoch 227 : Loss = (0.00234011) \n",
      "Epoch : 228 , Minibatch : 0 Loss = 0.00014794 Val Loss = 0.00231755\n",
      "Epoch 228 : Loss = (0.00231141) \n",
      "Epoch : 229 , Minibatch : 0 Loss = 0.00015625 Val Loss = 0.00228688\n",
      "Epoch 229 : Loss = (0.00227863) \n",
      "Epoch : 230 , Minibatch : 0 Loss = 0.00015891 Val Loss = 0.00225538\n",
      "Epoch 230 : Loss = (0.00224751) \n",
      "Epoch : 231 , Minibatch : 0 Loss = 0.00016516 Val Loss = 0.00222549\n",
      "Epoch 231 : Loss = (0.00221762) \n",
      "Epoch : 232 , Minibatch : 0 Loss = 0.00014496 Val Loss = 0.00219363\n",
      "Epoch 232 : Loss = (0.00218779) \n",
      "Epoch : 233 , Minibatch : 0 Loss = 0.00013030 Val Loss = 0.00216702\n",
      "Epoch 233 : Loss = (0.00215590) \n",
      "Epoch : 234 , Minibatch : 0 Loss = 0.00015244 Val Loss = 0.00213936\n",
      "Epoch 234 : Loss = (0.00212881) \n",
      "Epoch : 235 , Minibatch : 0 Loss = 0.00014067 Val Loss = 0.00210726\n",
      "Epoch 235 : Loss = (0.00209981) \n",
      "Epoch : 236 , Minibatch : 0 Loss = 0.00014246 Val Loss = 0.00207999\n",
      "Epoch 236 : Loss = (0.00207502) \n",
      "Epoch : 237 , Minibatch : 0 Loss = 0.00012320 Val Loss = 0.00205353\n",
      "Epoch 237 : Loss = (0.00204703) \n",
      "Epoch : 238 , Minibatch : 0 Loss = 0.00013113 Val Loss = 0.00202495\n",
      "Epoch 238 : Loss = (0.00201836) \n",
      "Epoch : 239 , Minibatch : 0 Loss = 0.00014812 Val Loss = 0.00200069\n",
      "Epoch 239 : Loss = (0.00199187) \n",
      "Epoch : 240 , Minibatch : 0 Loss = 0.00013593 Val Loss = 0.00197107\n",
      "Epoch 240 : Loss = (0.00196567) \n",
      "Epoch : 241 , Minibatch : 0 Loss = 0.00013572 Val Loss = 0.00194460\n",
      "Epoch 241 : Loss = (0.00193971) \n",
      "Epoch : 242 , Minibatch : 0 Loss = 0.00012720 Val Loss = 0.00192201\n",
      "Epoch 242 : Loss = (0.00191498) \n",
      "Epoch : 243 , Minibatch : 0 Loss = 0.00011927 Val Loss = 0.00189805\n",
      "Epoch 243 : Loss = (0.00189048) \n",
      "Epoch : 244 , Minibatch : 0 Loss = 0.00012293 Val Loss = 0.00186825\n",
      "Epoch 244 : Loss = (0.00186497) \n",
      "Epoch : 245 , Minibatch : 0 Loss = 0.00011206 Val Loss = 0.00184667\n",
      "Epoch 245 : Loss = (0.00183949) \n",
      "Epoch : 246 , Minibatch : 0 Loss = 0.00011784 Val Loss = 0.00182298\n",
      "Epoch 246 : Loss = (0.00181523) \n",
      "Epoch : 247 , Minibatch : 0 Loss = 0.00011268 Val Loss = 0.00180021\n",
      "Epoch 247 : Loss = (0.00179425) \n",
      "Epoch : 248 , Minibatch : 0 Loss = 0.00012314 Val Loss = 0.00177637\n",
      "Epoch 248 : Loss = (0.00177112) \n",
      "Epoch : 249 , Minibatch : 0 Loss = 0.00010788 Val Loss = 0.00175339\n",
      "Epoch 249 : Loss = (0.00174853) \n",
      "Epoch : 250 , Minibatch : 0 Loss = 0.00010756 Val Loss = 0.00173423\n",
      "Epoch 250 : Loss = (0.00172552) \n",
      "Epoch : 251 , Minibatch : 0 Loss = 0.00010756 Val Loss = 0.00170937\n",
      "Epoch 251 : Loss = (0.00170523) \n",
      "Epoch : 252 , Minibatch : 0 Loss = 0.00011492 Val Loss = 0.00168708\n",
      "Epoch 252 : Loss = (0.00168413) \n",
      "Epoch : 253 , Minibatch : 0 Loss = 0.00011256 Val Loss = 0.00166383\n",
      "Epoch 253 : Loss = (0.00165930) \n",
      "Epoch : 254 , Minibatch : 0 Loss = 0.00010362 Val Loss = 0.00164804\n",
      "Epoch 254 : Loss = (0.00164080) \n",
      "Epoch : 255 , Minibatch : 0 Loss = 0.00011098 Val Loss = 0.00162509\n",
      "Epoch 255 : Loss = (0.00162104) \n",
      "Epoch : 256 , Minibatch : 0 Loss = 0.00010860 Val Loss = 0.00160465\n",
      "Epoch 256 : Loss = (0.00159916) \n",
      "Epoch : 257 , Minibatch : 0 Loss = 0.00011227 Val Loss = 0.00158253\n",
      "Epoch 257 : Loss = (0.00157508) \n",
      "Epoch : 258 , Minibatch : 0 Loss = 0.00010329 Val Loss = 0.00156379\n",
      "Epoch 258 : Loss = (0.00155836) \n",
      "Epoch : 259 , Minibatch : 0 Loss = 0.00009549 Val Loss = 0.00154263\n",
      "Epoch 259 : Loss = (0.00154093) \n",
      "Epoch : 260 , Minibatch : 0 Loss = 0.00009364 Val Loss = 0.00152263\n",
      "Epoch 260 : Loss = (0.00151953) \n",
      "Epoch : 261 , Minibatch : 0 Loss = 0.00011012 Val Loss = 0.00150582\n",
      "Epoch 261 : Loss = (0.00150123) \n",
      "Epoch : 262 , Minibatch : 0 Loss = 0.00010201 Val Loss = 0.00148594\n",
      "Epoch 262 : Loss = (0.00148177) \n",
      "Epoch : 263 , Minibatch : 0 Loss = 0.00011200 Val Loss = 0.00146842\n",
      "Epoch 263 : Loss = (0.00146338) \n",
      "Epoch : 264 , Minibatch : 0 Loss = 0.00009441 Val Loss = 0.00145206\n",
      "Epoch 264 : Loss = (0.00144508) \n",
      "Epoch : 265 , Minibatch : 0 Loss = 0.00010639 Val Loss = 0.00143263\n",
      "Epoch 265 : Loss = (0.00142640) \n",
      "Epoch : 266 , Minibatch : 0 Loss = 0.00008532 Val Loss = 0.00141504\n",
      "Epoch 266 : Loss = (0.00141305) \n",
      "Epoch : 267 , Minibatch : 0 Loss = 0.00009313 Val Loss = 0.00139812\n",
      "Epoch 267 : Loss = (0.00139561) \n",
      "Epoch : 268 , Minibatch : 0 Loss = 0.00008845 Val Loss = 0.00138250\n",
      "Epoch 268 : Loss = (0.00137836) \n",
      "Epoch : 269 , Minibatch : 0 Loss = 0.00009891 Val Loss = 0.00136539\n",
      "Epoch 269 : Loss = (0.00136283) \n",
      "Epoch : 270 , Minibatch : 0 Loss = 0.00009301 Val Loss = 0.00134894\n",
      "Epoch 270 : Loss = (0.00134373) \n",
      "Epoch : 271 , Minibatch : 0 Loss = 0.00008976 Val Loss = 0.00133255\n",
      "Epoch 271 : Loss = (0.00133061) \n",
      "Epoch : 272 , Minibatch : 0 Loss = 0.00009835 Val Loss = 0.00131735\n",
      "Epoch 272 : Loss = (0.00131249) \n",
      "Epoch : 273 , Minibatch : 0 Loss = 0.00009024 Val Loss = 0.00130096\n",
      "Epoch 273 : Loss = (0.00130063) \n",
      "Epoch : 274 , Minibatch : 0 Loss = 0.00008363 Val Loss = 0.00128689\n",
      "Epoch 274 : Loss = (0.00128388) \n",
      "Epoch : 275 , Minibatch : 0 Loss = 0.00008035 Val Loss = 0.00127411\n",
      "Epoch 275 : Loss = (0.00127217) \n",
      "Epoch : 276 , Minibatch : 0 Loss = 0.00008529 Val Loss = 0.00125867\n",
      "Epoch 276 : Loss = (0.00125417) \n",
      "Epoch : 277 , Minibatch : 0 Loss = 0.00008950 Val Loss = 0.00124407\n",
      "Epoch 277 : Loss = (0.00124007) \n",
      "Epoch : 278 , Minibatch : 0 Loss = 0.00008327 Val Loss = 0.00122944\n",
      "Epoch 278 : Loss = (0.00122640) \n",
      "Epoch : 279 , Minibatch : 0 Loss = 0.00008163 Val Loss = 0.00121352\n",
      "Epoch 279 : Loss = (0.00121033) \n",
      "Epoch : 280 , Minibatch : 0 Loss = 0.00007206 Val Loss = 0.00120270\n",
      "Epoch 280 : Loss = (0.00119743) \n",
      "Epoch : 281 , Minibatch : 0 Loss = 0.00008628 Val Loss = 0.00118747\n",
      "Epoch 281 : Loss = (0.00118303) \n",
      "Epoch : 282 , Minibatch : 0 Loss = 0.00007010 Val Loss = 0.00117320\n",
      "Epoch 282 : Loss = (0.00117153) \n",
      "Epoch : 283 , Minibatch : 0 Loss = 0.00007951 Val Loss = 0.00116244\n",
      "Epoch 283 : Loss = (0.00115907) \n",
      "Epoch : 284 , Minibatch : 0 Loss = 0.00008053 Val Loss = 0.00114813\n",
      "Epoch 284 : Loss = (0.00114667) \n",
      "Epoch : 285 , Minibatch : 0 Loss = 0.00007311 Val Loss = 0.00113621\n",
      "Epoch 285 : Loss = (0.00113440) \n",
      "Epoch : 286 , Minibatch : 0 Loss = 0.00008205 Val Loss = 0.00112510\n",
      "Epoch 286 : Loss = (0.00112271) \n",
      "Epoch : 287 , Minibatch : 0 Loss = 0.00008097 Val Loss = 0.00110963\n",
      "Epoch 287 : Loss = (0.00110972) \n",
      "Epoch : 288 , Minibatch : 0 Loss = 0.00007766 Val Loss = 0.00109974\n",
      "Epoch 288 : Loss = (0.00109610) \n",
      "Epoch : 289 , Minibatch : 0 Loss = 0.00008398 Val Loss = 0.00108981\n",
      "Epoch 289 : Loss = (0.00108710) \n",
      "Epoch : 290 , Minibatch : 0 Loss = 0.00007245 Val Loss = 0.00107470\n",
      "Epoch 290 : Loss = (0.00107422) \n",
      "Epoch : 291 , Minibatch : 0 Loss = 0.00007245 Val Loss = 0.00106549\n",
      "Epoch 291 : Loss = (0.00106144) \n",
      "Epoch : 292 , Minibatch : 0 Loss = 0.00006473 Val Loss = 0.00105199\n",
      "Epoch 292 : Loss = (0.00105134) \n",
      "Epoch : 293 , Minibatch : 0 Loss = 0.00006738 Val Loss = 0.00104222\n",
      "Epoch 293 : Loss = (0.00103649) \n",
      "Epoch : 294 , Minibatch : 0 Loss = 0.00007477 Val Loss = 0.00102904\n",
      "Epoch 294 : Loss = (0.00102723) \n",
      "Epoch : 295 , Minibatch : 0 Loss = 0.00006923 Val Loss = 0.00101876\n",
      "Epoch 295 : Loss = (0.00101778) \n",
      "Epoch : 296 , Minibatch : 0 Loss = 0.00008103 Val Loss = 0.00100791\n",
      "Epoch 296 : Loss = (0.00100654) \n",
      "Epoch : 297 , Minibatch : 0 Loss = 0.00006056 Val Loss = 0.00099912\n",
      "Epoch 297 : Loss = (0.00099641) \n",
      "Epoch : 298 , Minibatch : 0 Loss = 0.00006014 Val Loss = 0.00098976\n",
      "Epoch 298 : Loss = (0.00098675) \n",
      "Epoch : 299 , Minibatch : 0 Loss = 0.00006521 Val Loss = 0.00097862\n",
      "Epoch 299 : Loss = (0.00097606) \n",
      "Epoch : 300 , Minibatch : 0 Loss = 0.00006610 Val Loss = 0.00096840\n",
      "Epoch 300 : Loss = (0.00096485) \n",
      "Epoch : 301 , Minibatch : 0 Loss = 0.00007623 Val Loss = 0.00095749\n",
      "Epoch 301 : Loss = (0.00095576) \n",
      "Epoch : 302 , Minibatch : 0 Loss = 0.00005925 Val Loss = 0.00094676\n",
      "Epoch 302 : Loss = (0.00094950) \n",
      "Epoch : 303 , Minibatch : 0 Loss = 0.00005785 Val Loss = 0.00093836\n",
      "Epoch 303 : Loss = (0.00093579) \n",
      "Epoch : 304 , Minibatch : 0 Loss = 0.00005567 Val Loss = 0.00092793\n",
      "Epoch 304 : Loss = (0.00092930) \n",
      "Epoch : 305 , Minibatch : 0 Loss = 0.00006345 Val Loss = 0.00092199\n",
      "Epoch 305 : Loss = (0.00091815) \n",
      "Epoch : 306 , Minibatch : 0 Loss = 0.00006604 Val Loss = 0.00091150\n",
      "Epoch 306 : Loss = (0.00091273) \n",
      "Epoch : 307 , Minibatch : 0 Loss = 0.00005782 Val Loss = 0.00090426\n",
      "Epoch 307 : Loss = (0.00090447) \n",
      "Epoch : 308 , Minibatch : 0 Loss = 0.00005540 Val Loss = 0.00089410\n",
      "Epoch 308 : Loss = (0.00089529) \n",
      "Epoch : 309 , Minibatch : 0 Loss = 0.00006148 Val Loss = 0.00088525\n",
      "Epoch 309 : Loss = (0.00088507) \n",
      "Epoch : 310 , Minibatch : 0 Loss = 0.00006050 Val Loss = 0.00087744\n",
      "Epoch 310 : Loss = (0.00087631) \n",
      "Epoch : 311 , Minibatch : 0 Loss = 0.00005230 Val Loss = 0.00086904\n",
      "Epoch 311 : Loss = (0.00086743) \n",
      "Epoch : 312 , Minibatch : 0 Loss = 0.00005615 Val Loss = 0.00085962\n",
      "Epoch 312 : Loss = (0.00086033) \n",
      "Epoch : 313 , Minibatch : 0 Loss = 0.00005731 Val Loss = 0.00085172\n",
      "Epoch 313 : Loss = (0.00085035) \n",
      "Epoch : 314 , Minibatch : 0 Loss = 0.00005540 Val Loss = 0.00084627\n",
      "Epoch 314 : Loss = (0.00084344) \n",
      "Epoch : 315 , Minibatch : 0 Loss = 0.00005978 Val Loss = 0.00083804\n",
      "Epoch 315 : Loss = (0.00083569) \n",
      "Epoch : 316 , Minibatch : 0 Loss = 0.00006065 Val Loss = 0.00083137\n",
      "Epoch 316 : Loss = (0.00082994) \n",
      "Epoch : 317 , Minibatch : 0 Loss = 0.00005379 Val Loss = 0.00082430\n",
      "Epoch 317 : Loss = (0.00082412) \n",
      "Epoch : 318 , Minibatch : 0 Loss = 0.00004995 Val Loss = 0.00081661\n",
      "Epoch 318 : Loss = (0.00081640) \n",
      "Epoch : 319 , Minibatch : 0 Loss = 0.00005168 Val Loss = 0.00080788\n",
      "Epoch 319 : Loss = (0.00080758) \n",
      "Epoch : 320 , Minibatch : 0 Loss = 0.00005901 Val Loss = 0.00080112\n",
      "Epoch 320 : Loss = (0.00080273) \n",
      "Epoch : 321 , Minibatch : 0 Loss = 0.00005031 Val Loss = 0.00079155\n",
      "Epoch 321 : Loss = (0.00079364) \n",
      "Epoch : 322 , Minibatch : 0 Loss = 0.00005108 Val Loss = 0.00078723\n",
      "Epoch 322 : Loss = (0.00078556) \n",
      "Epoch : 323 , Minibatch : 0 Loss = 0.00005081 Val Loss = 0.00078079\n",
      "Epoch 323 : Loss = (0.00078261) \n",
      "Epoch : 324 , Minibatch : 0 Loss = 0.00005630 Val Loss = 0.00077266\n",
      "Epoch 324 : Loss = (0.00077349) \n",
      "Epoch : 325 , Minibatch : 0 Loss = 0.00005335 Val Loss = 0.00076592\n",
      "Epoch 325 : Loss = (0.00076875) \n",
      "Epoch : 326 , Minibatch : 0 Loss = 0.00004691 Val Loss = 0.00076127\n",
      "Epoch 326 : Loss = (0.00075969) \n",
      "Epoch : 327 , Minibatch : 0 Loss = 0.00005427 Val Loss = 0.00075465\n",
      "Epoch 327 : Loss = (0.00075510) \n",
      "Epoch : 328 , Minibatch : 0 Loss = 0.00005028 Val Loss = 0.00074866\n",
      "Epoch 328 : Loss = (0.00074750) \n",
      "Epoch : 329 , Minibatch : 0 Loss = 0.00005141 Val Loss = 0.00074142\n",
      "Epoch 329 : Loss = (0.00074258) \n",
      "Epoch : 330 , Minibatch : 0 Loss = 0.00005230 Val Loss = 0.00073418\n",
      "Epoch 330 : Loss = (0.00073662) \n",
      "Epoch : 331 , Minibatch : 0 Loss = 0.00004542 Val Loss = 0.00072768\n",
      "Epoch 331 : Loss = (0.00073019) \n",
      "Epoch : 332 , Minibatch : 0 Loss = 0.00004861 Val Loss = 0.00072291\n",
      "Epoch 332 : Loss = (0.00072560) \n",
      "Epoch : 333 , Minibatch : 0 Loss = 0.00004721 Val Loss = 0.00071684\n",
      "Epoch 333 : Loss = (0.00071901) \n",
      "Epoch : 334 , Minibatch : 0 Loss = 0.00004840 Val Loss = 0.00071225\n",
      "Epoch 334 : Loss = (0.00071153) \n",
      "Epoch : 335 , Minibatch : 0 Loss = 0.00005010 Val Loss = 0.00070721\n",
      "Epoch 335 : Loss = (0.00070801) \n",
      "Epoch : 336 , Minibatch : 0 Loss = 0.00004712 Val Loss = 0.00070161\n",
      "Epoch 336 : Loss = (0.00070262) \n",
      "Epoch : 337 , Minibatch : 0 Loss = 0.00003880 Val Loss = 0.00069314\n",
      "Epoch 337 : Loss = (0.00069630) \n",
      "Epoch : 338 , Minibatch : 0 Loss = 0.00004846 Val Loss = 0.00069261\n",
      "Epoch 338 : Loss = (0.00068936) \n",
      "Epoch : 339 , Minibatch : 0 Loss = 0.00004458 Val Loss = 0.00068653\n",
      "Epoch 339 : Loss = (0.00068545) \n",
      "Epoch : 340 , Minibatch : 0 Loss = 0.00004870 Val Loss = 0.00067893\n",
      "Epoch 340 : Loss = (0.00068077) \n",
      "Epoch : 341 , Minibatch : 0 Loss = 0.00004154 Val Loss = 0.00067472\n",
      "Epoch 341 : Loss = (0.00067356) \n",
      "Epoch : 342 , Minibatch : 0 Loss = 0.00004441 Val Loss = 0.00066638\n",
      "Epoch 342 : Loss = (0.00066924) \n",
      "Epoch : 343 , Minibatch : 0 Loss = 0.00004435 Val Loss = 0.00066468\n",
      "Epoch 343 : Loss = (0.00066566) \n",
      "Epoch : 344 , Minibatch : 0 Loss = 0.00004336 Val Loss = 0.00066176\n",
      "Epoch 344 : Loss = (0.00066182) \n",
      "Epoch : 345 , Minibatch : 0 Loss = 0.00004339 Val Loss = 0.00065219\n",
      "Epoch 345 : Loss = (0.00065434) \n",
      "Epoch : 346 , Minibatch : 0 Loss = 0.00004277 Val Loss = 0.00064790\n",
      "Epoch 346 : Loss = (0.00065112) \n",
      "Epoch : 347 , Minibatch : 0 Loss = 0.00004411 Val Loss = 0.00064555\n",
      "Epoch 347 : Loss = (0.00064608) \n",
      "Epoch : 348 , Minibatch : 0 Loss = 0.00004536 Val Loss = 0.00063914\n",
      "Epoch 348 : Loss = (0.00064027) \n",
      "Epoch : 349 , Minibatch : 0 Loss = 0.00004405 Val Loss = 0.00063616\n",
      "Epoch 349 : Loss = (0.00063744) \n",
      "Epoch : 350 , Minibatch : 0 Loss = 0.00003943 Val Loss = 0.00062937\n",
      "Epoch 350 : Loss = (0.00063196) \n",
      "Epoch : 351 , Minibatch : 0 Loss = 0.00004235 Val Loss = 0.00062796\n",
      "Epoch 351 : Loss = (0.00062713) \n",
      "Epoch : 352 , Minibatch : 0 Loss = 0.00004712 Val Loss = 0.00062218\n",
      "Epoch 352 : Loss = (0.00062558) \n",
      "Epoch : 353 , Minibatch : 0 Loss = 0.00003675 Val Loss = 0.00062230\n",
      "Epoch 353 : Loss = (0.00062031) \n",
      "Epoch : 354 , Minibatch : 0 Loss = 0.00004333 Val Loss = 0.00061241\n",
      "Epoch 354 : Loss = (0.00061819) \n",
      "Epoch : 355 , Minibatch : 0 Loss = 0.00004223 Val Loss = 0.00060904\n",
      "Epoch 355 : Loss = (0.00061047) \n",
      "Epoch : 356 , Minibatch : 0 Loss = 0.00003892 Val Loss = 0.00060660\n",
      "Epoch 356 : Loss = (0.00060737) \n",
      "Epoch : 357 , Minibatch : 0 Loss = 0.00003910 Val Loss = 0.00060454\n",
      "Epoch 357 : Loss = (0.00060442) \n",
      "Epoch : 358 , Minibatch : 0 Loss = 0.00003681 Val Loss = 0.00059798\n",
      "Epoch 358 : Loss = (0.00059807) \n",
      "Epoch : 359 , Minibatch : 0 Loss = 0.00004140 Val Loss = 0.00059274\n",
      "Epoch 359 : Loss = (0.00059682) \n",
      "Epoch : 360 , Minibatch : 0 Loss = 0.00003991 Val Loss = 0.00059152\n",
      "Epoch 360 : Loss = (0.00058919) \n",
      "Epoch : 361 , Minibatch : 0 Loss = 0.00003943 Val Loss = 0.00058761\n",
      "Epoch 361 : Loss = (0.00058669) \n",
      "Epoch : 362 , Minibatch : 0 Loss = 0.00003830 Val Loss = 0.00058383\n",
      "Epoch 362 : Loss = (0.00058287) \n",
      "Epoch : 363 , Minibatch : 0 Loss = 0.00003630 Val Loss = 0.00057757\n",
      "Epoch 363 : Loss = (0.00057873) \n",
      "Epoch : 364 , Minibatch : 0 Loss = 0.00003982 Val Loss = 0.00057396\n",
      "Epoch 364 : Loss = (0.00057402) \n",
      "Epoch : 365 , Minibatch : 0 Loss = 0.00004041 Val Loss = 0.00057152\n",
      "Epoch 365 : Loss = (0.00056893) \n",
      "Epoch : 366 , Minibatch : 0 Loss = 0.00004470 Val Loss = 0.00056636\n",
      "Epoch 366 : Loss = (0.00056571) \n",
      "Epoch : 367 , Minibatch : 0 Loss = 0.00003490 Val Loss = 0.00056446\n",
      "Epoch 367 : Loss = (0.00056446) \n",
      "Epoch : 368 , Minibatch : 0 Loss = 0.00003558 Val Loss = 0.00056130\n",
      "Epoch 368 : Loss = (0.00055963) \n",
      "Epoch : 369 , Minibatch : 0 Loss = 0.00003681 Val Loss = 0.00055569\n",
      "Epoch 369 : Loss = (0.00055516) \n",
      "Epoch : 370 , Minibatch : 0 Loss = 0.00003362 Val Loss = 0.00055301\n",
      "Epoch 370 : Loss = (0.00055328) \n",
      "Epoch : 371 , Minibatch : 0 Loss = 0.00004101 Val Loss = 0.00054926\n",
      "Epoch 371 : Loss = (0.00055149) \n",
      "Epoch : 372 , Minibatch : 0 Loss = 0.00003225 Val Loss = 0.00054979\n",
      "Epoch 372 : Loss = (0.00054789) \n",
      "Epoch : 373 , Minibatch : 0 Loss = 0.00003389 Val Loss = 0.00054422\n",
      "Epoch 373 : Loss = (0.00054339) \n",
      "Epoch : 374 , Minibatch : 0 Loss = 0.00004223 Val Loss = 0.00054073\n",
      "Epoch 374 : Loss = (0.00053960) \n",
      "Epoch : 375 , Minibatch : 0 Loss = 0.00003701 Val Loss = 0.00053498\n",
      "Epoch 375 : Loss = (0.00053650) \n",
      "Epoch : 376 , Minibatch : 0 Loss = 0.00003642 Val Loss = 0.00053427\n",
      "Epoch 376 : Loss = (0.00053355) \n",
      "Epoch : 377 , Minibatch : 0 Loss = 0.00004199 Val Loss = 0.00053054\n",
      "Epoch 377 : Loss = (0.00053170) \n",
      "Epoch : 378 , Minibatch : 0 Loss = 0.00003433 Val Loss = 0.00052631\n",
      "Epoch 378 : Loss = (0.00052649) \n",
      "Epoch : 379 , Minibatch : 0 Loss = 0.00003549 Val Loss = 0.00052387\n",
      "Epoch 379 : Loss = (0.00052565) \n",
      "Epoch : 380 , Minibatch : 0 Loss = 0.00003099 Val Loss = 0.00051931\n",
      "Epoch 380 : Loss = (0.00052252) \n",
      "Epoch : 381 , Minibatch : 0 Loss = 0.00003132 Val Loss = 0.00051650\n",
      "Epoch 381 : Loss = (0.00051945) \n",
      "Epoch : 382 , Minibatch : 0 Loss = 0.00003153 Val Loss = 0.00051391\n",
      "Epoch 382 : Loss = (0.00051504) \n",
      "Epoch : 383 , Minibatch : 0 Loss = 0.00003469 Val Loss = 0.00051218\n",
      "Epoch 383 : Loss = (0.00051466) \n",
      "Epoch : 384 , Minibatch : 0 Loss = 0.00003549 Val Loss = 0.00051051\n",
      "Epoch 384 : Loss = (0.00051033) \n",
      "Epoch : 385 , Minibatch : 0 Loss = 0.00003213 Val Loss = 0.00050476\n",
      "Epoch 385 : Loss = (0.00050634) \n",
      "Epoch : 386 , Minibatch : 0 Loss = 0.00003079 Val Loss = 0.00050294\n",
      "Epoch 386 : Loss = (0.00050607) \n",
      "Epoch : 387 , Minibatch : 0 Loss = 0.00003123 Val Loss = 0.00049993\n",
      "Epoch 387 : Loss = (0.00049952) \n",
      "Epoch : 388 , Minibatch : 0 Loss = 0.00003192 Val Loss = 0.00049844\n",
      "Epoch 388 : Loss = (0.00050035) \n",
      "Epoch : 389 , Minibatch : 0 Loss = 0.00003296 Val Loss = 0.00049365\n",
      "Epoch 389 : Loss = (0.00049451) \n",
      "Epoch : 390 , Minibatch : 0 Loss = 0.00003597 Val Loss = 0.00049141\n",
      "Epoch 390 : Loss = (0.00049359) \n",
      "Epoch : 391 , Minibatch : 0 Loss = 0.00003567 Val Loss = 0.00048891\n",
      "Epoch 391 : Loss = (0.00048971) \n",
      "Epoch : 392 , Minibatch : 0 Loss = 0.00003141 Val Loss = 0.00048569\n",
      "Epoch 392 : Loss = (0.00048712) \n",
      "Epoch : 393 , Minibatch : 0 Loss = 0.00003442 Val Loss = 0.00048220\n",
      "Epoch 393 : Loss = (0.00048468) \n",
      "Epoch : 394 , Minibatch : 0 Loss = 0.00003418 Val Loss = 0.00048083\n",
      "Epoch 394 : Loss = (0.00048092) \n",
      "Epoch : 395 , Minibatch : 0 Loss = 0.00003082 Val Loss = 0.00047851\n",
      "Epoch 395 : Loss = (0.00047871) \n",
      "Epoch : 396 , Minibatch : 0 Loss = 0.00003400 Val Loss = 0.00047544\n",
      "Epoch 396 : Loss = (0.00047463) \n",
      "Epoch : 397 , Minibatch : 0 Loss = 0.00002721 Val Loss = 0.00047225\n",
      "Epoch 397 : Loss = (0.00047344) \n",
      "Epoch : 398 , Minibatch : 0 Loss = 0.00002831 Val Loss = 0.00046846\n",
      "Epoch 398 : Loss = (0.00046942) \n",
      "Epoch : 399 , Minibatch : 0 Loss = 0.00003025 Val Loss = 0.00046599\n",
      "Epoch 399 : Loss = (0.00046694) \n",
      "Epoch : 400 , Minibatch : 0 Loss = 0.00002947 Val Loss = 0.00046179\n",
      "Epoch 400 : Loss = (0.00046641) \n",
      "Epoch : 401 , Minibatch : 0 Loss = 0.00003079 Val Loss = 0.00046268\n",
      "Epoch 401 : Loss = (0.00046548) \n",
      "Epoch : 402 , Minibatch : 0 Loss = 0.00003082 Val Loss = 0.00045830\n",
      "Epoch 402 : Loss = (0.00046328) \n",
      "Epoch : 403 , Minibatch : 0 Loss = 0.00003374 Val Loss = 0.00045839\n",
      "Epoch 403 : Loss = (0.00045741) \n",
      "Epoch : 404 , Minibatch : 0 Loss = 0.00002584 Val Loss = 0.00045457\n",
      "Epoch 404 : Loss = (0.00045311) \n",
      "Epoch : 405 , Minibatch : 0 Loss = 0.00003076 Val Loss = 0.00045314\n",
      "Epoch 405 : Loss = (0.00045413) \n",
      "Epoch : 406 , Minibatch : 0 Loss = 0.00002921 Val Loss = 0.00044954\n",
      "Epoch 406 : Loss = (0.00045058) \n",
      "Epoch : 407 , Minibatch : 0 Loss = 0.00003111 Val Loss = 0.00044793\n",
      "Epoch 407 : Loss = (0.00044900) \n",
      "Epoch : 408 , Minibatch : 0 Loss = 0.00002873 Val Loss = 0.00044399\n",
      "Epoch 408 : Loss = (0.00044596) \n",
      "Epoch : 409 , Minibatch : 0 Loss = 0.00003272 Val Loss = 0.00044313\n",
      "Epoch 409 : Loss = (0.00044677) \n",
      "Epoch : 410 , Minibatch : 0 Loss = 0.00002769 Val Loss = 0.00043997\n",
      "Epoch 410 : Loss = (0.00044104) \n",
      "Epoch : 411 , Minibatch : 0 Loss = 0.00003186 Val Loss = 0.00043929\n",
      "Epoch 411 : Loss = (0.00043902) \n",
      "Epoch : 412 , Minibatch : 0 Loss = 0.00002477 Val Loss = 0.00043547\n",
      "Epoch 412 : Loss = (0.00043949) \n",
      "Epoch : 413 , Minibatch : 0 Loss = 0.00003159 Val Loss = 0.00043479\n",
      "Epoch 413 : Loss = (0.00043416) \n",
      "Epoch : 414 , Minibatch : 0 Loss = 0.00002587 Val Loss = 0.00043014\n",
      "Epoch 414 : Loss = (0.00043169) \n",
      "Epoch : 415 , Minibatch : 0 Loss = 0.00002921 Val Loss = 0.00042963\n",
      "Epoch 415 : Loss = (0.00043029) \n",
      "Epoch : 416 , Minibatch : 0 Loss = 0.00002804 Val Loss = 0.00042668\n",
      "Epoch 416 : Loss = (0.00042647) \n",
      "Epoch : 417 , Minibatch : 0 Loss = 0.00002825 Val Loss = 0.00042456\n",
      "Epoch 417 : Loss = (0.00042564) \n",
      "Epoch : 418 , Minibatch : 0 Loss = 0.00002691 Val Loss = 0.00042307\n",
      "Epoch 418 : Loss = (0.00042248) \n",
      "Epoch : 419 , Minibatch : 0 Loss = 0.00002736 Val Loss = 0.00042033\n",
      "Epoch 419 : Loss = (0.00042042) \n",
      "Epoch : 420 , Minibatch : 0 Loss = 0.00003362 Val Loss = 0.00041422\n",
      "Epoch 420 : Loss = (0.00042021) \n",
      "Epoch : 421 , Minibatch : 0 Loss = 0.00002557 Val Loss = 0.00041461\n",
      "Epoch 421 : Loss = (0.00041708) \n",
      "Epoch : 422 , Minibatch : 0 Loss = 0.00002992 Val Loss = 0.00041118\n",
      "Epoch 422 : Loss = (0.00041306) \n",
      "Epoch : 423 , Minibatch : 0 Loss = 0.00002754 Val Loss = 0.00041065\n",
      "Epoch 423 : Loss = (0.00041291) \n",
      "Epoch : 424 , Minibatch : 0 Loss = 0.00002676 Val Loss = 0.00040910\n",
      "Epoch 424 : Loss = (0.00041065) \n",
      "Epoch : 425 , Minibatch : 0 Loss = 0.00003043 Val Loss = 0.00040579\n",
      "Epoch 425 : Loss = (0.00040877) \n",
      "Epoch : 426 , Minibatch : 0 Loss = 0.00002876 Val Loss = 0.00040495\n",
      "Epoch 426 : Loss = (0.00040641) \n",
      "Epoch : 427 , Minibatch : 0 Loss = 0.00002632 Val Loss = 0.00040054\n",
      "Epoch 427 : Loss = (0.00040445) \n",
      "Epoch : 428 , Minibatch : 0 Loss = 0.00002646 Val Loss = 0.00039989\n",
      "Epoch 428 : Loss = (0.00040138) \n",
      "Epoch : 429 , Minibatch : 0 Loss = 0.00002849 Val Loss = 0.00039977\n",
      "Epoch 429 : Loss = (0.00039646) \n",
      "Epoch : 430 , Minibatch : 0 Loss = 0.00003120 Val Loss = 0.00039390\n",
      "Epoch 430 : Loss = (0.00039676) \n",
      "Epoch : 431 , Minibatch : 0 Loss = 0.00002369 Val Loss = 0.00039396\n",
      "Epoch 431 : Loss = (0.00039417) \n",
      "Epoch : 432 , Minibatch : 0 Loss = 0.00003037 Val Loss = 0.00039154\n",
      "Epoch 432 : Loss = (0.00039336) \n",
      "Epoch : 433 , Minibatch : 0 Loss = 0.00002739 Val Loss = 0.00038826\n",
      "Epoch 433 : Loss = (0.00039148) \n",
      "Epoch : 434 , Minibatch : 0 Loss = 0.00002763 Val Loss = 0.00038880\n",
      "Epoch 434 : Loss = (0.00039047) \n",
      "Epoch : 435 , Minibatch : 0 Loss = 0.00002435 Val Loss = 0.00038537\n",
      "Epoch 435 : Loss = (0.00038561) \n",
      "Epoch : 436 , Minibatch : 0 Loss = 0.00002655 Val Loss = 0.00038233\n",
      "Epoch 436 : Loss = (0.00038448) \n",
      "Epoch : 437 , Minibatch : 0 Loss = 0.00002635 Val Loss = 0.00038025\n",
      "Epoch 437 : Loss = (0.00038272) \n",
      "Epoch : 438 , Minibatch : 0 Loss = 0.00002614 Val Loss = 0.00037825\n",
      "Epoch 438 : Loss = (0.00038084) \n",
      "Epoch : 439 , Minibatch : 0 Loss = 0.00002408 Val Loss = 0.00037438\n",
      "Epoch 439 : Loss = (0.00037915) \n",
      "Epoch : 440 , Minibatch : 0 Loss = 0.00002629 Val Loss = 0.00037414\n",
      "Epoch 440 : Loss = (0.00037611) \n",
      "Epoch : 441 , Minibatch : 0 Loss = 0.00002182 Val Loss = 0.00037318\n",
      "Epoch 441 : Loss = (0.00037730) \n",
      "Epoch : 442 , Minibatch : 0 Loss = 0.00002047 Val Loss = 0.00037226\n",
      "Epoch 442 : Loss = (0.00037354) \n",
      "Epoch : 443 , Minibatch : 0 Loss = 0.00002554 Val Loss = 0.00037068\n",
      "Epoch 443 : Loss = (0.00037280) \n",
      "Epoch : 444 , Minibatch : 0 Loss = 0.00002366 Val Loss = 0.00036663\n",
      "Epoch 444 : Loss = (0.00036687) \n",
      "Epoch : 445 , Minibatch : 0 Loss = 0.00002453 Val Loss = 0.00036630\n",
      "Epoch 445 : Loss = (0.00036722) \n",
      "Epoch : 446 , Minibatch : 0 Loss = 0.00002313 Val Loss = 0.00036252\n",
      "Epoch 446 : Loss = (0.00036472) \n",
      "Epoch : 447 , Minibatch : 0 Loss = 0.00002587 Val Loss = 0.00036255\n",
      "Epoch 447 : Loss = (0.00036475) \n",
      "Epoch : 448 , Minibatch : 0 Loss = 0.00002253 Val Loss = 0.00036204\n",
      "Epoch 448 : Loss = (0.00036100) \n",
      "Epoch : 449 , Minibatch : 0 Loss = 0.00002521 Val Loss = 0.00035849\n",
      "Epoch 449 : Loss = (0.00035983) \n",
      "Epoch : 450 , Minibatch : 0 Loss = 0.00002450 Val Loss = 0.00035959\n",
      "Epoch 450 : Loss = (0.00035521) \n",
      "Epoch : 451 , Minibatch : 0 Loss = 0.00002289 Val Loss = 0.00035432\n",
      "Epoch 451 : Loss = (0.00035706) \n",
      "Epoch : 452 , Minibatch : 0 Loss = 0.00002483 Val Loss = 0.00035095\n",
      "Epoch 452 : Loss = (0.00035554) \n",
      "Epoch : 453 , Minibatch : 0 Loss = 0.00002283 Val Loss = 0.00035140\n",
      "Epoch 453 : Loss = (0.00035197) \n",
      "Epoch : 454 , Minibatch : 0 Loss = 0.00002244 Val Loss = 0.00034821\n",
      "Epoch 454 : Loss = (0.00035104) \n",
      "Epoch : 455 , Minibatch : 0 Loss = 0.00002173 Val Loss = 0.00034434\n",
      "Epoch 455 : Loss = (0.00034937) \n",
      "Epoch : 456 , Minibatch : 0 Loss = 0.00002339 Val Loss = 0.00034714\n",
      "Epoch 456 : Loss = (0.00034663) \n",
      "Epoch : 457 , Minibatch : 0 Loss = 0.00002220 Val Loss = 0.00034180\n",
      "Epoch 457 : Loss = (0.00034469) \n",
      "Epoch : 458 , Minibatch : 0 Loss = 0.00002337 Val Loss = 0.00034294\n",
      "Epoch 458 : Loss = (0.00034368) \n",
      "Epoch : 459 , Minibatch : 0 Loss = 0.00002578 Val Loss = 0.00033903\n",
      "Epoch 459 : Loss = (0.00034136) \n",
      "Epoch : 460 , Minibatch : 0 Loss = 0.00002772 Val Loss = 0.00033706\n",
      "Epoch 460 : Loss = (0.00034043) \n",
      "Epoch : 461 , Minibatch : 0 Loss = 0.00002384 Val Loss = 0.00033578\n",
      "Epoch 461 : Loss = (0.00033599) \n",
      "Epoch : 462 , Minibatch : 0 Loss = 0.00002140 Val Loss = 0.00033337\n",
      "Epoch 462 : Loss = (0.00033629) \n",
      "Epoch : 463 , Minibatch : 0 Loss = 0.00002101 Val Loss = 0.00033379\n",
      "Epoch 463 : Loss = (0.00033486) \n",
      "Epoch : 464 , Minibatch : 0 Loss = 0.00002423 Val Loss = 0.00033182\n",
      "Epoch 464 : Loss = (0.00033113) \n",
      "Epoch : 465 , Minibatch : 0 Loss = 0.00002185 Val Loss = 0.00032946\n",
      "Epoch 465 : Loss = (0.00033143) \n",
      "Epoch : 466 , Minibatch : 0 Loss = 0.00001937 Val Loss = 0.00032619\n",
      "Epoch 466 : Loss = (0.00032872) \n",
      "Epoch : 467 , Minibatch : 0 Loss = 0.00002202 Val Loss = 0.00032392\n",
      "Epoch 467 : Loss = (0.00032789) \n",
      "Epoch : 468 , Minibatch : 0 Loss = 0.00002119 Val Loss = 0.00032458\n",
      "Epoch 468 : Loss = (0.00032613) \n",
      "Epoch : 469 , Minibatch : 0 Loss = 0.00002140 Val Loss = 0.00032166\n",
      "Epoch 469 : Loss = (0.00032327) \n",
      "Epoch : 470 , Minibatch : 0 Loss = 0.00002015 Val Loss = 0.00032049\n",
      "Epoch 470 : Loss = (0.00031886) \n",
      "Epoch : 471 , Minibatch : 0 Loss = 0.00002164 Val Loss = 0.00031900\n",
      "Epoch 471 : Loss = (0.00031865) \n",
      "Epoch : 472 , Minibatch : 0 Loss = 0.00002086 Val Loss = 0.00031698\n",
      "Epoch 472 : Loss = (0.00032094) \n",
      "Epoch : 473 , Minibatch : 0 Loss = 0.00002223 Val Loss = 0.00031421\n",
      "Epoch 473 : Loss = (0.00031626) \n",
      "Epoch : 474 , Minibatch : 0 Loss = 0.00002140 Val Loss = 0.00031242\n",
      "Epoch 474 : Loss = (0.00031510) \n",
      "Epoch : 475 , Minibatch : 0 Loss = 0.00002018 Val Loss = 0.00031281\n",
      "Epoch 475 : Loss = (0.00031403) \n",
      "Epoch : 476 , Minibatch : 0 Loss = 0.00001809 Val Loss = 0.00030982\n",
      "Epoch 476 : Loss = (0.00031137) \n",
      "Epoch : 477 , Minibatch : 0 Loss = 0.00002077 Val Loss = 0.00030765\n",
      "Epoch 477 : Loss = (0.00030890) \n",
      "Epoch : 478 , Minibatch : 0 Loss = 0.00001785 Val Loss = 0.00030637\n",
      "Epoch 478 : Loss = (0.00030643) \n",
      "Epoch : 479 , Minibatch : 0 Loss = 0.00002000 Val Loss = 0.00030577\n",
      "Epoch 479 : Loss = (0.00030515) \n",
      "Epoch : 480 , Minibatch : 0 Loss = 0.00002280 Val Loss = 0.00030267\n",
      "Epoch 480 : Loss = (0.00030389) \n",
      "Epoch : 481 , Minibatch : 0 Loss = 0.00001955 Val Loss = 0.00030223\n",
      "Epoch 481 : Loss = (0.00030416) \n",
      "Epoch : 482 , Minibatch : 0 Loss = 0.00002381 Val Loss = 0.00030079\n",
      "Epoch 482 : Loss = (0.00030392) \n",
      "Epoch : 483 , Minibatch : 0 Loss = 0.00001913 Val Loss = 0.00029823\n",
      "Epoch 483 : Loss = (0.00029981) \n",
      "Epoch : 484 , Minibatch : 0 Loss = 0.00001997 Val Loss = 0.00029626\n",
      "Epoch 484 : Loss = (0.00029796) \n",
      "Epoch : 485 , Minibatch : 0 Loss = 0.00001800 Val Loss = 0.00029409\n",
      "Epoch 485 : Loss = (0.00029841) \n",
      "Epoch : 486 , Minibatch : 0 Loss = 0.00002220 Val Loss = 0.00029445\n",
      "Epoch 486 : Loss = (0.00029382) \n",
      "Epoch : 487 , Minibatch : 0 Loss = 0.00001919 Val Loss = 0.00029108\n",
      "Epoch 487 : Loss = (0.00029439) \n",
      "Epoch : 488 , Minibatch : 0 Loss = 0.00001869 Val Loss = 0.00028992\n",
      "Epoch 488 : Loss = (0.00029272) \n",
      "Epoch : 489 , Minibatch : 0 Loss = 0.00001910 Val Loss = 0.00029036\n",
      "Epoch 489 : Loss = (0.00029191) \n",
      "Epoch : 490 , Minibatch : 0 Loss = 0.00002053 Val Loss = 0.00028813\n",
      "Epoch 490 : Loss = (0.00028798) \n",
      "Epoch : 491 , Minibatch : 0 Loss = 0.00002190 Val Loss = 0.00028732\n",
      "Epoch 491 : Loss = (0.00028965) \n",
      "Epoch : 492 , Minibatch : 0 Loss = 0.00002044 Val Loss = 0.00028136\n",
      "Epoch 492 : Loss = (0.00028741) \n",
      "Epoch : 493 , Minibatch : 0 Loss = 0.00001803 Val Loss = 0.00028273\n",
      "Epoch 493 : Loss = (0.00028485) \n",
      "Epoch : 494 , Minibatch : 0 Loss = 0.00001892 Val Loss = 0.00028253\n",
      "Epoch 494 : Loss = (0.00028446) \n",
      "Epoch : 495 , Minibatch : 0 Loss = 0.00002038 Val Loss = 0.00027981\n",
      "Epoch 495 : Loss = (0.00028265) \n",
      "Epoch : 496 , Minibatch : 0 Loss = 0.00002107 Val Loss = 0.00027815\n",
      "Epoch 496 : Loss = (0.00028223) \n",
      "Epoch : 497 , Minibatch : 0 Loss = 0.00001690 Val Loss = 0.00027525\n",
      "Epoch 497 : Loss = (0.00027648) \n",
      "Epoch : 498 , Minibatch : 0 Loss = 0.00001696 Val Loss = 0.00027418\n",
      "Epoch 498 : Loss = (0.00027829) \n",
      "Epoch : 499 , Minibatch : 0 Loss = 0.00001690 Val Loss = 0.00027260\n",
      "Epoch 499 : Loss = (0.00027430) \n",
      "Epoch : 500 , Minibatch : 0 Loss = 0.00001901 Val Loss = 0.00027263\n",
      "Epoch 500 : Loss = (0.00027439) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f541af2ce10>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAD4CAYAAACqnDJ3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAfZklEQVR4nO3df7DddX3n8eer9yahtJWEy7VlCTHp5M7ojc1GPYlUp0FDlUQXgmtcb6bW0OJk1oXtdJ06hHG3XVKcMbQ7cZiFtlRokREvNJVy3YoBIXWZEYGbQJBAs7kGDCmsDSaA0TbMhdf+cT5ZT47n3HMuublfgq/HzHfy/X5+vD+fzwmTN9/v9zPnyDYRERHT7eeqnkBERPxsSgKKiIhKJAFFREQlkoAiIqISSUAREVGJ3qoncLI444wzPH/+/KqnERFxUtm+fftztvtb1SUBdWn+/PmMjo5WPY2IiJOKpO+1q8sjuIiIqERXCUjSSkm7JY1J2tCifpakW0v9A5LmN9RdUcp3Szq/U0xJC0qMPSXmzFL+KUmPS3pU0j2S3tTQZ11pv0fSuobyd0j6ThnjGkkq5adLuru0v1vSnMl9bBERcbw6JiBJPcC1wCpgEFgrabCp2SXAIdsLgc3AptJ3EBgCFgErgesk9XSIuQnYbHsAOFRiAzwM1GwvBrYAV5cxTgf+CHgnsAz4o4aE8mfAemCgHCtL+QbgnjLGPeU6IiKmUTd3QMuAMdt7bb8EDAOrm9qsBm4q51uA88rdxmpg2PYR208CYyVey5ilz4oSgxLzIgDb22z/uJR/G5hbzs8H7rZ90PYh4G5gpaQzgTfYvt/17xv64tFYTfO9qaE8IiKmSTcJ6Czg6Ybr/aWsZRvb48ALQN8EfduV9wHPlxjtxoL6XdGdHeZ3VjlvNe9ftv1sme+zwBtbjBERESdQN7vg1KKs+RtM27VpV94q8U3U/icDSR8DasC5r3LsrklaT/0RHvPmzZtM14iI6KCbO6D9wNkN13OBZ9q1kdQLnAYcnKBvu/LngNklxk+NJek3gc8AF9o+0mF++/nJY7rmWN8vj+gof/5zq4Xbvt52zXatv7/lNvaIiHiVuklADwEDZXfaTOqbCkaa2owAR3efrQHuLe9dRoChsktuAfWNAA+2i1n6bCsxKDHvAJD0NuAvqCefxoSxFXi/pDll88H7ga3l0doPJZ1T3i19/GispvmuayiPiIhp0vERnO1xSZdR/4e+B7jR9i5JG4FR2yPADcDNksao3/kMlb67JN0GPA6MA5fafhmgVcwy5OXAsKSrqO98u6GU/wnwi8DflN3U+2xfaPugpD+mntQANto+WM4/Cfw18PPU3xkdfW/0OeA2SZcA+4CPdP2JRUTElFB+kK47tVrN+SaEiIjJkbTddq1VXb4JISIiKpEEFBERlUgCioiISiQBRUREJZKAIiKiEklAERFRiSSgiIioRBJQRERUIgkoIiIqkQQUERGVSAKKiIhKJAFFREQlkoAiIqISSUAREVGJJKCIiKhEElBERFQiCSgiIirRVQKStFLSbkljkja0qJ8l6dZS/4Ck+Q11V5Ty3ZLO7xRT0oISY0+JObOUL5e0Q9K4pDVN42+S9Fg5PtpQfp+kR8rxjKS/K+XvkfRCQ90fdv+RRUTEVOiYgCT1ANcCq4BBYK2kwaZmlwCHbC8ENgObSt9BYAhYBKwErpPU0yHmJmCz7QHgUIkNsA+4GLilaX4fBN4OLAHeCXxa0hsAbP+G7SW2lwD3A19p6Hrf0TrbGzt9DhERMbW6uQNaBozZ3mv7JWAYWN3UZjVwUznfApwnSaV82PYR208CYyVey5ilz4oSgxLzIgDbT9l+FHilaexB4Ju2x23/CNhJPdn9f5J+qcT9uy7WGxER06CbBHQW8HTD9f5S1rKN7XHgBaBvgr7tyvuA50uMdmM12wmsknSqpDOA9wJnN7X5EHCP7Rcbyn5d0k5Jd0pa1GGMiIiYYr1dtFGLMnfZpl15q8Q3Ufu2bN8laSnwLeAA9Udt403N1gJfaLjeAbzJ9mFJH6B+ZzTQHFvSemA9wLx58yaaRkRETFI3d0D7OfaOYi7wTLs2knqB04CDE/RtV/4cMLvEaDfWT7H92fIu533Uk9ieo3WS+qg/8vv7hvYv2j5czr8GzCh3T81xr7dds13r7+/vNI2IiJiEbhLQQ8BA2Z02k/qmgpGmNiPAunK+BrjXtkv5UNklt4D6XcaD7WKWPttKDErMOyaaXNnU0FfOFwOLgbsamnwE+F+2/7Whz6+U901IWlY+hx908VlERMQU6fgIzva4pMuArUAPcKPtXZI2AqO2R4AbgJsljVG/8xkqfXdJug14nPpjsUttvwzQKmYZ8nJgWNJVwMMlNuUx2+3AHOACSVfaXgTMAO4r+eRF4GMN75Aoc/lc07LWAJ+UNA78CzBUkl9EREwT5d/d7tRqNY+OjlY9jYiIk4qk7bZrreryTQgREVGJJKCIiKhEElBERFQiCSgiIiqRBBQREZVIAoqIiEokAUVERCWSgCIiohJJQBERUYkkoIiIqEQSUEREVCIJKCIiKpEEFBERlUgCioiISiQBRUREJZKAIiKiEklAERFRia4SkKSVknZLGpO0oUX9LEm3lvoHJM1vqLuilO+WdH6nmJIWlBh7SsyZpXy5pB2SxiWtaRp/k6THyvHRhvK/lvSkpEfKsaSUS9I1ZexHJb29+48sIiKmQscEJKkHuBZYBQwCayUNNjW7BDhkeyGwGdhU+g4CQ8AiYCVwnaSeDjE3AZttDwCHSmyAfcDFwC1N8/sg8HZgCfBO4NOS3tDQ5NO2l5TjkVK2Chgox3rgzzp9DhERMbW6uQNaBozZ3mv7JWAYWN3UZjVwUznfApwnSaV82PYR208CYyVey5ilz4oSgxLzIgDbT9l+FHilaexB4Ju2x23/CNhJPdlNZDXwRdd9G5gt6cwuPouIiJgi3SSgs4CnG673l7KWbWyPAy8AfRP0bVfeBzxfYrQbq9lOYJWkUyWdAbwXOLuh/rPlMdtmSbMmsaaIiDiBuklAalHmLttMVXlbtu8CvgZ8C/gycD9wNIFdAbwZWAqcDlzeYb7HkLRe0qik0QMHDkw0jYiImKRuEtB+jr2jmAs8066NpF7gNODgBH3blT9H/XFY7wRj/RTbny3veN5HPbnsKeXPlsdsR4C/ov7or9s1Yft62zXbtf7+/k7TiIiISegmAT0EDJTdaTOpbyoYaWozAqwr52uAe227lA+VXXILqL/0f7BdzNJnW4lBiXnHRJMrmxr6yvliYDFwV7k+s/wp6u+SHmuY78fLbrhzgBdsP9vFZxEREVOkt1MD2+OSLgO2Aj3AjbZ3SdoIjNoeAW4AbpY0Rv3OZ6j03SXpNuBx6o/FLrX9MkCrmGXIy4FhSVcBD5fYSFoK3A7MAS6QdKXtRcAM4L56juFF4GMN75C+JKmf+l3RI8B/LOVfAz5AfVPEj4HfmeTnFhERx0n1m47opFareXR0tOppREScVCRtt11rVZdvQoiIiEokAUVERCWSgCIiohJJQBERUYkkoIiIqEQSUEREVCIJKCIiKpEEFBERlUgCioiISiQBRUREJZKAIiKiEklAERFRiSSgiIioRBJQRERUIgkoIiIqkQQUERGVSAKKiIhKdJWAJK2UtFvSmKQNLepnSbq11D8gaX5D3RWlfLek8zvFlLSgxNhTYs4s5csl7ZA0LmlN0/ibJD1Wjo82lH+pjPGYpBslzSjl75H0gqRHyvGH3X9kERExFTomIEk9wLXAKmAQWCtpsKnZJcAh2wuBzcCm0ncQGAIWASuB6yT1dIi5CdhsewA4VGID7AMuBm5pmt8HgbcDS4B3Ap+W9IZS/SXgzcCvAT8PfKKh6322l5RjY6fPISIiplY3d0DLgDHbe22/BAwDq5varAZuKudbgPMkqZQP2z5i+0lgrMRrGbP0WVFiUGJeBGD7KduPAq80jT0IfNP2uO0fATupJztsf80F8CAwt4v1RkTENOgmAZ0FPN1wvb+UtWxjexx4AeiboG+78j7g+RKj3VjNdgKrJJ0q6QzgvcDZjQ3Ko7ffBr7eUPzrknZKulPSog5jRETEFOvtoo1alLnLNu3KWyW+idq3ZfsuSUuBbwEHgPuB8aZm1wH/2/Z95XoH8CbbhyV9APg7YKA5tqT1wHqAefPmTTSNiIiYpG7ugPZz7B3FXOCZdm0k9QKnAQcn6Nuu/DlgdonRbqyfYvuz5V3O+6gnsT1H6yT9EdAPfKqh/Yu2D5fzrwEzyt1Tc9zrbdds1/r7+ztNIyIiJqGbBPQQMFB2p82kvqlgpKnNCLCunK8B7i3vXUaAobJLbgH1u4wH28UsfbaVGJSYd0w0ubKpoa+cLwYWA3eV608A5wNrbb/S0OdXyvsmJC0rn8MPuvgsIiJiinR8BGd7XNJlwFagB7jR9i5JG4FR2yPADcDNksao3/kMlb67JN0GPE79sdiltl8GaBWzDHk5MCzpKuDhEpvymO12YA5wgaQrbS8CZgD3lXzyIvCxhndIfw58D7i/1H+l7HhbA3xS0jjwL8BQSX4RETFNlH93u1Or1Tw6Olr1NCIiTiqSttuutarLNyFEREQlkoAiIqISSUAREVGJJKCIiKhEElBERFQiCSgiIiqRBBQREZVIAoqIiEokAUVERCWSgCIiohJJQBERUYkkoIiIqEQSUEREVCIJKCIiKpEEFBERlUgCioiISiQBRUREJbpKQJJWStotaUzShhb1syTdWuofkDS/oe6KUr5b0vmdYkpaUGLsKTFnlvLlknZIGpe0pmn8TZIeK8dHu4jVdr4RETE9OiYgST3AtcAqYBBYK2mwqdklwCHbC4HNwKbSdxAYAhYBK4HrJPV0iLkJ2Gx7ADhUYgPsAy4Gbmma3weBtwNLgHcCn5b0hg6xWs43IiKmTzd3QMuAMdt7bb8EDAOrm9qsBm4q51uA8ySplA/bPmL7SWCsxGsZs/RZUWJQYl4EYPsp248CrzSNPQh80/a47R8BO4GVE8WaYL4RETFNuklAZwFPN1zvL2Ut29geB14A+ibo2668D3i+xGg3VrOdwCpJp0o6A3gvcHaHWO3mGxER06S3izat7gzcZZt25a0S30Tt27J9l6SlwLeAA8D9wHiHWF2NI2k9sB5g3rx5E00jIiImqZs7oP3U7yiOmgs8066NpF7gNODgBH3blT8HzC4x2o31U2x/1vYS2++jnlz2dIjVbr7Nca+3XbNd6+/v7zSNiIiYhG4S0EPAQNlRNpP6poKRpjYjwLpyvga417ZL+VDZdbYAGAAebBez9NlWYlBi3jHR5Mqmhr5yvhhYDNzVIVa7+UZExDTp+AjO9riky4CtQA9wo+1dkjYCo7ZHgBuAmyWNUb+TGCp9d0m6DXic+mOxS22/DNAqZhnycmBY0lXAwyU25THb7cAc4AJJV9peBMwA7it7CF4EPtbw3qdlrHbzjYiI6aP8j393arWaR0dHq55GRMRJRdJ227VWdfkmhIiIqEQSUEREVCIJKCIiKpEEFBERlUgCioiISiQBRUREJZKAIiKiEklAERFRiSSgiIioRBJQRERUIgkoIiIqkQQUERGVSAKKiIhKJAFFREQlkoAiIqISSUAREVGJJKCIiKhEVwlI0kpJuyWNSdrQon6WpFtL/QOS5jfUXVHKd0s6v1NMSQtKjD0l5sxSvlzSDknjktY0jX+1pF2SnpB0jep+SdIjDcdzkj5f2l8s6UBD3Scm+8FFRMTx6ZiAJPUA1wKrgEFgraTBpmaXAIdsLwQ2A5tK30FgCFgErASuk9TTIeYmYLPtAeBQiQ2wD7gYuKVpfu8C3g0sBt4KLAXOtf1D20uOHsD3gK80dL21of4LnT6HiIiYWt3cAS0Dxmzvtf0SMAysbmqzGripnG8BzpOkUj5s+4jtJ4GxEq9lzNJnRYlBiXkRgO2nbD8KvNI0toFTgJnALGAG8P3GBpIGgDcC93Wx3oiImAbdJKCzgKcbrveXspZtbI8DLwB9E/RtV94HPF9itBvrGLbvB7YBz5Zjq+0nmpqtpX7H44ayD0t6VNIWSWdPNEZEREy9bhKQWpS5yzZTVd5+ctJC4C3AXOrJaoWk5U3NhoAvN1x/FZhvezHwDX5y99Yce72kUUmjBw4cmGgaERExSd0koP1A4x3CXOCZdm0k9QKnAQcn6Nuu/DlgdonRbqxmHwK+bfuw7cPAncA5Rysl/Vug1/b2o2W2f2D7SLn8S+AdrQLbvt52zXatv7+/wzQiImIyuklADwEDZXfaTOp3EyNNbUaAdeV8DXBvedw1AgyVXXILgAHgwXYxS59tJQYl5h0d5rcPOFdSr6QZwLlA4yO4tRx794OkMxsuL2xqHxER06C3UwPb45IuA7YCPcCNtndJ2giM2h4BbgBuljRG/c5nqPTdJek24HFgHLjU9ssArWKWIS8HhiVdBTxcYiNpKXA7MAe4QNKVthdR37CwAvgO9cd1X7f91YYl/AfgA03L+j1JF5Y5HaS+uy4iIqaRjn0vH+3UajWPjo5WPY2IiJOKpO22a63q8k0IERFRiSSgiIioRBJQRERUIgkoIiIqkQQUERGVSAKKiIhKJAFFREQlkoAiIqISSUAREVGJJKCIiKhEElBERFQiCSgiIiqRBBQREZVIAoqIiEokAUVERCWSgCIiohJJQBERUYmuEpCklZJ2SxqTtKFF/SxJt5b6ByTNb6i7opTvlnR+p5iSFpQYe0rMmaV8uaQdksYlrWka/2pJuyQ9IekaSSrl/1DGeKQcb+w034iImB4dE5CkHuBaYBUwCKyVNNjU7BLgkO2FwGZgU+k7CAwBi4CVwHWSejrE3ARstj0AHCqxAfYBFwO3NM3vXcC7gcXAW4GlwLkNTX7L9pJy/PNE842IiOnTzR3QMmDM9l7bLwHDwOqmNquBm8r5FuC8cheyGhi2fcT2k8BYidcyZumzosSgxLwIwPZTth8FXmka28ApwExgFjAD+H6HNbWbb0RETJNuEtBZwNMN1/tLWcs2tseBF4C+Cfq2K+8Dni8x2o11DNv3A9uAZ8ux1fYTDU3+qjx++28NSabdfCMiYpp0k4Ba3Rm4yzZTVd5+ctJC4C3AXOqJZYWk5aX6t2z/GvAb5fjtDvNtjr1e0qik0QMHDkw0jYiImKRuEtB+4OyG67nAM+3aSOoFTgMOTtC3XflzwOwSo91YzT4EfNv2YduHgTuBcwBs/1P584fU3x0t6zDfY9i+3nbNdq2/v7/DNCIiYjK6SUAPAQNld9pM6psKRprajADryvka4F7bLuVDZdfZAmAAeLBdzNJnW4lBiXlHh/ntA86V1CtpBvUNCE+U6zMASvm/Ax7rMN+IiJgmvZ0a2B6XdBmwFegBbrS9S9JGYNT2CHADcLOkMep3EkOl7y5JtwGPA+PApbZfBmgVswx5OTAs6Srg4RIbSUuB24E5wAWSrrS9iPomghXAd6g/Rvu67a9K+gVga0k+PcA3gL8sY7Scb0RETB/lf/y7U6vVPDo6WvU0IiJOKpK22661qss3IURERCWSgCIiohJJQBERUYkkoIiIqEQSUEREVCIJKCIiKpEEFBERlUgCioiISiQBRUREJZKAIiKiEklAERFRiSSgiIioRBJQRERUIgkoIiIqkQQUERGVSAKKiIhKJAFFREQlukpAklZK2i1pTNKGFvWzJN1a6h+QNL+h7opSvlvS+Z1iSlpQYuwpMWeW8uWSdkgal7SmafyrJe2S9ISka1R3qqS/l/SPpe5zDe0vlnRA0iPl+MRkPrSIiDh+HROQpB7gWmAVMAislTTY1OwS4JDthcBmYFPpOwgMAYuAlcB1kno6xNwEbLY9ABwqsQH2ARcDtzTN713Au4HFwFuBpcC5pfpPbb8ZeBvwbkmrGrreantJOb7Q6XOIiIip1c0d0DJgzPZe2y8Bw8DqpjargZvK+RbgPEkq5cO2j9h+Ehgr8VrGLH1WlBiUmBcB2H7K9qPAK01jGzgFmAnMAmYA37f9Y9vbSt+XgB3A3C7WGxER06CbBHQW8HTD9f5S1rKN7XHgBaBvgr7tyvuA50uMdmMdw/b9wDbg2XJstf1EYxtJs4ELgHsaij8s6VFJWySd3Sq2pPWSRiWNHjhwYKJpRETEJHWTgNSizF22mary9pOTFgJvoX53cxawQtLyhvpe4MvANbb3luKvAvNtLwa+wU/u3o4d2L7eds12rb+/f6JpRETEJHWTgPYDjXcIc4Fn2rUp/+CfBhycoG+78ueA2SVGu7GafQj4tu3Dtg8DdwLnNNRfD+yx/fmjBbZ/YPtIufxL4B0dxoiIiCnWTQJ6CBgou9NmUt9UMNLUZgRYV87XAPfadikfKrvkFgADwIPtYpY+20oMSsw7OsxvH3CupF5JM6hvQHgCQNJV1JPh7zd2kHRmw+WFR9tHRMT06ZiAyvuYy4Ct1P+hvs32LkkbJV1Ymt0A9EkaAz4FbCh9dwG3AY8DXwcutf1yu5gl1uXAp0qsvhIbSUsl7Qc+AvyFpKPttwDfBb4D7AR22v6qpLnAZ6jvstvRtN3698rW7J3A71HfXRcREdNI9ZuO6KRWq3l0dLTqaUREnFQkbbdda1WXb0KIiIhKJAFFREQlkoAiIqISSUAREVGJJKCIiKhEElBERFQiCSgiIiqRBBQREZVIAoqIiEokAUVERCWSgCIiohJJQBERUYkkoIiIqEQSUEREVCIJKCIiKpEEFBERlUgCioiISnSVgCStlLRb0pikDS3qZ0m6tdQ/IGl+Q90VpXy3pPM7xZS0oMTYU2LOLOXLJe2QNC5pTdP4V5ef2H5C0jWSVMrfIek7ZYzG8tMl3V3GuFvSnMl9bBERcbw6JiBJPcC1wCpgEFgrabCp2SXAIdsLgc3AptJ3EBgCFgErgesk9XSIuQnYbHsAOFRiA+wDLgZuaZrfu4B3A4uBtwJLgXNL9Z8B64GBcqws5RuAe8oY95TriIiYRt3cAS0Dxmzvtf0SMAysbmqzGripnG8Bzit3G6uBYdtHbD8JjJV4LWOWPitKDErMiwBsP2X7UeCVprENnALMBGYBM4DvSzoTeIPt+20b+OLRWE3zvamhPCIipkk3Cegs4OmG6/2lrGUb2+PAC0DfBH3blfcBz5cY7cY6hu37gW3As+XYavuJ0m9/m3n/su1nS/9ngTe2ii1pvaRRSaMHDhyYaBoRETFJ3SQgtShzl22mqrz95KSFwFuAudQTzApJy19NrJ9qbF9vu2a71t/fP5muERHRQTcJaD9wdsP1XOCZdm0k9QKnAQcn6Nuu/DlgdonRbqxmHwK+bfuw7cPAncA5ZYy5beZ99BEd5c9/7jBGRERMsW4S0EPAQNmdNpP6poKRpjYjwLpyvga4t7x3GQGGyi65BdQ3AjzYLmbps63EoMS8o8P89gHnSuqVNIP6BoQnyqO1H0o6p7xb+nhDrMb5djNGRERMMdX/ze/QSPoA8HmgB7jR9mclbQRGbY9IOgW4GXgb9TufIdt7S9/PAL8LjAO/b/vOdjFL+a9S35RwOvAw8DHbRyQtBW4H5gD/Cvxf24vKjrrrgOXUH7F93fanSqwa8NfAz1O/M/rPti2pD7gNmEc9gX3E9sEOn8EB4HsdP6zXnjOo31n+LMmaX/9+1tYLJ++a32S75TuMrhJQnLwkjdquVT2P6ZQ1v/79rK0XXp9rzjchREREJZKAIiKiEklAr3/XVz2BCmTNr38/a+uF1+Ga8w4oIiIqkTugiIioRBJQRERUIgnodaDbn5eQtK602SNpXYv6EUmPnfgZH7/jWbOkUyX9vaR/LD/j8bnpnX33TsRPobzWvdo1S3qfpO3lJ1i2S1ox3XN/tY7n77nUz5N0WNIfTNecp4TtHCf5AVwNbCjnG4BNLdqcDuwtf84p53Ma6v899Z+6eKzq9ZzoNQOnAu8tbWYC9wGrql5Ti/n3AN8FfrXMcycw2NTmPwF/Xs6HgFvL+WBpPwtYUOL0VL2mE7zmtwH/ppy/FfinqtdzotfcUP+3wN8Af1D1eiZz5A7o9aGbn5c4H7jb9kHbh4C7Kb+PJOkXgU8BV03DXKfKq16z7R/b3gbg+s+B7ODY7w18rTgRP4XyWveq12z7YdtHv+9xF3CKpFnTMuvjczx/z0i6iPr/XO2apvlOmSSg14dufl5iop/V+GPgfwA/PpGTnGLHu2YAJM0GLqD+w4SvNSfip1Be645nzY0+DDxs+8gJmudUetVrlvQLwOXAldMwzynX27lJvBZI+gbwKy2qPtNtiBZllrQEWGj7vzQ/V67aiVpzQ/xe4MvANS7fXfgacyJ+CuW17njWXK+UFlH/ZeX3T+G8TqTjWfOV1H9B+nC5ITqpJAGdJGz/Zrs6Sd+XdKbtZyf4eYn9wHsarucC/wD8OvAOSU9R/+/hjZL+wfZ7qNgJXPNR1wN7bH9+CqZ7Ikzmp1D2d/lTKK91x7NmJM2l/qXFH7f93RM/3SlxPGt+J7BG0tXAbOAVSf9q+3+e+GlPgapfQuU4/gP4E459IX91izanA09Sfwk/p5yf3tRmPifPJoTjWjP1911/C/xc1WuZYI291J/tL+AnL6cXNbW5lGNfTt9Wzhdx7CaEvZwcmxCOZ82zS/sPV72O6VpzU5v/zkm2CaHyCeSYgr/E+vPve4A95c+j/8jWgC80tPtd6i+jx4DfaRHnZEpAr3rN1P8P08ATwCPl+ETVa2qzzg8A/4f6LqnPlLKNwIXl/BTqu5/GqP/W1q829P1M6beb1+Auv6leM/BfgR81/J0+Aryx6vWc6L/nhhgnXQLKV/FEREQlsgsuIiIqkQQUERGVSAKKiIhKJAFFREQlkoAiIqISSUAREVGJJKCIiKjE/wOdGCPTdV4wRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_vae = VAE()\n",
    "model_vae = model_vae.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model_vae.parameters(), lr=1e-4)\n",
    "\n",
    "train_dataset = LatentDataset(model, 1500, 0)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = LatentDataset(model, data_len=100, offset=1500)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "print_per = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    print_loss = 0\n",
    "    loss_record = []\n",
    "    for i, x in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model_vae(x)\n",
    "        \n",
    "        loss = loss_function(recon_batch, x, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        print_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i%print_per == 0):            \n",
    "            # validation\n",
    "            model_vae.eval()\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                val_loss = 0.\n",
    "                for _, val_x in enumerate(val_loader):\n",
    "                    recon_val_x, mu, logvar = model_vae(val_x)\n",
    "                    \n",
    "                    print(recon_val_x.shape)\n",
    "                    \n",
    "                    val_loss += loss_function(recon_val_x, val_x, mu, logvar).item()\n",
    "                    \n",
    "            model_vae.train()\n",
    "            \n",
    "            print(\"Epoch : {} , Minibatch : {} Loss = {:.8f} Val Loss = {:.8f}\".format(epoch+1, i, print_loss, val_loss))\n",
    "            loss_record.append(print_loss)\n",
    "            print_loss = 0\n",
    "            \n",
    "    print(\"Epoch {} : Loss = ({:.8f}) \".format(epoch+1, train_loss))\n",
    "    \n",
    "    \n",
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CMAP = np.asarray([[0, 0, 0], [127, 212, 255], [255, 255, 127], [255, 255, 127], # 'background','skin', 'l_brow', 'r_brow'\n",
    "                    [255, 255, 170], [255, 255, 170], [240, 157, 240], [255, 212, 255], #'l_eye', 'r_eye', 'r_nose', 'l_nose',\n",
    "                    [31, 162, 230], [127, 255, 255], [127, 255, 255], #'mouth', 'u_lip', 'l_lip'\n",
    "                    [0, 255, 85], [0, 255, 85], [0, 255, 170], [255, 255, 170], #'l_ear', 'r_ear', 'ear_r', 'eye_g'\n",
    "                    [127, 170, 255], [85, 0, 255], [255, 170, 127], #'neck', 'neck_l', 'cloth'\n",
    "                    [212, 127, 255], [0, 170, 255]#, 'hair', 'hat'\n",
    "                    ])\n",
    "\n",
    "_CMAP =torch.tensor(_CMAP, dtype=torch.float32) / 255.0\n",
    "\n",
    "\n",
    "def _campos2matrix(cam_pos, cam_center=None, cam_up=None):\n",
    "    _cam_target = np.asarray([0,0.11,0.1]) if cam_center is None else cam_center\n",
    "    _cam_target = _cam_target.reshape((1, 3))\n",
    "    # print('*** cam_center = ', _cam_target.shape)\n",
    "\n",
    "    _cam_up = np.asarray([0.0, 1.0, 0.0]) if cam_up is None else cam_up\n",
    "\n",
    "    cam_dir = (_cam_target-cam_pos)\n",
    "    cam_dir = cam_dir / np.linalg.norm(cam_dir)\n",
    "    cam_right = np.cross(cam_dir, _cam_up)\n",
    "    cam_right = cam_right / np.linalg.norm(cam_right)\n",
    "    cam_up = np.cross(cam_right, cam_dir)\n",
    "\n",
    "    cam_R = np.concatenate([cam_right.T, cam_up.T, cam_dir.T], axis=1)\n",
    "\n",
    "    cam_P = np.eye(4)\n",
    "    cam_P[:3, :3] = cam_R\n",
    "    cam_P[:3, 3] = cam_pos\n",
    "\n",
    "    return cam_P\n",
    "\n",
    "\n",
    "_CAM_INT = np.load(\n",
    "    '/home/anpei/liury/data/facial-data/seg_face_real/intrinsics_avg.npy')\n",
    "\n",
    "_CAM_INT[0, 0] *= (_IMG_SIZE / 512.0)\n",
    "_CAM_INT[1, 1] *= (_IMG_SIZE / 512.0)\n",
    "_CAM_INT[0, 2] *= (_IMG_SIZE / 512.0)\n",
    "_CAM_INT[1, 2] *= (_IMG_SIZE / 512.0)\n",
    "\n",
    "_CAM_INT = torch.from_numpy(_CAM_INT).float().unsqueeze(0)\n",
    "\n",
    "lookat = np.asarray([-1.5, -1.5, 0.0])\n",
    "cam_center =  np.asarray([-1.5, -1.5, 1.5])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "_CAM_POSE = _campos2matrix(cam_center, lookat, cam_up)\n",
    "_CAM_POSE = torch.from_numpy(_CAM_POSE).float().unsqueeze(0)\n",
    "\n",
    "_UV = np.mgrid[0:_IMG_SIZE, 0:_IMG_SIZE].astype(np.int32)\n",
    "_UV = torch.from_numpy(np.flip(_UV, axis=0).copy()).long()\n",
    "_UV = _UV.reshape(2, -1).transpose(1, 0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def render_latent(model, z):\n",
    "    B, L = z.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pose = _CAM_POSE.expand(B, -1, -1)\n",
    "        cam_int = _CAM_INT.expand(B, -1, -1)\n",
    "        uv = _UV.expand(B, -1, -1)\n",
    "\n",
    "        predictions, depth_maps = model(pose, z, cam_int, uv)\n",
    "\n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "\n",
    "        out_img = util.lin2img(pred, color_map=_CMAP).cpu().numpy()\n",
    "        out_seg = pred.view(img_sidelength, img_sidelength, 1).cpu().numpy()\n",
    "\n",
    "        out_img = (out_img.squeeze().transpose(1, 2, 0)) * 255.0\n",
    "        out_img = out_img.round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        out_seg = out_seg.squeeze().astype(np.uint8)\n",
    "\n",
    "        return out_img, out_seg\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
