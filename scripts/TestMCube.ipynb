{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "*** cams: \n",
      "[[300   0  64]\n",
      " [  0 300  64]\n",
      " [  0   0   1]]\n",
      "[[ 1. -0.  0.  0.]\n",
      " [-0. -1.  0.  0.]\n",
      " [ 0. -0. -1.  1.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "[INIT embedding] variable.\n",
      "[INIT renderer] FC, with renderer = FC\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SRNsModel:\n\tUnexpected key(s) in state_dict: \"ray_marcher.lstm.linear.4.weight\", \"ray_marcher.lstm.linear.5.weight\", \"ray_marcher.lstm.linear.6.weight\", \"ray_marcher.lstm.bias.4.weight\", \"ray_marcher.lstm.bias.4.bias\", \"ray_marcher.lstm.bias.5.weight\", \"ray_marcher.lstm.bias.5.bias\", \"ray_marcher.lstm.bias.6.weight\", \"ray_marcher.lstm.bias.6.bias\". \n\tsize mismatch for ray_marcher.lstm.linear.3.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for ray_marcher.lstm.bias.3.weight: copying a param with shape torch.Size([256, 3]) from checkpoint, the shape in current model is torch.Size([1, 3]).\n\tsize mismatch for ray_marcher.lstm.bias.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dfbf2d7fb85d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n\u001b[0;32m---> 65\u001b[0;31m                  overwrite_embeddings=False, overwrite_cam=True)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SRNs/util.py\u001b[0m in \u001b[0;36mcustom_load\u001b[0;34m(model, path, discriminator, overwrite_embeddings, overwrite_renderer, overwrite_cam, optimizer)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/srns/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SRNsModel:\n\tUnexpected key(s) in state_dict: \"ray_marcher.lstm.linear.4.weight\", \"ray_marcher.lstm.linear.5.weight\", \"ray_marcher.lstm.linear.6.weight\", \"ray_marcher.lstm.bias.4.weight\", \"ray_marcher.lstm.bias.4.bias\", \"ray_marcher.lstm.bias.5.weight\", \"ray_marcher.lstm.bias.5.bias\", \"ray_marcher.lstm.bias.6.weight\", \"ray_marcher.lstm.bias.6.bias\". \n\tsize mismatch for ray_marcher.lstm.linear.3.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for ray_marcher.lstm.bias.3.weight: copying a param with shape torch.Size([256, 3]) from checkpoint, the shape in current model is torch.Size([1, 3]).\n\tsize mismatch for ray_marcher.lstm.bias.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from modeling import SRNsModel\n",
    "from dataset.face_dataset import _campos2matrix\n",
    "\n",
    "import util\n",
    "from sklearn import mixture\n",
    "import meshplot as mp\n",
    "\n",
    "_RENDERER = 'FC'\n",
    "_ORTHO = False\n",
    "\n",
    "# _MODEL_PATH = '/home/anpei/liury/log/SRNs/072515face_seg_real/checkpoints/epoch_0045_iter_050000.pth'\n",
    "# _OPT_CAM = False\n",
    "# _ORTHO = True\n",
    "# _TOT_NUM_INSTANCES = 1689\n",
    "# focal = 30\n",
    "# cam_center =  np.asarray([0, 0, 2.0])\n",
    "# dpt_range = [0.0, 5.0]\n",
    "\n",
    "_MODEL_PATH = '../log/080320new_lstm/checkpoints//epoch_0184_iter_090000.pth'\n",
    "_OPT_CAM = False\n",
    "_ORTHO = False\n",
    "_TOT_NUM_INSTANCES = 122\n",
    "focal = 300\n",
    "cam_center =  np.asarray([0, 0, 1.0])\n",
    "dpt_range = [0.8, 1.2]\n",
    "\n",
    "_IMG_SIZE = 128\n",
    "_OUT_SIZE = 128\n",
    "\n",
    "_DEFAULT_CAM_INT = np.array([[focal,0,_IMG_SIZE//2],[0,focal,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0, 0, 0])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "_DEFAULT_CAM_POSE =  _campos2matrix(cam_center, lookat, cam_up)\n",
    "\n",
    "print('*** cams: ')\n",
    "print(_DEFAULT_CAM_INT)\n",
    "print(_DEFAULT_CAM_POSE)\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "model = SRNsModel(num_instances=_TOT_NUM_INSTANCES,\n",
    "                  latent_dim=256,\n",
    "                  renderer=_RENDERER,\n",
    "                  tracing_steps=10,\n",
    "                  freeze_networks=True,\n",
    "                  out_channels=20,\n",
    "                  img_sidelength=_IMG_SIZE,\n",
    "                  output_sidelength=_OUT_SIZE,\n",
    "                  opt_cam=_OPT_CAM,\n",
    "                  orthogonal=_ORTHO,\n",
    "                 )\n",
    "\n",
    "util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n",
    "                 overwrite_embeddings=False, overwrite_cam=True)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "_NUM_COMP = 1\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=_NUM_COMP, covariance_type='full', random_state=0)\n",
    "\n",
    "gmm.fit(model.latent_codes.weight.data.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT embedding] variable.\n",
      "[INIT renderer] FC, with renderer = FC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SRNsModel(\n",
       "  (latent_codes): Embedding(221, 256)\n",
       "  (hyper_phi): HyperFC(\n",
       "    (layers): ModuleList(\n",
       "      (0): NewCls(\n",
       "        (hyper_linear): HyperLinear(\n",
       "          (hypo_params): FCBlock(\n",
       "            (net): Sequential(\n",
       "              (0): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_nl): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): NewCls(\n",
       "        (hyper_linear): HyperLinear(\n",
       "          (hypo_params): FCBlock(\n",
       "            (net): Sequential(\n",
       "              (0): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): Linear(in_features=256, out_features=65792, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_nl): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): NewCls(\n",
       "        (hyper_linear): HyperLinear(\n",
       "          (hypo_params): FCBlock(\n",
       "            (net): Sequential(\n",
       "              (0): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): Linear(in_features=256, out_features=65792, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_nl): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ray_marcher): Raymarcher(\n",
       "    (lstm): Linear(\n",
       "      (linear): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=False)\n",
       "      )\n",
       "      (bias): ModuleList(\n",
       "        (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=3, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=3, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=3, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pixel_generator): FCBlock(\n",
       "    (net): Sequential(\n",
       "      (0): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Linear(in_features=256, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (l2_loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from modeling import SRNsModel\n",
    "import util\n",
    "from sklearn import mixture\n",
    "\n",
    "_RENDERER = 'FC'\n",
    "_ORTHO = False\n",
    "\n",
    "_MODEL_PATH = '/home/anpei/liury/log/SRNs/092303face_seg_real_hidden3/checkpoints/epoch_0100_iter_020000.pth'\n",
    "# _MODEL_PATH = '../log/080320new_lstm/checkpoints//epoch_0184_iter_090000.pth'\n",
    "_OPT_CAM = False\n",
    "_ORTHO = True\n",
    "_TOT_NUM_INSTANCES = 221\n",
    "\n",
    "_IMG_SIZE = 128\n",
    "_OUT_SIZE = 128\n",
    "\n",
    "model = SRNsModel(num_instances=_TOT_NUM_INSTANCES,\n",
    "                  latent_dim=256,\n",
    "                  renderer=_RENDERER,\n",
    "                  tracing_steps=10,\n",
    "                  freeze_networks=True,\n",
    "                  out_channels=20,\n",
    "                  img_sidelength=_IMG_SIZE,\n",
    "                  output_sidelength=_OUT_SIZE,\n",
    "                  opt_cam=_OPT_CAM,\n",
    "                  orthogonal=_ORTHO,\n",
    "                 )\n",
    "\n",
    "util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n",
    "                 overwrite_embeddings=False, overwrite_cam=True)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edfff2c0e8c4af187b9a6ef64ac10be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=136, description='ins_id', max=220), FloatSlider(value=0.8, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from volRenderer import render_scene_cam, VolRender, _LABEL, _CMAP\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def render_dpt(ins_id, dpt, vis_cls):\n",
    "    \n",
    "    latent = model.get_embedding({'instance_idx': torch.LongTensor([ins_id]).squeeze().cuda()}).unsqueeze(0)\n",
    "    \n",
    "    out_img, out_seg, prob, dpt_map = render_scene_cam(\n",
    "        model, latent, _DEFAULT_CAM_POSE, _DEFAULT_CAM_INT, _OUT_SIZE, dpt=dpt)\n",
    "    \n",
    "    figure=plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(out_img)\n",
    "    plt.grid(\"off\");\n",
    "    plt.axis(\"off\");\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"depth\")\n",
    "    plt.imshow(dpt_map)\n",
    "    plt.grid(\"off\");\n",
    "    plt.axis(\"off\");\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(_LABEL[vis_cls])\n",
    "    plt.imshow(prob[:, :, vis_cls], cmap=plt.get_cmap('magma'))\n",
    "    plt.grid(\"off\");\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "\n",
    "dpt_slider = FloatSlider(min=dpt_range[0], max=dpt_range[1], step=0.005)\n",
    "cls_slider = IntSlider(min=0, max=len(_LABEL)-1, step=1, value=0)\n",
    "ins_slider = IntSlider(min=0, max=_TOT_NUM_INSTANCES-1, step=1, value=136)\n",
    "    \n",
    "interactive_plot = interactive(render_dpt, ins_id=ins_slider, dpt=dpt_slider, vis_cls=cls_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.get_embedding({'instance_idx': torch.LongTensor([ins_slider.value]).squeeze().cuda()}).unsqueeze(0)\n",
    "_OUT_SIZE = 512\n",
    "renderer = VolRender(\n",
    "    model, z, _DEFAULT_CAM_POSE, _DEFAULT_CAM_INT, dpt_range, \n",
    "    ortho=_ORTHO, level_set=-0.5, resolution=(_OUT_SIZE, _OUT_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "from scipy.stats import norm\n",
    "\n",
    "remap_list = np.array([0,1,2,2,3,3,4,5,6,7,8,9,9,10,11,12,13,14,15,16]).astype('float')\n",
    "def id_remap(seg):\n",
    "    return remap_list[seg.astype('int')]\n",
    "\n",
    "def vis_condition_img(img):\n",
    "    part_colors = [[0, 0, 0], [127, 212, 255], [255, 255, 127], [255, 255, 170],#'skin',1 'eye_brow'2,  'eye'3\n",
    "                    [240, 157, 240], [255, 212, 255], #'r_nose'4, 'l_nose'5\n",
    "                    [31, 162, 230], [127, 255, 255], [127, 255, 255],#'mouth'6, 'u_lip'7,'l_lip'8\n",
    "                    [0, 255, 85], [0, 255, 170], #'ear'9 'ear_r'10\n",
    "                    [255, 255, 170],\n",
    "                    [127, 170, 255], [85, 0, 255], [255, 170, 127], #'neck'11, 'neck_l'12, 'cloth'13\n",
    "                    [212, 127, 255], [0, 170, 255],#, 'hair'14, 'hat'15\n",
    "                    [255, 255, 0], [255, 255, 85], [255, 255, 170],\n",
    "                    [255, 0, 255], [255, 85, 255], [255, 170, 255],\n",
    "                    [0, 255, 255], [85, 255, 255], [170, 255, 255], [100, 150, 200]]\n",
    "    H,W = img.shape\n",
    "    condition_img_color = np.zeros((H,W,3)).astype('uint8')\n",
    "\n",
    "    num_of_class = int(np.max(img))\n",
    "    for pi in range(1, num_of_class + 1):\n",
    "        index = np.where(img == pi)\n",
    "        condition_img_color[index[0], index[1],:] = part_colors[pi]\n",
    "    return condition_img_color\n",
    "\n",
    "def _campos2matrix(cam_pos, cam_center=None, cam_up=None):\n",
    "    _cam_target = np.asarray([0,0.11,0.1]) if cam_center is None else cam_center\n",
    "    _cam_target = _cam_target.reshape((1, 3))\n",
    "    # print('*** cam_center = ', _cam_target.shape)\n",
    "\n",
    "    _cam_up = np.asarray([0.0, 1.0, 0.0]) if cam_up is None else cam_up\n",
    "\n",
    "    cam_dir = (_cam_target-cam_pos)\n",
    "    cam_dir = cam_dir / np.linalg.norm(cam_dir)\n",
    "    cam_right = np.cross(cam_dir, _cam_up)\n",
    "    cam_right = cam_right / np.linalg.norm(cam_right)\n",
    "    cam_up = np.cross(cam_right, cam_dir)\n",
    "\n",
    "    cam_R = np.concatenate([cam_right.T, -cam_up.T, cam_dir.T], axis=1)\n",
    "\n",
    "    cam_P = np.eye(4)\n",
    "    cam_P[:3, :3] = cam_R\n",
    "    cam_P[:3, 3] = cam_pos\n",
    "\n",
    "    return cam_P\n",
    "\n",
    "def render_spiral_path(cam_center,lookat,radii,src_latent,trgt_latent, directions=None):\n",
    "    # ROTATE\n",
    "    R = np.linalg.norm(cam_center-lookat) + radii[0]\n",
    "\n",
    "    theta = []\n",
    "    theta_range = [0.0, -0.45, 0.45, 0.0]\n",
    "    for i in range(len(theta_range)-1):\n",
    "        theta.append( np.linspace(theta_range[i],theta_range[i+1], num=_INTERP_STEPS))\n",
    "#         theta.append(np.logspace(0.0, 1, 10, endpoint=False)[::-1]/100)\n",
    "    theta = np.concatenate(theta)\n",
    "    x = R*np.sin(theta)\n",
    "    y = np.zeros_like(x)\n",
    "    z = R*np.cos(theta)\n",
    "    cam_T = np.stack([x,y,z],axis=1) + lookat.reshape((1,3))\n",
    "\n",
    "    vis_outputs,out_segs = [],[]\n",
    "    for i in range(len(theta)):\n",
    "        cam_pose = _campos2matrix(cam_T[i], lookat)      \n",
    "        out_img, out_seg,_,_ = render_scene_cam(model, src_latent, cam_pose, _DEFAULT_CAM_INT, _OUT_SIZE)  \n",
    "\n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "\n",
    "    # SPPIRAL PATH\n",
    "    t = np.linspace(0, 4*np.pi, _INTERP_STEPS*4, endpoint=True)\n",
    "    for k in range(len(t)):\n",
    "        cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "        cam_T = cam_T[[1,2,0]] + cam_center\n",
    "        cam_pose = _campos2matrix(cam_T, lookat)\n",
    "        out_img, out_seg,_,_ = render_scene_cam(model, src_latent, cam_pose, _DEFAULT_CAM_INT, _OUT_SIZE)  \n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "    init_seg = out_seg.copy()\n",
    "    if directions is not None:\n",
    "        mouse_count = np.sum(init_seg==8)\n",
    "        init_seg[(init_seg>=2)*(init_seg<=10)] = 1\n",
    "        \n",
    "        direction = directions[0] if np.sum(mouse_count)<40 else directions[1]\n",
    "    \n",
    "        smile_latent = src_latent.clone()\n",
    "        steps = [0.3/_INTERP_STEPS]*_INTERP_STEPS*2\n",
    "        for j in steps:\n",
    "            smile_latent += j*direction\n",
    "            out_img, out_seg,_,_ = render_scene_cam(model, smile_latent, cam_pose, _DEFAULT_CAM_INT, _OUT_SIZE)  \n",
    "            mask = (out_seg>=2)*(out_seg<=10)\n",
    "            result = np.array(init_seg)\n",
    "            result[mask] = out_seg[mask]\n",
    "            out_segs.append(result)\n",
    "            vis_outputs.append(vis_condition_img(id_remap(result)))\n",
    "            \n",
    "    length = len(out_segs)\n",
    "    for i in range(_INTERP_STEPS*2):\n",
    "        out_segs.append(out_segs[length-i-1])\n",
    "        vis_outputs.append(vis_outputs[length-i-1])\n",
    "        \n",
    "    cdf_scale = 1.0/(1.0-norm.cdf(-_INTERP_STEPS,0,6)*2)\n",
    "    for idx in range(-_INTERP_STEPS,_INTERP_STEPS+1):\n",
    "        \n",
    "        _w = (norm.cdf(idx,0,6)-norm.cdf(-_INTERP_STEPS,0,6))*cdf_scale\n",
    "        latent = (1.0-_w)*src_latent + _w*trgt_latent\n",
    "        \n",
    "        out_img, out_seg,_,_ = render_scene_cam(model, latent, cam_pose, _DEFAULT_CAM_INT, _OUT_SIZE) \n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "\n",
    "#     for k in range(len(t)):\n",
    "#         cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "#         cam_T = cam_T[[1,2,0]] + cam_center\n",
    "#         cam_pose = _campos2matrix(cam_T, lookat)\n",
    "#         out_img, out_seg,_,_ = render_scene_cam(model, latent, cam_pose, _DEFAULT_CAM_INT, _OUT_SIZE)  \n",
    "#         vis_outputs.append(out_img)\n",
    "#         out_segs.append(out_seg)\n",
    "\n",
    "#     for idx in range(-_INTERP_STEPS,_INTERP_STEPS+1):\n",
    "#         _w = (norm.cdf(idx,0,6)-norm.cdf(-_INTERP_STEPS,0,6))*cdf_scale\n",
    "#         latent = (1.0-_w)*trgt_latent + _w*src_latent\n",
    "#         out_img, out_seg,_,_ = render_scene_cam(model, latent, cam_pose, _DEFAULT_CAM_INT, _OUT_SIZE)\n",
    "#         vis_outputs.append(out_img)\n",
    "#         out_segs.append(out_seg)\n",
    "\n",
    "    length = len(out_segs)\n",
    "    for i in range(_INTERP_STEPS*2):\n",
    "        out_segs.append(out_segs[length-i-1])\n",
    "        vis_outputs.append(vis_outputs[length-i-1])\n",
    "        \n",
    "\n",
    "    return out_segs,vis_outputs\n",
    "\n",
    "def render_spiral_path_withDepth(render, src_latent, trgt_latent=None, dst_latent=None, directions=None):\n",
    "    # ROTATE\n",
    "    R = np.linalg.norm(cam_center-lookat) + radii[0]\n",
    "\n",
    "    theta = []\n",
    "    theta_range = [0.0, -0.55, 0.55, 0.0]\n",
    "    for i in range(len(theta_range)-1):\n",
    "        theta.append( np.linspace(theta_range[i],theta_range[i+1], num=_INTERP_STEPS))\n",
    "\n",
    "    theta = np.concatenate(theta)\n",
    "    x = R*np.sin(theta)\n",
    "    y = np.zeros_like(x)\n",
    "    z = R*np.cos(theta)\n",
    "    cam_T = np.stack([x,y,z],axis=1) + lookat.reshape((1,3))\n",
    "\n",
    "    vis_outputs,out_segs = [],[]\n",
    "    for i in range(len(theta)):\n",
    "        cam_pose = _campos2matrix(cam_T[i], lookat)        \n",
    "        _, out_img_with_init, _, out_seg_with_init, _, _, _ = renderer.render(cam_pose, _DEFAULT_CAM_INT, resolution=[_IMG_SIZE, _IMG_SIZE])\n",
    "\n",
    "        vis_outputs.append(out_img_with_init)\n",
    "        out_segs.append(out_seg_with_init)\n",
    "\n",
    "\n",
    "    # SPPIRAL PATH\n",
    "    t = np.linspace(0, 4*np.pi, _INTERP_STEPS*4, endpoint=True)\n",
    "    for k in range(len(t)):\n",
    "        cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "        cam_T = cam_T[[1,2,0]] + cam_center\n",
    "        cam_pose = _campos2matrix(cam_T, lookat)\n",
    "        _, out_img_with_init, _, out_seg_with_init, _, _, _ = renderer.render(cam_pose, _DEFAULT_CAM_INT, resolution=[_IMG_SIZE, _IMG_SIZE])\n",
    "\n",
    "        vis_outputs.append(out_img_with_init)\n",
    "        out_segs.append(out_seg_with_init)\n",
    "\n",
    "    init_seg = out_seg_with_init.copy()\n",
    "    if directions is not None:\n",
    "        mouse_count = np.sum(init_seg==8)\n",
    "        out_seg_with_init[(init_seg>=2)*(init_seg<=10)] = 1\n",
    "        \n",
    "        direction = directions[0] if np.sum(mouse_count)<40 else directions[1]\n",
    "    \n",
    "        smile_latent = src_latent.clone()\n",
    "        steps = [0.3/_INTERP_STEPS]*_INTERP_STEPS*2 \n",
    "        for j in steps:\n",
    "            smile_latent += j*direction\n",
    "            render = VolRender(model, smile_latent, _DEFAULT_CAM_POSE, _DEFAULT_CAM_INT, dpt_range, \n",
    "                ortho=_ORTHO, level_set=-0.5, resolution=(_OUT_SIZE, _OUT_SIZE))\n",
    "            \n",
    "            _, out_img_with_init, _, out_seg_with_init, _, _, _ = renderer.render(cam_pose, _DEFAULT_CAM_INT, resolution=[_IMG_SIZE, _IMG_SIZE])\n",
    "            mask = (out_seg_with_init>=2)*(out_seg_with_init<=10)\n",
    "            result = np.array(init_seg)\n",
    "            result[mask] = out_seg_with_init[mask]\n",
    "            out_segs.append(result)\n",
    "            \n",
    "            vis_outputs.append(vis_condition_img(id_remap(result)))\n",
    "    \n",
    "    length = len(out_segs)\n",
    "    for i in range(_INTERP_STEPS*2):\n",
    "        out_segs.append(out_segs[length-i-1])\n",
    "        vis_outputs.append(vis_outputs[length-i-1])\n",
    "        \n",
    "    cdf_scale = 1.0/(1.0-norm.cdf(-_INTERP_STEPS,0,6)*2)\n",
    "    for idx in range(-_INTERP_STEPS,_INTERP_STEPS+1):\n",
    "        \n",
    "        _w = (norm.cdf(idx,0,6)-norm.cdf(-_INTERP_STEPS,0,6))*cdf_scale\n",
    "        latent = (1.0-_w)*src_latent + _w*trgt_latent\n",
    "        \n",
    "        render = VolRender(model, latent, _DEFAULT_CAM_POSE, _DEFAULT_CAM_INT, dpt_range, \n",
    "                ortho=_ORTHO, level_set=-0.5, resolution=(_OUT_SIZE, _OUT_SIZE))\n",
    "            \n",
    "        _, out_img_with_init, _, out_seg_with_init, _, _, _ = renderer.render(cam_pose, _DEFAULT_CAM_INT, resolution=[_IMG_SIZE, _IMG_SIZE])\n",
    "        vis_outputs.append(out_img_with_init)\n",
    "        out_segs.append(out_seg_with_init)\n",
    "\n",
    "\n",
    "#     for k in range(len(t)):\n",
    "#         cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "#         cam_T = cam_T[[1,2,0]] + cam_center\n",
    "#         cam_pose = _campos2matrix(cam_T, lookat)\n",
    "#         out_img, out_seg = render_scene_cam(model, cam_pose, trgt_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO)  \n",
    "#         vis_outputs.append(out_img)\n",
    "#         out_segs.append(out_seg)\n",
    "\n",
    "#     for idx in range(-_INTERP_STEPS//2,_INTERP_STEPS//2+1):\n",
    "#         _w = (norm.cdf(idx,0,6)-norm.cdf(-_INTERP_STEPS//2,0,6))*cdf_scale\n",
    "#         latent = (1.0-_w)*trgt_latent + _w*src_latent\n",
    "#         out_img, out_seg = render_scene_cam(model, cam_pose, latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO) \n",
    "#         vis_outputs.append(out_img)\n",
    "#         out_segs.append(out_seg)\n",
    "\n",
    "\n",
    "    length = len(out_segs)\n",
    "    for i in range(_INTERP_STEPS*2):\n",
    "        out_segs.append(out_segs[length-i-1])\n",
    "        vis_outputs.append(vis_outputs[length-i-1])\n",
    "        \n",
    "    return out_segs,vis_outputs\n",
    "\n",
    "_OUT_SIZE = 128\n",
    "_IMG_SIZE = 128\n",
    "f = 480*(_IMG_SIZE/128)\n",
    "_DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0, 0.0, 0.0])\n",
    "cam_center =  np.asarray([0, 0.0, 1.0])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "radii, focus_depth = np.asarray([0.1,0.12,0.1]), 4.5 # z,x,y\n",
    "\n",
    "\n",
    "\n",
    "num_comp = 16\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=num_comp, covariance_type='full')\n",
    "gmm.fit(model.latent_codes.weight.data.cpu().numpy())\n",
    "\n",
    "_LOG_ROOT = '/mnt/data/new_disk/chenap/dataset/mv'\n",
    "smile_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/happiness_dir.npy')).float()\n",
    "neutral_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/neutral_dir.npy')).float()\n",
    "directions = torch.stack((smile_dir,neutral_dir),dim=0)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    src_latent = torch.from_numpy(gmm.sample(1)[0]).float()\n",
    "    dst_latent = torch.from_numpy(gmm.sample(1)[0]).float()\n",
    "    \n",
    "#     z = model.get_embedding({'instance_idx': torch.LongTensor([ins_slider.value]).squeeze().cuda()}).unsqueeze(0)\n",
    "\n",
    "#     render = VolRender(\n",
    "#         model, src_latent, _DEFAULT_CAM_POSE, _DEFAULT_CAM_INT, dpt_range, \n",
    "#         ortho=_ORTHO, level_set=-0.5, resolution=(_OUT_SIZE, _OUT_SIZE))\n",
    "\n",
    "#     out_segs,vis_outputs = render_spiral_path_withDepth(render, src_latent, dst_latent, directions=directions)\n",
    "\n",
    "    out_segs,vis_outputs = render_spiral_path(cam_center,lookat,radii,src_latent,dst_latent, directions=directions)\n",
    "    \n",
    "    # save data\n",
    "    os.makedirs(os.path.join(_LOG_ROOT, '%03d'%i),exist_ok=True)\n",
    "    os.makedirs(os.path.join(_LOG_ROOT, 'vis'),exist_ok=True)\n",
    "    for k,out_seg in enumerate(out_segs):\n",
    "        output_fp = os.path.join(_LOG_ROOT, '%03d'%i,'%03d.png'%k)\n",
    "        util.write_img(out_seg, output_fp)\n",
    "    \n",
    "    vis_fp = output_fp = os.path.join(_LOG_ROOT, 'vis','%03d.gif'%i)\n",
    "    imageio.mimsave(vis_fp, vis_outputs, fps=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/happiness_dir.npy')).float().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Surface level must be within volume data range.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-387f377df187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m renderer = VolRender(\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DEFAULT_CAM_POSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DEFAULT_CAM_INT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpt_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     ortho=_ORTHO, level_set=-0.5, resolution=(_OUT_SIZE, _OUT_SIZE))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SRNs/scripts/volRenderer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, latent, cam_P, cam_K, dpt_range, ortho, level_set, resolution)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mortho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mortho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_mesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SRNs/scripts/volRenderer.py\u001b[0m in \u001b[0;36mbuild_mesh\u001b[0;34m(self, vis, export_fp)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mvol_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_vol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarching_cubes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvol_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvol_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/srns/lib/python3.7/site-packages/skimage/measure/_marching_cubes_lewiner.py\u001b[0m in \u001b[0;36mmarching_cubes\u001b[0;34m(volume, level, spacing, gradient_direction, step_size, allow_degenerate, method, mask)\u001b[0m\n\u001b[1;32m    127\u001b[0m         return _marching_cubes_lewiner(volume, level, spacing,\n\u001b[1;32m    128\u001b[0m                                        \u001b[0mgradient_direction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                                        allow_degenerate, use_classic=False, mask=mask)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lorensen'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         return _marching_cubes_lewiner(volume, level, spacing,\n",
      "\u001b[0;32m~/anaconda3/envs/srns/lib/python3.7/site-packages/skimage/measure/_marching_cubes_lewiner.py\u001b[0m in \u001b[0;36m_marching_cubes_lewiner\u001b[0;34m(volume, level, spacing, gradient_direction, step_size, allow_degenerate, use_classic, mask)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvolume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mvolume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Surface level must be within volume data range.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;31m# spacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Surface level must be within volume data range."
     ]
    }
   ],
   "source": [
    "z = model.get_embedding({'instance_idx': torch.LongTensor([91]).squeeze().cuda()}).unsqueeze(0)\n",
    "_OUT_SIZE = 256\n",
    "dpt_range = [0.83,1.15]\n",
    "_DEFAULT_CAM_INT = np.array([[400*_OUT_SIZE/128,0,_OUT_SIZE//2],[0,400*_OUT_SIZE/128,_OUT_SIZE//2],[0,0,1]])\n",
    "renderer = VolRender(\n",
    "    model, z, _DEFAULT_CAM_POSE, _DEFAULT_CAM_INT, dpt_range, \n",
    "    ortho=_ORTHO, level_set=-0.5, resolution=(_OUT_SIZE, _OUT_SIZE))\n",
    "p = mp.plot(renderer.world_v, renderer.f, 1.0-renderer.color,return_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_steps = np.linspace(1.1,3.0, num=8)\n",
    "for i,d in enumerate(depth_steps):\n",
    "    cam_center =  np.asarray([0, 0.0, d])\n",
    "    _DEFAULT_CAM_INT = np.array([[400*_OUT_SIZE/128+200*i,0,_OUT_SIZE//2],[0,400*_OUT_SIZE/128+200*i,_OUT_SIZE//2],[0,0,1]])\n",
    "    cam_P = _campos2matrix(cam_center, lookat)   \n",
    "\n",
    "    out_seg, out_seg_with_init, pred_dpt, pred_dpt_with_init, depth_piror = renderer.render(cam_P, _DEFAULT_CAM_INT, resolution=[_OUT_SIZE, _OUT_SIZE])\n",
    "    depth_piror[depth_piror==0] = 10\n",
    "    \n",
    "    depth = np.concatenate((pred_dpt, pred_dpt_with_init, depth_piror),axis=1)\n",
    "    depth_min, depth_max = np.min(depth[depth>0]), np.max(depth[depth>0])\n",
    "    depth = (depth-cam_center[2]+0.25)/0.3\n",
    "    depth_image = np.round(depth*255).astype('uint8')\n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=1.0), cv2.COLORMAP_JET)\n",
    "    \n",
    "    result = np.concatenate((out_seg,out_seg_with_init,depth_colormap),axis=1)\n",
    "    cv2.imwrite('/home/anpei/Desktop/softgan_test/result/our_marching_cube_%03d.png'%i,result[...,::-1])\n",
    "#     plt.imshow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1.1\n",
    "theta = []\n",
    "theta_range = [-1.0, 1.0]\n",
    "for i in range(len(theta_range)-1):\n",
    "    theta.append( np.linspace(theta_range[i],theta_range[i+1], num=5, endpoint=True))\n",
    "theta = np.concatenate(theta)\n",
    "x = R*np.sin(theta)\n",
    "y = np.zeros_like(x)\n",
    "z = R*np.cos(theta)\n",
    "cam_T = np.stack([x,y,z],axis=1) + lookat.reshape((1,3))\n",
    "_DEFAULT_CAM_INT = np.array([[450*_OUT_SIZE/128,0,_OUT_SIZE//2],[0,450*_OUT_SIZE/128,_OUT_SIZE//2],[0,0,1]])\n",
    "\n",
    "vis_outputs = []\n",
    "for i in range(len(theta)):\n",
    "    cam_P = _campos2matrix(cam_T[i], lookat)   \n",
    "    out_seg, out_seg_with_init, pred_dpt, pred_dpt_with_init, depth_piror = renderer.render(cam_P, _DEFAULT_CAM_INT, resolution=[_OUT_SIZE, _OUT_SIZE])\n",
    "\n",
    "    cv2.imwrite('/home/anpei/Desktop/softgan_test/result/our_segmap_%03d.png'%i,out_seg[...,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ,  0. ,  0. ],\n",
       "       [-0. , -1. , -0. , -0. ],\n",
       "       [ 0. ,  0. ,  1. , -1.1],\n",
       "       [ 0. ,  0. ,  0. ,  1. ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_P = _campos2matrix(cam_T[0], lookat)   \n",
    "render_cam_P = np.array(cam_P)\n",
    "render_cam_P[:,2] = -render_cam_P[:,2]\n",
    "render_cam_P = np.linalg.inv(render_cam_P)\n",
    "render_cam_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh,pyrender\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def renderDepth_ortho(mesh,cam=np.eye(4),model=np.eye(4),scale=[1,1],resolution=[720,1280]):\n",
    "\n",
    "    obj = pyrender.Mesh.from_trimesh(mesh, smooth=False)\n",
    "\n",
    "    scene = pyrender.Scene(ambient_light=[1.0, 1.0, 1.0], bg_color=[0, 0, 0])\n",
    "    camera = pyrender.OrthographicCamera(xmag=1, ymag=resolution[1]/2/scale[1], znear=1e-3, zfar=10000.0)\n",
    "    light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=2.0)\n",
    "\n",
    "    scene.add(obj, pose=  model)\n",
    "    scene.add(light, pose=  model)\n",
    "    scene.add(camera, pose = cam)\n",
    "    \n",
    "    # render scene\n",
    "    r = pyrender.OffscreenRenderer(resolution[1], resolution[0])\n",
    "    color, depth = r.render(scene)\n",
    "    \n",
    "    return color, depth\n",
    "\n",
    "def render_perp(mesh,fy,cam=np.eye(4),model=np.eye(4),resolution=[512,512]):\n",
    "    \n",
    "    obj = pyrender.Mesh.from_trimesh(mesh, smooth=True)\n",
    "    fovY = 2*np.arctan(resolution[0]/2/fy)\n",
    "    # compose scene\n",
    "    scene = pyrender.Scene(ambient_light=[0.1, 0.1, 0.1], bg_color=[0, 0, 0])\n",
    "    camera = pyrender.PerspectiveCamera( yfov=fovY)\n",
    "    light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=8)\n",
    "\n",
    "    scene.add(obj, pose=  model)\n",
    "    scene.add(light, pose=  model)\n",
    "    scene.add(camera, pose = cam)\n",
    "    \n",
    "    # render scene\n",
    "    r = pyrender.OffscreenRenderer(resolution[1], resolution[0])\n",
    "    color, depth = r.render(scene)\n",
    "    return color,depth\n",
    "\n",
    "def render(mesh,fy,cam=np.eye(4),model=np.eye(4),resolution=[512,512]):\n",
    "\n",
    "    obj = pyrender.Mesh.from_trimesh(mesh, smooth=True)\n",
    "    fovY = 2*np.arctan(resolution[0]/2/fy)\n",
    "    # compose scene\n",
    "    scene = pyrender.Scene(ambient_light=[0.1, 0.1, 0.1], bg_color=[0, 0, 0])\n",
    "    camera = pyrender.PerspectiveCamera( yfov=fovY)\n",
    "    light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=8)\n",
    "\n",
    "    scene.add(obj, pose=  model)\n",
    "    scene.add(light, pose=  model)\n",
    "    scene.add(camera, pose = cam)\n",
    "    \n",
    "    # render scene\n",
    "    r = pyrender.OffscreenRenderer(resolution[1], resolution[0])\n",
    "    color, depth = r.render(scene)\n",
    "    color,depth = color[::-1,::-1],depth[::-1,::-1]\n",
    "    return color,depth\n",
    "\n",
    "def _campos2matrix(cam_pos, cam_center=None, cam_up=None):\n",
    "    _cam_target = np.asarray([0,0.11,0.1]) if cam_center is None else cam_center\n",
    "    _cam_target = _cam_target.reshape((1, 3))\n",
    "    # print('*** cam_center = ', _cam_target.shape)\n",
    "\n",
    "    _cam_up = np.asarray([0.0, 1.0, 0.0]) if cam_up is None else cam_up\n",
    "\n",
    "    cam_dir = (_cam_target-cam_pos)\n",
    "    cam_dir = cam_dir / np.linalg.norm(cam_dir)\n",
    "    cam_right = np.cross(cam_dir, _cam_up)\n",
    "    cam_right = cam_right / np.linalg.norm(cam_right)\n",
    "    cam_up = np.cross(cam_right, cam_dir)\n",
    "\n",
    "    cam_R = np.concatenate([cam_right.T, -cam_up.T, cam_dir.T], axis=1)\n",
    "\n",
    "    cam_P = np.eye(4)\n",
    "    cam_P[:3, :3] = cam_R\n",
    "    cam_P[:3, 3] = cam_pos\n",
    "\n",
    "    return cam_P\n",
    "\n",
    "mesh = trimesh.load('../123.obj')\n",
    "\n",
    "# mesh.vertices -= np.array([0,0.1,0.11]).reshape((1,3))\n",
    "# mesh = trimesh.load('C:/Users/chenap/Desktop/test/0/00000.obj',preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498805284500122]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498284339904785]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1497433185577393]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1496504545211792]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1497087478637695]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1499719619750977]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1502052545547485]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1504024267196655]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1506596803665161]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1507985591888428]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1509833335876465]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1505554914474487]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1499687433242798]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1500885486602783]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.148819088935852]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.10576331615448]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.104229211807251]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.102722406387329]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1013976335525513]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.113299012184143]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.113299012184143]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.102722406387329]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.10576331615448]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1500885486602783]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1505554914474487]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1507985591888428]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1504024267196655]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1499719619750977]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1496504545211792]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498284339904785]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498947143554688]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498947143554688]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498111486434937]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1496378183364868]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1491889953613281]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1492329835891724]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1493085622787476]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1052403450012207]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.103330135345459]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1017448902130127]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1017448902130127]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1033751964569092]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.103330135345459]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1043105125427246]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1052403450012207]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1485108137130737]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1493085622787476]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1494140625]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1492329835891724]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1489307880401611]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1491889953613281]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1494959592819214]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1496378183364868]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.149722933769226]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498111486434937]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498442888259888]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498947143554688]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1499325037002563]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498947143554688]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 1.1498805284500122]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f,_IMG_SIZE = 500,128\n",
    "cam_K = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0, 0.0, 0.0])\n",
    "cam_center =  np.asarray([0, 0.0, 1.0])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "radii, focus_depth = np.asarray([0.1,0.12,0.1]), 4.5 # z,x,y\n",
    "\n",
    "\n",
    "# ROTATE\n",
    "R = np.linalg.norm(cam_center-lookat) + radii[0]\n",
    "\n",
    "theta = []\n",
    "_INTERP_STEPS = 20\n",
    "theta_range = [0.0, -0.55, 0.55, 0.0]\n",
    "for i in range(len(theta_range)-1):\n",
    "    theta.append( np.linspace(theta_range[i],theta_range[i+1], num=_INTERP_STEPS))\n",
    "theta = np.concatenate(theta)\n",
    "x = R*np.sin(theta)\n",
    "y = np.zeros_like(x)\n",
    "z = R*np.cos(theta)\n",
    "cam_T = np.stack([x,y,z],axis=1) + lookat.reshape((1,3))\n",
    "\n",
    "_IMG_SIZE = 128\n",
    "vis_outputs = []\n",
    "for i in range(len(theta)):\n",
    "    cam_P = _campos2matrix(cam_T[i], lookat)   \n",
    "    cam_P[:3, [1,2]] = -cam_P[:3,[1,2]]\n",
    "#     cam_P[:,0] = -cam_P[:,0]\n",
    "#     cam_P[2,3] = -cam_P[2,3]\n",
    "    \n",
    "    cam_P = np.linalg.inv(cam_P)\n",
    "    \n",
    "    color, depth = render_perp(mesh,f,model=cam_P,resolution=[_IMG_SIZE,_IMG_SIZE])\n",
    "#     color = np.zeros((128,128)).astype('uint8')\n",
    "#     point_after = np.dot(cam_P[:3, :3], mesh.vertices.T) + cam_P[:3, [3]]\n",
    "#     point_after = np.dot(cam_K,point_after)\n",
    "#     point_after[:2] = np.round(point_after[:2]/point_after[[2]])\n",
    "    \n",
    "#     mask = (point_after[0]>0)*(point_after[1]>0)*(point_after[0]<_IMG_SIZE)*(point_after[1]<_IMG_SIZE)\n",
    "\n",
    "#     point_after[2] = point_after[2] - np.min(point_after[2,mask])\n",
    "#     point_after[2] = point_after[2]*2000\n",
    "#     point_after = point_after.astype('int')\n",
    "    \n",
    "#     color[point_after[1,mask],point_after[0,mask]] = point_after[2,mask]\n",
    "    \n",
    "    vis_outputs.append(depth)\n",
    "    \n",
    "vis_fp ='../test.gif'\n",
    "imageio.mimsave(vis_fp, vis_outputs, fps=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:/Users/chenap/Desktop/test/render_imgs/z.npy',model.latent_codes.weight.data.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
