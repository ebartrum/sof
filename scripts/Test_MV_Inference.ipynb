{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seg face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT embedding] variable.\n",
      "[INIT renderer] FC, with renderer = FC\n",
      "SOFModel(\n",
      "  (latent_codes): Embedding(221, 256)\n",
      "  (hyper_phi): HyperFC(\n",
      "    (layers): ModuleList(\n",
      "      (0): NewCls(\n",
      "        (hyper_linear): HyperLinear(\n",
      "          (hypo_params): FCBlock(\n",
      "            (net): Sequential(\n",
      "              (0): FCLayer(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (2): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): FCLayer(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (2): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm_nl): Sequential(\n",
      "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): NewCls(\n",
      "        (hyper_linear): HyperLinear(\n",
      "          (hypo_params): FCBlock(\n",
      "            (net): Sequential(\n",
      "              (0): FCLayer(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (2): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): FCLayer(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (2): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=65792, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm_nl): Sequential(\n",
      "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): NewCls(\n",
      "        (hyper_linear): HyperLinear(\n",
      "          (hypo_params): FCBlock(\n",
      "            (net): Sequential(\n",
      "              (0): FCLayer(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (2): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): FCLayer(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (2): ReLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (2): Linear(in_features=256, out_features=65792, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm_nl): Sequential(\n",
      "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ray_marcher): Raymarcher(\n",
      "    (lstm): Linear(\n",
      "      (linear): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (2): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (3): Linear(in_features=256, out_features=1, bias=False)\n",
      "      )\n",
      "      (bias): ModuleList(\n",
      "        (0): Linear(in_features=3, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=3, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=3, out_features=256, bias=True)\n",
      "        (3): Linear(in_features=3, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pixel_generator): FCBlock(\n",
      "    (net): Sequential(\n",
      "      (0): FCLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): FCLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): FCLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (3): Linear(in_features=256, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l2_loss): MSELoss()\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1220330/1296802253.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n\u001b[0m\u001b[1;32m     40\u001b[0m                  overwrite_embeddings=False, overwrite_cam=True)\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/liury/code/facial_details/SOF/util.py\u001b[0m in \u001b[0;36mcustom_load\u001b[0;34m(model, path, discriminator, overwrite_embeddings, overwrite_renderer, overwrite_cam, optimizer)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mwhole_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moverwrite_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "sys.path.append('..')\n",
    "\n",
    "from modeling import SOFModel\n",
    "import util\n",
    "from sklearn import mixture\n",
    "\n",
    "_RENDERER = 'FC'\n",
    "_ORTHO = False\n",
    "\n",
    "_MODEL_PATH = '../checkpoints/epoch_0250_iter_050000.pth'\n",
    "_OPT_CAM = False\n",
    "_ORTHO = True\n",
    "_TOT_NUM_INSTANCES = 221\n",
    "\n",
    "_IMG_SIZE = 128\n",
    "_OUT_SIZE = 128\n",
    "\n",
    "\n",
    "model = SOFModel(num_instances=_TOT_NUM_INSTANCES,\n",
    "                  latent_dim=256,\n",
    "                  renderer=_RENDERER,\n",
    "                  tracing_steps=10,\n",
    "                  freeze_networks=True,\n",
    "                  out_channels=20,\n",
    "                  img_sidelength=_IMG_SIZE,\n",
    "                  output_sidelength=_OUT_SIZE,\n",
    "                  opt_cam=_OPT_CAM,\n",
    "                  orthogonal=_ORTHO,\n",
    "                 )\n",
    "\n",
    "print(model)\n",
    "\n",
    "util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n",
    "                 overwrite_embeddings=False, overwrite_cam=True)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.face_dataset import _campos2matrix\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "_CMAP = np.asarray([[0, 0, 0], [127, 212, 255], [255, 255, 127], [255, 255, 127], # 'background','skin', 'l_brow', 'r_brow'\n",
    "                    [255, 255, 170], [255, 255, 170], [240, 157, 240], [255, 212, 255], #'l_eye', 'r_eye', 'r_nose', 'l_nose',\n",
    "                    [31, 162, 230], [127, 255, 255], [127, 255, 255], #'mouth', 'u_lip', 'l_lip'\n",
    "                    [0, 255, 85], [0, 255, 85], [0, 255, 170], [255, 255, 170], #'l_ear', 'r_ear', 'ear_r', 'eye_g'\n",
    "                    [127, 170, 255], [85, 0, 255], [255, 170, 127], #'neck', 'neck_l', 'cloth'\n",
    "                    [212, 127, 255], [0, 170, 255]#, 'hair', 'hat'\n",
    "                    ])\n",
    "\n",
    "_CMAP =torch.tensor(_CMAP, dtype=torch.float32) / 255.0\n",
    "\n",
    "def _build_cam_int(focal, H, W):\n",
    "    return np.array([  [focal, 0., W // 2, 0.],\n",
    "                       [0., focal, H // 2, 0],\n",
    "                       [0., 0, 1, 0],\n",
    "                       [0, 0, 0, 1]])\n",
    "\n",
    "\n",
    "def render_scene(model, pose, z, focal, img_sidelength):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pose = torch.from_numpy(pose).float().unsqueeze(0)\n",
    "        cam_int = torch.from_numpy(\n",
    "            _build_cam_int(focal, _IMG_SIZE, _IMG_SIZE)).float().unsqueeze(0)\n",
    "\n",
    "        uv = np.mgrid[0:_IMG_SIZE, 0:_IMG_SIZE].astype(np.int32)\n",
    "        uv = torch.from_numpy(np.flip(uv, axis=0).copy()).long()\n",
    "        uv = uv.reshape(2, -1).transpose(1, 0).unsqueeze(0)\n",
    "\n",
    "#         print(pose.shape, cam_int.shape, uv.shape, z.shape)\n",
    "\n",
    "        predictions, depth_maps = model(pose, z, cam_int, uv)\n",
    "\n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "\n",
    "#         print(pred.shape)\n",
    "\n",
    "        out_img = util.lin2img(pred, color_map=_CMAP).cpu().numpy()\n",
    "        out_seg = pred.view(img_sidelength, img_sidelength, 1).cpu().numpy()\n",
    "        \n",
    "        out_img = (out_img.squeeze().transpose(1, 2, 0)) * 255.0\n",
    "        out_img = out_img.round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        out_seg = out_seg.squeeze().astype(np.uint8)\n",
    "        out_seg = morphology.area_closing(out_seg, area_threshold=6000)\n",
    "\n",
    "\n",
    "        return out_img, out_seg\n",
    "    \n",
    "    \n",
    "def render_scene_cam(model, pose, z, cam_int, img_sidelength,orthogonal=True):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pose = torch.from_numpy(pose).float().unsqueeze(0).\n",
    "        cam_int = torch.from_numpy(cam_int).float().unsqueeze(0)\n",
    "        \n",
    "#         print('*** cam_int = ', cam_int)\n",
    "\n",
    "        uv = np.mgrid[0:_IMG_SIZE, 0:_IMG_SIZE].astype(np.int32)\n",
    "        uv = torch.from_numpy(np.flip(uv, axis=0).copy()).long()\n",
    "        uv = uv.reshape(2, -1).transpose(1, 0).unsqueeze(0)\n",
    "\n",
    "#         print(pose.shape, cam_int.shape, uv.shape, z.shape)\n",
    "\n",
    "        predictions, depth_maps = model(pose, z, cam_int, uv, orthogonal=orthogonal)\n",
    "\n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "\n",
    "#         print(pred.shape)\n",
    "\n",
    "        out_img = util.lin2img(pred, color_map=_CMAP).cpu().numpy()\n",
    "        out_seg = pred.view(img_sidelength, img_sidelength, 1).cpu().numpy()\n",
    "        \n",
    "        out_img = (out_img.squeeze().transpose(1, 2, 0)) * 255.0\n",
    "        out_img = out_img.round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "#         output_fp = os.path.join(instance_dir, '%02d_seg.png'%(observation_idx))\n",
    "        out_seg = out_seg.squeeze().astype(np.uint8)\n",
    "\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(out_img)\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(out_seg)\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        return out_img, out_seg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** lookat =  [0. 0. 0.]\n",
      "Logging root =  ../log/mv\n",
      "Processing gmm : 16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1220330/3670432512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m#         out_segs,vis_outputs = render_spiral_path(cam_center,lookat,radii,src_latent,trgt_latent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mout_segs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvis_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_uniform_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_center\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlookat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mradii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_latent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrgt_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1220330/3670432512.py\u001b[0m in \u001b[0;36mrender_uniform_path\u001b[0;34m(cam_center, lookat, radii, src_latent, trgt_latent)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mcam_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_campos2matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_T\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mout_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_scene_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam_pose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DEFAULT_CAM_INT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_OUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morthogonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_ORTHO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mvis_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "# render with spiral path\n",
    "\n",
    "import random,os,imageio\n",
    "from scipy.stats import norm\n",
    "from dataset.face_dataset import _campos2matrix\n",
    "from volRenderer import render_scene_cam\n",
    "\n",
    "def render_spiral_path(cam_center,lookat,radii,src_latent,trgt_latent):\n",
    "    # ROTATE\n",
    "    R = np.linalg.norm(cam_center-lookat) + radii[0]\n",
    "\n",
    "    theta = []\n",
    "    theta_range = [0.0, -0.55, 0.55, 0.0]\n",
    "    for i in range(len(theta_range)-1):\n",
    "        theta.append( np.linspace(theta_range[i],theta_range[i+1], num=_INTERP_STEPS))\n",
    "#         theta.append(np.logspace(0.0, 1, 10, endpoint=False)[::-1]/100)\n",
    "    theta = np.concatenate(theta)\n",
    "    x = R*np.sin(theta)\n",
    "    y = np.zeros_like(x)\n",
    "    z = R*np.cos(theta)\n",
    "    cam_T = np.stack([x,y,z],axis=1) + lookat.reshape((1,3))\n",
    "\n",
    "    vis_outputs,out_segs = [],[]\n",
    "    for i in range(len(theta)):\n",
    "        cam_pose = _campos2matrix(cam_T[i], lookat)        \n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO)  \n",
    "\n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "\n",
    "    # SPPIRAL PATH\n",
    "    t = np.linspace(0, 4*np.pi, _INTERP_STEPS*4, endpoint=True)\n",
    "    for k in range(len(t)):\n",
    "        cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "        cam_T = cam_T[[1,2,0]] + cam_center\n",
    "        cam_pose = _campos2matrix(cam_T, lookat)\n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO)  \n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "    cdf_scale = 1.0/(1.0-norm.cdf(-_INTERP_STEPS//2,0,6)*2)\n",
    "    for idx in range(-_INTERP_STEPS//2,_INTERP_STEPS//2+1):\n",
    "        \n",
    "        _w = (norm.cdf(idx,0,6)-norm.cdf(-_INTERP_STEPS//2,0,6))*cdf_scale\n",
    "        latent = (1.0-_w)*src_latent + _w*trgt_latent\n",
    "        \n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO) \n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "\n",
    "    for k in range(len(t)):\n",
    "        cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "        cam_T = cam_T[[1,2,0]] + cam_center\n",
    "        cam_pose = _campos2matrix(cam_T, lookat)\n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, trgt_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO)  \n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "    for idx in range(-_INTERP_STEPS//2,_INTERP_STEPS//2+1):\n",
    "        _w = (norm.cdf(idx,0,6)-norm.cdf(-_INTERP_STEPS//2,0,6))*cdf_scale\n",
    "        latent = (1.0-_w)*trgt_latent + _w*src_latent\n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO) \n",
    "        vis_outputs.append(out_img)\n",
    "        out_segs.append(out_seg)\n",
    "\n",
    "    return out_segs,vis_outputs\n",
    "\n",
    "def render_uniform_path(cam_center,lookat,radii,src_latent,trgt_latent):\n",
    "    # ROTATE\n",
    "    R = np.linalg.norm(cam_center-lookat) + radii[0]\n",
    "\n",
    "    theta = []\n",
    "    theta_range = [-0.55, 0.55]\n",
    "    for i in range(len(theta_range)-1):\n",
    "        theta.append( np.linspace(theta_range[i],theta_range[i+1], num=_INTERP_STEPS))\n",
    "        \n",
    "    ys = np.linspace(0.3,-0.2,5,endpoint=True)\n",
    "    \n",
    "    theta = np.concatenate(theta)\n",
    "    x = R*np.sin(theta)\n",
    "    z = R*np.cos(theta)\n",
    "    \n",
    "    vis_outputs,out_segs = [],[]\n",
    "    for y in ys:\n",
    "        \n",
    "        cam_T = np.stack([x,np.ones_like(x)*y,z],axis=1) + lookat.reshape((1,3))\n",
    "\n",
    "        for i in range(len(theta)):\n",
    "            cam_pose = _campos2matrix(cam_T[i], lookat)    \n",
    "            out_img, out_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=_ORTHO)  \n",
    "            vis_outputs.append(out_img)\n",
    "            out_segs.append(out_seg)\n",
    "\n",
    "    return out_segs,vis_outputs\n",
    "\n",
    "\n",
    "# f = 30\n",
    "# _DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "# lookat = np.asarray([0,0.0,0.1])\n",
    "# print('*** lookat = ', lookat)\n",
    "\n",
    "# cam_center =  lookat + np.asarray([0, 0, 0.9])\n",
    "# cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "\n",
    "# radii, focus_depth = np.asarray([0.1,0.3,0.2]), 4.5 # z,x,y\n",
    "\n",
    "f = 30\n",
    "_DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0,0.0,0.0])\n",
    "print('*** lookat = ', lookat)\n",
    "\n",
    "cam_center =  lookat + np.asarray([0., 0.0, 4.5])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "\n",
    "radii, focus_depth = np.asarray([0.1,0.3,0.2]), 4.5 # z,x,y\n",
    "\n",
    "_LOG_ROOT = os.path.join('../log/mv')\n",
    "os.makedirs(_LOG_ROOT, exist_ok=True)\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 3\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "random.shuffle(all_instances)\n",
    "\n",
    "\n",
    "_ORTHO=False\n",
    "\n",
    "\n",
    "for num_comp in [16]:#, 2, 4, 8, 16\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=num_comp, covariance_type='full')\n",
    "    gmm.fit(model.latent_codes.weight.data.cpu().numpy())\n",
    "\n",
    "    print('Processing gmm :', num_comp)\n",
    "    \n",
    "    for i in range(1000):\n",
    "\n",
    "#         src_idx = all_instances[i]\n",
    "#         trgt_idx = all_instances[(i+173)%_TOT_NUM_INSTANCES]\n",
    "\n",
    "#         src_latent = model.get_embedding({'instance_idx': torch.LongTensor([src_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "#         trgt_latent = model.get_embedding({'instance_idx': torch.LongTensor([trgt_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        src_latent = torch.from_numpy(gmm.sample(1)[0]).float().cuda()\n",
    "        trgt_latent = torch.from_numpy(gmm.sample(1)[0]).float().cuda()\n",
    "    \n",
    "    \n",
    "        output_dir = os.path.join(\n",
    "            _LOG_ROOT, 'gmm_%02d'%(num_comp), '%03d'%(i))\n",
    "        os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "\n",
    "#         out_segs,vis_outputs = render_spiral_path(cam_center,lookat,radii,src_latent,trgt_latent)\n",
    "        out_segs,vis_outputs = render_uniform_path(cam_center,lookat,radii,src_latent,trgt_latent)\n",
    "        \n",
    "        \n",
    "        for k,out_seg in enumerate(out_segs):\n",
    "            output_fp = os.path.join(output_dir, '%04d.png'%k)\n",
    "            util.write_img(out_seg, output_fp)\n",
    "        \n",
    "#         for k in np.random.randint(0,len(out_segs),6):\n",
    "#             output_fp = os.path.join(os.path.join(_LOG_ROOT, 'subSample', '%02d_%04d.png'%(i,k)))\n",
    "#             util.write_img(out_segs[k], output_fp)        \n",
    "\n",
    "        vis_fp = os.path.join(_LOG_ROOT, 'vis', '%02d_'%(num_comp)+os.path.basename(output_dir)+'.gif')\n",
    "        print('\\t [DONE] save vis to : ', vis_fp)\n",
    "        imageio.mimsave(vis_fp, vis_outputs, fps=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "List = sorted(os.listdir('../log/mv/gmm_16'))\n",
    "os.makedirs(f'/mnt/data/new_disk/chenap/dataset/segmaps/subSample/', exist_ok=True)\n",
    "for folder in List:\n",
    "    ind = np.random.randint(0,15,6)\n",
    "    for i, item in enumerate(ind):\n",
    "        copyfile(f'../log/mv/gmm_16/{folder}/{item:04d}.png', f'/mnt/data/new_disk/chenap/dataset/segmaps//subSample/{folder}_{i:03d}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape eplotion\n",
    "f = 500\n",
    "_DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0,0.0,0.1])\n",
    "print('*** lookat = ', lookat)\n",
    "\n",
    "cam_center =  lookat + np.asarray([0, 0, 0.9])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "\n",
    "radii, focus_depth = np.asarray([0.1,0.3,0.2]), 4.5 # z,x,y\n",
    "\n",
    "\n",
    "_INTERP_STEPS = 20\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "random.shuffle(all_instances)\n",
    "\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=16, covariance_type='full')\n",
    "gmm.fit(model.latent_codes.weight.data.cpu().numpy())\n",
    "\n",
    "cam_center =  lookat + np.asarray([-0.2, 0.0, 1.0])\n",
    "cam_pose = _campos2matrix(cam_center, lookat)\n",
    "\n",
    "_LOG_ROOT = '../log/one_to_more'\n",
    "smile_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/happiness_dir.npy')).float()\n",
    "neutral_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/neutral_dir.npy')).float()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    src_latent = torch.from_numpy(gmm.sample(1)[0]).float()\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    init_img, init_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "    mouse_count = np.sum(init_seg==8)\n",
    "    direction = smile_dir if np.sum(mouse_count)<20 else neutral_dir\n",
    "#     src_latent -= direction*(mouse_count/40)\n",
    "    frames.append(init_seg)\n",
    "\n",
    "    # expression\n",
    "    _INTERP_STEPS = 6\n",
    "    dst_latent = src_latent.clone()\n",
    "    steps = [0.3/_INTERP_STEPS]*_INTERP_STEPS \n",
    "    for j in steps:\n",
    "        dst_latent += direction*j\n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, dst_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "        mask = (out_seg>=2)*(out_seg<=10)\n",
    "\n",
    "        result = np.array(init_seg)\n",
    "        result[(result>=2)*(result<=10)] = 1\n",
    "        result[mask] = out_seg[mask]\n",
    "        frames.append(result)\n",
    "\n",
    "    # shape\n",
    "    for j in range(4):\n",
    "        direction = torch.from_numpy(gmm.sample(1)[0]).float() - src_latent\n",
    "        dst_latent = src_latent.clone()\n",
    "        steps = [1.0/_INTERP_STEPS]*_INTERP_STEPS\n",
    "        for k in steps:\n",
    "            dst_latent += k*direction\n",
    "            out_img, out_seg = render_scene_cam(model, cam_pose, dst_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "            frames.append(out_seg)\n",
    "\n",
    "    output_dir = os.path.join( _LOG_ROOT, '%03d'%i)\n",
    "    os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "    os.makedirs(os.path.join(_LOG_ROOT,'vis'), exist_ok=True)\n",
    "        \n",
    "    for k,out_seg in enumerate(frames):\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%k)\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        frames[k] = vis_condition_img(id_remap(out_seg))\n",
    "            \n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis', '%03d.gif'%i)\n",
    "    print('\\t [DONE] save vis to : ', vis_fp)\n",
    "    imageio.mimsave(vis_fp, frames, fps=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smail animate\n",
    "import random,os,imageio\n",
    "from scipy.stats import norm\n",
    "\n",
    "remap_list = np.array([0,1,2,2,3,3,4,5,6,7,8,9,9,10,11,12,13,14,15,16]).astype('float')\n",
    "def id_remap(seg):\n",
    "    return remap_list[seg.astype('int')]\n",
    "\n",
    "def vis_condition_img(img):\n",
    "    part_colors = [[0, 0, 0], [127, 212, 255], [255, 255, 127], [255, 255, 170],#'skin',1 'eye_brow'2,  'eye'3\n",
    "                    [240, 157, 240], [255, 212, 255], #'r_nose'4, 'l_nose'5\n",
    "                    [31, 162, 230], [127, 255, 255], [127, 255, 255],#'mouth'6, 'u_lip'7,'l_lip'8\n",
    "                    [0, 255, 85], [0, 255, 170], #'ear'9 'ear_r'10\n",
    "                    [255, 255, 170],\n",
    "                    [127, 170, 255], [85, 0, 255], [255, 170, 127], #'neck'11, 'neck_l'12, 'cloth'13\n",
    "                    [212, 127, 255], [0, 170, 255],#, 'hair'14, 'hat'15\n",
    "                    [255, 255, 0], [255, 255, 85], [255, 255, 170],\n",
    "                    [255, 0, 255], [255, 85, 255], [255, 170, 255],\n",
    "                    [0, 255, 255], [85, 255, 255], [170, 255, 255], [100, 150, 200]]\n",
    "    H,W = img.shape\n",
    "    condition_img_color = np.zeros((H,W,3)).astype('uint8')\n",
    "\n",
    "    num_of_class = int(np.max(img))\n",
    "    for pi in range(1, num_of_class + 1):\n",
    "        index = np.where(img == pi)\n",
    "        condition_img_color[index[0], index[1],:] = part_colors[pi]\n",
    "    return condition_img_color\n",
    "\n",
    "f = 500\n",
    "_DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0,0.0,0.1])\n",
    "print('*** lookat = ', lookat)\n",
    "\n",
    "cam_center =  lookat + np.asarray([0, 0, 0.9])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "\n",
    "radii, focus_depth = np.asarray([0.1,0.3,0.2]), 4.5 # z,x,y\n",
    "\n",
    "_LOG_ROOT = os.path.join('../log/test')\n",
    "os.makedirs(_LOG_ROOT, exist_ok=True)\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 20\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "random.shuffle(all_instances)\n",
    "\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=16, covariance_type='full')\n",
    "gmm.fit(model.latent_codes.weight.data.cpu().numpy())\n",
    "\n",
    "cam_pose = _campos2matrix(cam_center, lookat)\n",
    "\n",
    "_LOG_ROOT = '../log/smile'\n",
    "smile_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/happiness_dir.npy')).float()\n",
    "neutral_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/neutral_dir.npy')).float()\n",
    "\n",
    "\n",
    "_INTERP_STEPS = 10\n",
    "for i in range(50):\n",
    "    src_latent = torch.from_numpy(gmm.sample(1)[0]).float()\n",
    "\n",
    "    frames = []\n",
    "\n",
    "#     src_latent -= smile_dir\n",
    "    init_img, init_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "    mouse_count = np.sum(init_seg==8)\n",
    "    init_seg[(init_seg>=2)*(init_seg<=10)] = 1\n",
    "    \n",
    "\n",
    "    direction = smile_dir if np.sum(mouse_count)<20 else neutral_dir\n",
    "    src_latent -= direction*(mouse_count/200)\n",
    "        \n",
    "    steps = [0.8/_INTERP_STEPS]*_INTERP_STEPS + [-1.0/_INTERP_STEPS]*_INTERP_STEPS\n",
    "    for j in steps:\n",
    "        src_latent += j*direction\n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "        mask = (out_seg>=2)*(out_seg<=10)\n",
    "\n",
    "        result = np.array(init_seg)\n",
    "        result[mask] = out_seg[mask]\n",
    "        frames.append(result)\n",
    "\n",
    "    output_dir = os.path.join( _LOG_ROOT, '%03d'%i)\n",
    "    os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "    os.makedirs(os.path.join(_LOG_ROOT,'vis'), exist_ok=True)\n",
    "        \n",
    "    for k,out_seg in enumerate(frames):\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%k)\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        frames[k] = vis_condition_img(id_remap(out_seg))\n",
    "            \n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis', '%03d.gif'%i)\n",
    "    #print('\\t [DONE] save vis to : ', vis_fp)\n",
    "    imageio.mimsave(vis_fp, frames, fps=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hair\n",
    "\n",
    "f = 500\n",
    "_DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0,0.0,0.1])\n",
    "print('*** lookat = ', lookat)\n",
    "\n",
    "cam_center =  lookat + np.asarray([0, 0, 0.9])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "\n",
    "radii, focus_depth = np.asarray([0.1,0.3,0.2]), 4.5 # z,x,y\n",
    "\n",
    "_LOG_ROOT = os.path.join('../log/test')\n",
    "os.makedirs(_LOG_ROOT, exist_ok=True)\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 20\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "random.shuffle(all_instances)\n",
    "\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=16, covariance_type='full')\n",
    "gmm.fit(model.latent_codes.weight.data.cpu().numpy())\n",
    "\n",
    "cam_pose = _campos2matrix(cam_center, lookat)\n",
    "\n",
    "_LOG_ROOT = '../log/hair'\n",
    "smile_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/happiness_dir.npy')).float()*0.8\n",
    "neutral_dir = torch.from_numpy(np.load('/home/anpei/Jack12/MicrosoftAzure/neutral_dir.npy')).float()\n",
    "\n",
    "\n",
    "_INTERP_STEPS = 10\n",
    "for i in range(50):\n",
    "    src_latent = torch.from_numpy(gmm.sample(1)[0]).float()\n",
    "\n",
    "    frames = []\n",
    "\n",
    "#     src_latent -= smile_dir\n",
    "    init_img, init_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "    mouse_count = np.sum(init_seg==8)\n",
    "    mask_init = (init_seg>=2)*(init_seg<=15)*(init_seg!=11)*(init_seg!=12)\n",
    "    \n",
    "\n",
    "    direction = smile_dir if np.sum(mouse_count)<20 else neutral_dir\n",
    "    src_latent -= direction*(mouse_count/200)\n",
    "        \n",
    "    steps = [1.0/_INTERP_STEPS]*_INTERP_STEPS + [-1.0/_INTERP_STEPS]*_INTERP_STEPS\n",
    "    for j in steps:\n",
    "        src_latent += j*direction\n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "        mask = (out_seg>=2)*(out_seg<=10)\n",
    "\n",
    "        out_seg[mask] = 1\n",
    "        out_seg[mask_init] = init_seg[mask_init]\n",
    "        frames.append(out_seg)\n",
    "\n",
    "    output_dir = os.path.join( _LOG_ROOT, '%03d'%i)\n",
    "    os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "    os.makedirs(os.path.join(_LOG_ROOT,'vis'), exist_ok=True)\n",
    "        \n",
    "    for k,out_seg in enumerate(frames):\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%k)\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        frames[k] = vis_condition_img(id_remap(out_seg))\n",
    "            \n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis', '%03d.gif'%i)\n",
    "    #print('\\t [DONE] save vis to : ', vis_fp)\n",
    "    imageio.mimsave(vis_fp, frames, fps=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "\n",
    "f = 500\n",
    "_DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "lookat = np.asarray([0,0.0,0.1])\n",
    "print('*** lookat = ', lookat)\n",
    "\n",
    "cam_center =  lookat + np.asarray([0, 0, 0.9])\n",
    "cam_up = np.asarray([0.0, 1.0, 0.0])\n",
    "\n",
    "\n",
    "radii, focus_depth = np.asarray([0.1,0.3,0.2]), 4.5 # z,x,y\n",
    "\n",
    "_LOG_ROOT = os.path.join('../log/test')\n",
    "os.makedirs(_LOG_ROOT, exist_ok=True)\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 20\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "random.shuffle(all_instances)\n",
    "\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=16, covariance_type='full')\n",
    "gmm.fit(model.latent_codes.weight.data.cpu().numpy())\n",
    "\n",
    "cam_pose = _campos2matrix(cam_center, lookat)\n",
    "\n",
    "_LOG_ROOT = '../log/shape'\n",
    "\n",
    "\n",
    "_INTERP_STEPS = 10\n",
    "for i in range(50):\n",
    "    src_latent = torch.from_numpy(gmm.sample(1)[0]).float()\n",
    "    dst_latent = torch.from_numpy(gmm.sample(1)[0]).float()\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    driection = dst_latent - src_latent\n",
    "    steps = [1.0/_INTERP_STEPS]*_INTERP_STEPS + [-1.0/_INTERP_STEPS]*_INTERP_STEPS\n",
    "    for j in steps:\n",
    "        src_latent += j*direction\n",
    "        out_img, out_seg = render_scene_cam(model, cam_pose, src_latent, _DEFAULT_CAM_INT, _OUT_SIZE, orthogonal=False)\n",
    "        frames.append(out_seg)\n",
    "\n",
    "    output_dir = os.path.join( _LOG_ROOT, '%03d'%i)\n",
    "    os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "    os.makedirs(os.path.join(_LOG_ROOT,'vis'), exist_ok=True)\n",
    "        \n",
    "    for k,out_seg in enumerate(frames):\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%k)\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        frames[k] = vis_condition_img(id_remap(out_seg))\n",
    "            \n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis', '%03d.gif'%i)\n",
    "    #print('\\t [DONE] save vis to : ', vis_fp)\n",
    "    imageio.mimsave(vis_fp, frames, fps=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vis all instances\n",
    "\n",
    "import random,os\n",
    "\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SOF/test', 'custo_070821face_seg_800_imae')\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 2\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "for i in range(1):\n",
    "\n",
    "    lat_idx = [all_instances[i], random.choice(all_instances)]\n",
    "    src_idx = all_instances[i]\n",
    "    \n",
    "    src_latent = model.get_embedding({'instance_idx': torch.LongTensor([src_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "    \n",
    "    cam_K = '../checkpoints/intrinsics.txt'\n",
    "    cam_int = data_util.parse_intrinsics(cam_K, trgt_sidelength=128)\n",
    "    focal = cam_int[0, 0]\n",
    "    cx, cy = cam_int[:2, 2]\n",
    "\n",
    "    cam_center = np.asarray([0., 0.11, 0.1])\n",
    "    cam_T = np.asarray([0.0, 0.0, 1.0]) + cam_center\n",
    "    cam_pose = _campos2matrix(cam_T, cam_center)\n",
    "    print(cam_pose)\n",
    "\n",
    "\n",
    "    latent = src_latent \n",
    "\n",
    "    out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)        \n",
    "\n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis','%04d.png'%src_idx)\n",
    "#     print('[DONE] save vis to : ', vis_fp)\n",
    "    util.write_img(out_img[...,::-1], vis_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/anpei/liury/data/facial-data/seg_face_real/intrinsics_avg.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_388446/1745618119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m _DEFAULT_CAM_INT = np.load(\n\u001b[0m\u001b[1;32m      2\u001b[0m     '/home/anpei/liury/data/facial-data/seg_face_real/intrinsics_avg.npy')\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_DEFAULT_CAM_INT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_IMG_SIZE\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_IMG_SIZE\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nr_env/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/anpei/liury/data/facial-data/seg_face_real/intrinsics_avg.npy'"
     ]
    }
   ],
   "source": [
    "_DEFAULT_CAM_INT = np.load(\n",
    "    '/home/anpei/liury/data/facial-data/seg_face_real/intrinsics_avg.npy')\n",
    "\n",
    "f = 30\n",
    "_DEFAULT_CAM_INT = np.array([[f,0,_IMG_SIZE//2],[0,f,_IMG_SIZE//2],[0,0,1]])\n",
    "\n",
    "\n",
    "lookat = np.asarray([0, 0, 0])\n",
    "print('*** lookat = ', lookat)\n",
    "\n",
    "cam_center =  np.asarray([0, 0, 2.2])\n",
    "cam_up = np.asarray([0.0, -1.0, 0.0])\n",
    "\n",
    "radii, focus_depth = np.asarray([0.1,0.3,0.2]), 4.5 # z,x,y\n",
    "\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SOF/test', 'spiral_gmm')\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 15\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "random.shuffle(all_instances)\n",
    "\n",
    "# test camera pose\n",
    "z = torch.from_numpy(gmm.sample(1)[0]).float().cuda()\n",
    "out_img, out_seg = render_scene_cam(\n",
    "    model, _campos2matrix(cam_center, lookat, cam_up), z, _DEFAULT_CAM_INT, _OUT_SIZE)\n",
    "\n",
    "\n",
    "plt.imshow(out_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_CAM_INT = '../checkpoints/intrinsics.txt'\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SOF')\n",
    "\n",
    "cam_int = data_util.parse_intrinsics(_DEFAULT_CAM_INT, trgt_sidelength=128)\n",
    "focal = cam_int[0, 0]\n",
    "cam_center = np.array([0,0.11,0.1])\n",
    "\n",
    "\n",
    "for i in range(150):\n",
    "    lat_idx = torch.randint(0, _TOT_NUM_INSTANCES, (2,)).squeeze().cuda()\n",
    "    src_latent = model.get_embedding({'instance_idx': lat_idx[0]}).unsqueeze(0)\n",
    "    trgt_latent = model.get_embedding({'instance_idx': lat_idx[1]}).unsqueeze(0)\n",
    "    \n",
    "    src_idx = lat_idx[0].cpu().numpy()\n",
    "    trgt_idx = lat_idx[1].cpu().numpy()\n",
    "        \n",
    "    output_dir = os.path.join(\n",
    "        _LOG_ROOT, 'interp_latent', 's%04d_t%04d'%(src_idx, trgt_idx))\n",
    "    os.makedirs(os.path.join(output_dir, 'seg'), exist_ok=True)\n",
    "    \n",
    "    print('[%02d] %04d -> %04d: %s'%(i, src_idx, trgt_idx, output_dir))\n",
    "    \n",
    "    R = np.random.rand()*0.5 + 0.7\n",
    "    theta = np.random.rand()*0.4 + (np.pi/2-0.2)\n",
    "    phi = np.random.rand()*1.2 + (np.pi/2-0.6)\n",
    "    \n",
    "    x = R * np.sin(theta) * np.cos(phi)\n",
    "    y = R * np.sin(theta) * np.sin(phi)\n",
    "    z = R * np.cos(theta)\n",
    "    \n",
    "    cam_pose = _campos2matrix(np.array([x, z, y])+cam_center, cam_center)\n",
    "    \n",
    "    steps = 30\n",
    "    \n",
    "    img_outputs = []    \n",
    "    \n",
    "    for idx in range(steps):\n",
    "        _w = float(idx) / (steps - 1)\n",
    "        latent = (1.0-_w)*src_latent + _w*trgt_latent\n",
    "        \n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "        \n",
    "        img_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        \n",
    "    imageio.mimsave(os.path.join(output_dir, 'output.gif'), img_outputs, fps=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent\n",
    "from glob import glob\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "from dataset import data_util\n",
    "from dataset.data_util import load_seg_map\n",
    "\n",
    "_ORI_DATA_ROOT = '/mnt/new_disk2/liury/data/facial-data/CelebAMask-HQ/segmap_20'\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SOF')\n",
    "_LATENT_ROOT = os.path.join(_LOG_ROOT, '060115face_celebA/latent_codes')\n",
    "\n",
    "all_latents = glob(os.path.join(_LATENT_ROOT, '*.npy'))\n",
    "\n",
    "print('tot latents: ', len(all_latents))\n",
    "\n",
    "_DEFAULT_CAM_INT = '../checkpoints/intrinsics.txt'\n",
    "cam_int = data_util.parse_intrinsics(_DEFAULT_CAM_INT, trgt_sidelength=128)\n",
    "focal = cam_int[0, 0]\n",
    "cam_center = np.array([0,0.11,0.0])\n",
    "\n",
    "for i in range(27, len(all_latents)):\n",
    "    out_imgs = []    \n",
    "    \n",
    "    item_id = int(os.path.basename(all_latents[i]).split('.')[0].split('_')[1])\n",
    "    latent = np.load(all_latents[i])\n",
    "    \n",
    "    # load ori\n",
    "    ori_seg = torch.from_numpy(load_seg_map(os.path.join(_ORI_DATA_ROOT, '%d.png'%(item_id)), _OUT_SIZE))\n",
    "    ori_seg_img = util.lin2img(ori_seg.unsqueeze(0), color_map=_CMAP).cpu().numpy()\n",
    "    ori_seg_img = (ori_seg_img.squeeze().transpose(1, 2, 0)) * 255.0\n",
    "    ori_seg_img = ori_seg_img.round().clip(0, 255).astype(np.uint8)\n",
    "    out_imgs.append(torch.from_numpy(ori_seg_img).permute(2, 0, 1).unsqueeze(0))\n",
    "        \n",
    "    # predict\n",
    "    cam_T = latent[:3]\n",
    "    latent = torch.from_numpy(latent[3:]).unsqueeze(0).cuda()\n",
    "    R = np.linalg.norm(cam_T)\n",
    "\n",
    "    cam_pose = _campos2matrix(cam_T+cam_center, cam_center)\n",
    "    out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "    out_imgs.append(torch.from_numpy(out_img).permute(2, 0, 1).unsqueeze(0))\n",
    "    \n",
    "    # calc mIoU\n",
    "    ori_seg = F.one_hot(ori_seg.squeeze().long(), 20)\n",
    "    out_seg = F.one_hot(torch.from_numpy(out_seg).flatten().long(), 20)\n",
    "    \n",
    "    print(ori_seg.shape, out_seg.shape)\n",
    "    break\n",
    "    \n",
    "    mIoU = torch.mean(torch.div(\n",
    "        torch.sum(ori_seg&out_seg, dim=0).float()+1e-8,\n",
    "        torch.sum(ori_seg|out_seg, dim=0).float()+1e-8))\n",
    "                \n",
    "    for step in range(6):\n",
    "    \n",
    "        theta = np.random.rand()*0.4 + (np.pi/2-0.2)\n",
    "        phi = np.random.rand()*1.2 + (np.pi/2-0.6)\n",
    "\n",
    "        x = R * np.sin(theta) * np.cos(phi)\n",
    "        y = R * np.sin(theta) * np.sin(phi)\n",
    "        z = R * np.cos(theta)\n",
    "\n",
    "        cam_pose = _campos2matrix(np.array([x, z, y])+cam_center, cam_center)\n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "                \n",
    "        out_imgs.append(torch.from_numpy(out_img).permute(2, 0, 1).unsqueeze(0))\n",
    "    \n",
    "        \n",
    "    out_imgs = make_grid(torch.cat(out_imgs), nrow=8, padding=1).permute((1, 2, 0)).cpu().numpy()\n",
    "    \n",
    "    ############## vis ##################    \n",
    "    fig = plt.figure(figsize=(45, 5))\n",
    "    plt.imshow(out_imgs)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    ####################################\n",
    "    \n",
    "    output_dir = os.path.join(_LOG_ROOT, 'change_view_celebA', '%06d'%(item_id))\n",
    "    os.makedirs(os.path.join(output_dir, 'seg'), exist_ok=True)    \n",
    "    render_custo_path(0, latent, cam_center=cam_center, output_dir=output_dir)\n",
    "    \n",
    "    print('[%06d] %d : R = %f, mIoU = %f, output_dir = %s'%(i, item_id, R, mIoU, output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    lat_idx = torch.randint(0, _TOT_NUM_INSTANCES, (2,)).squeeze().cuda()\n",
    "    src_latent = model.get_embedding({'instance_idx': lat_idx[0]}).unsqueeze(0)\n",
    "    trgt_latent = model.get_embedding({'instance_idx': lat_idx[1]}).unsqueeze(0)\n",
    "    \n",
    "    src_idx = lat_idx[0].cpu().numpy()\n",
    "    trgt_idx = lat_idx[1].cpu().numpy()\n",
    "    \n",
    "    print('%02d: %04d -> %04d'%(i, src_idx, trgt_idx))\n",
    "    \n",
    "    output_dir = os.path.join(\n",
    "        _LOG_ROOT, 'out_mode_3', 's%04d_t%04d'%(src_idx, trgt_idx))\n",
    "    os.makedirs(os.path.join(output_dir, 'seg'), exist_ok=True)\n",
    "    \n",
    "    print(output_dir)\n",
    "    \n",
    "    render_custo_path(0, src_latent, trgt_latent, output_dir=output_dir)\n",
    "\n",
    "    cam_pose = _campos2matrix(np.array([0., 0., 0.8])+cam_center, cam_center)\n",
    "    out_img, out_seg = render_scene(model, cam_pose, _get_latent(i), focal, _OUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.face_dataset import FaceRandomPoseDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "_OUTPUT_DIR = os.path.join(_LOG_ROOT, 'output_iter_020000')\n",
    "_MODE = 'sphere'\n",
    "_R = 1.5\n",
    "\n",
    "\n",
    "_NUM_INSTANCES=10\n",
    "_NUM_OBSERVATIONS=25\n",
    "\n",
    "output_dir = os.path.join(_OUTPUT_DIR, _MODE+'_128')\n",
    "sample_instances = list(np.random.choice(range(_TOT_NUM_INSTANCES), _NUM_INSTANCES, replace=False))\n",
    "\n",
    "# writer = SummaryWriter(output_dir)\n",
    "\n",
    "dataset = FaceRandomPoseDataset(\n",
    "    intrinsics=_CAM_INT,\n",
    "    cam_center=_CAM_CENTER,\n",
    "    num_instances=sample_instances, \n",
    "    num_observations=_NUM_OBSERVATIONS, \n",
    "    sample_radius=_R, mode=_MODE)\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                     collate_fn=dataset.collate_fn,\n",
    "                     batch_size=1,\n",
    "                     shuffle=False,\n",
    "                     drop_last=False)\n",
    "\n",
    "print('Beginning evaluation...')\n",
    "\n",
    "images = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for idx, model_input in enumerate(dataloader):\n",
    "        model_input, ground_truth = model_input\n",
    "        \n",
    "        pose = model_input['pose']\n",
    "        \n",
    "#         print(pose)\n",
    "        \n",
    "        intrinsics = model_input['intrinsics']\n",
    "        uv = model_input['uv']\n",
    "        z = model.get_embedding(model_input)\n",
    "        \n",
    "        model_outputs = model(pose, z, intrinsics, uv)\n",
    "        predictions, depth_maps = model_outputs\n",
    "                \n",
    "        batch_size, tensor_len, channels = predictions.shape\n",
    "        img_sidelen = np.sqrt(tensor_len).astype(int)\n",
    "                \n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "        output_img = util.lin2img(pred, color_map=).cpu().numpy()\n",
    "        output_pred = pred.view(batch_size, img_sidelen, img_sidelen, 1).cpu().numpy()\n",
    "\n",
    "        for i in range(output_img.shape[0]):\n",
    "            instance_idx = int(model_input['instance_idx'][i].squeeze().detach().cpu().numpy().astype(np.int64))\n",
    "            observation_idx = model_input['observation_idx'][i]\n",
    "            \n",
    "            instance_dir = os.path.join(output_dir, \"%03d\" % instance_idx)\n",
    "            os.makedirs(instance_dir, exist_ok=True)\n",
    "            \n",
    "            img = output_img[i, :, :, :].squeeze().transpose(1, 2, 0)\n",
    "            img *= 255\n",
    "            img = img.round().clip(0, 255).astype(np.uint8)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "            if not instance_idx in images:\n",
    "                images[instance_idx] = [None] * _NUM_OBSERVATIONS\n",
    "\n",
    "            images[instance_idx][observation_idx] = img\n",
    "#             output_fp = os.path.join(instance_dir, '%02d.png'%(observation_idx))\n",
    "#             util.write_img(img, output_fp)\n",
    "            \n",
    "            output_fp = os.path.join(instance_dir, '%02d_seg.png'%(observation_idx))\n",
    "            seg_img = output_pred[i, :, :].squeeze().astype(np.uint8)\n",
    "            \n",
    "            util.write_img(seg_img, output_fp)\n",
    "            \n",
    "#             dpt_img = (output_dpt[i, :, :, :].squeeze() * 255.0).astype(np.uint8)\n",
    "#             dpt_img = cv2.applyColorMap(dpt_img, cv2.COLORMAP_JET)\n",
    "#             output_fp = os.path.join(instance_dir, '%02d_depth.png'%(observation_idx))\n",
    "# #             print('Save output for instance %03d - %02d: %s'%(instance_idx, observation_idx, output_fp))\n",
    "#             util.write_img(dpt_img, output_fp)\n",
    "            print('Save output for instance %04d - %02d: %s'%(instance_idx, observation_idx, output_fp))\n",
    "        \n",
    "            if observation_idx == _NUM_OBSERVATIONS - 1:\n",
    "                imageio.mimsave(os.path.join(instance_dir, 'output.gif'), images[instance_idx], fps=5.0)\n",
    "                print('=== [DONE] saving output.gif.')\n",
    "            \n",
    "#         print('[DONE] Save output for instance %03d.'%(instance_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# move seg_face_8000\n",
    "import os\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "\n",
    "data_root = '/data/anpei/facial-data/seg_face_8000/images'\n",
    "img_fps = glob(os.path.join(data_root, '*_seg_*.png'))\n",
    "img_ids = list(set([os.path.basename(x)[:5] for x in img_fps]))\n",
    "\n",
    "for img_id in img_ids:\n",
    "    output_dir = os.path.join(data_root, img_id)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    copyfile(\n",
    "        os.path.join(data_root, '..', 'cam2world.npy'), \n",
    "        os.path.join(output_dir, 'cam2world.npy'))\n",
    "    \n",
    "    img_fps = glob(os.path.join(data_root, '%s_seg_*.png'%(img_id)))\n",
    "    print(output_dir, len(imgs))\n",
    "    for img_fp in sorted(img_fps):\n",
    "        out_name = os.path.basename(img_fp).split('_')\n",
    "        out_sub_dir = out_name[0]\n",
    "        out_name = '_'.join([out_name[1], '%02d'%(int(out_name[-2])*5+int(out_name[-1][:2]))]) + '.png'\n",
    "        out_name = os.path.join(os.path.dirname(img_fp), out_sub_dir, out_name)\n",
    "#         print('\\t> ', img_fp, out_name)\n",
    "        os.rename(img_fp, out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/data/anpei/facial-data/seg_face_2000/10996'\n",
    "\n",
    "cam_param = np.load(os.path.join(data_dir, 'cameras.npy'), allow_pickle=True).item() \n",
    "print(np.asarray(cam_param['zRange']).shape, np.asarray(cam_param['extrinsics']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as torch_models\n",
    "import torch\n",
    "\n",
    "resnet_model = torch_models.resnet18(pretrained=False)\n",
    "# resnet_model.layer4 = torch.nn.Identity()\n",
    "# resnet_model.fc = torch.nn.Identity()\n",
    "resnet_model.conv1 = torch.nn.Conv2d(19, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "print(resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = resnet_model(torch.Tensor(8, 19, 128, 128))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "aa = torch.randint(0, 19, size=(1, 3, 3))\n",
    "print(aa.shape)\n",
    "\n",
    "bb = F.one_hot(aa)\n",
    "print(bb.shape, bb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.empty(256)\n",
    "print(aa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "instance_dir = '/data/anpei/facial-data/seg_body/94'\n",
    "seg_imgs = img = cv2.imread(os.path.join(instance_dir, 'semantic_mask', 'image.cam05_000094.png'), cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "print(np.unique(seg_imgs))\n",
    "\n",
    "\n",
    "fs = cv2.FileStorage(\"/data/anpei/facial-data/seg_body/Calib_T_samba/cam05_000000/intrinsic.xml\", cv2.FILE_STORAGE_READ)\n",
    "intrinsics = fs.getNode(\"M\").mat()\n",
    "\n",
    "print(intrinsics)\n",
    "\n",
    "H, W = seg_imgs.shape\n",
    "cx, cy = intrinsics[:2, 2]\n",
    "\n",
    "x_coord, y_coord = np.where(seg_imgs != 0)\n",
    "bbox = np.asarray([np.min(x_coord), np.min(y_coord), np.max(x_coord), np.max(y_coord)])\n",
    "print(bbox)\n",
    "\n",
    "sidelength = max(bbox[2] - bbox[0], bbox[3] - bbox[1])\n",
    "center = np.asarray([bbox[2] + bbox[0], bbox[3] + bbox[1]]) / 2.0\n",
    "bbox = np.ceil(np.asarray([\n",
    "    center[0]-sidelength/2.0, \n",
    "    center[1]-sidelength/2.0, \n",
    "    center[0]+sidelength/2.0,\n",
    "    center[1]+sidelength/2.0])).astype(np.int64)\n",
    "\n",
    "cx, cy = intrinsics[:2, 2] - bbox[:2]\n",
    "\n",
    "print(bbox, bbox.dtype)\n",
    "seg_imgs = seg_imgs[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "print(seg_imgs.shape)\n",
    "\n",
    "\n",
    "print(cx, cy, bbox)\n",
    "intrinsics[:2, 2] = [cx, cy]\n",
    "print(intrinsics)\n",
    "\n",
    "\n",
    "print(H, W, cx, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.body_dataset import BodyPartDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dataset = BodyPartDataset(\n",
    "        data_type='seg',\n",
    "        img_sidelength=128, \n",
    "        sample_observations=list(range(5)),\n",
    "        sample_instances=list(range(2)))\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                     collate_fn=dataset.collate_fn,\n",
    "                     batch_size=1,\n",
    "                     shuffle=False,\n",
    "                     drop_last=False)\n",
    "\n",
    "print(len(dataloader))\n",
    "\n",
    "for idx, model_input in enumerate(dataloader):\n",
    "    model_input, ground_truth = model_input\n",
    "        \n",
    "    seg_img = model_input['rgb']\n",
    "    pose = model_input['pose']\n",
    "    intrinsics = model_input['intrinsics']\n",
    "    uv = model_input['uv']    \n",
    "    \n",
    "    print('> ', np.unique(seg_img), seg_img.shape)\n",
    "    print('> pose = ')\n",
    "    print(pose)\n",
    "    print('> cam_int = ')\n",
    "    print(intrinsics)\n",
    "    print(seg_img.shape, np.unique(seg_img))\n",
    "    \n",
    "    output_img = util.lin2img(seg_img, color_map=dataset.color_map).cpu().numpy()\n",
    "    \n",
    "    plt.imshow(output_img.squeeze().transpose(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "H, W = 512, 512\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(   self, \n",
    "                    in_channels, \n",
    "                    out_channels, \n",
    "                    kernel_size=3, \n",
    "                    stride=1, \n",
    "                    num_groups=2):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, stride//2),\n",
    "            nn.GroupNorm(num_groups, out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.conv1_1 = ConvBlock(in_channels, 8, 3, 1)\n",
    "        self.conv1_2 = ConvBlock(8, 16, 3, 2)\n",
    "        self.conv2_1 = ConvBlock(16, 16, 3, 1)\n",
    "        self.conv2_2 = ConvBlock(16, 32, 3, 2)\n",
    "        self.conv3_1 = ConvBlock(32, 32, 3, 1)\n",
    "        self.deconv4_1 = nn.ConvTranspose2d(32, 16, 3, 2)\n",
    "\n",
    "        self.conv4_2 = ConvBlock(32, 16, 3, 1)\n",
    "        self.deconv5_1 = nn.ConvTranspose2d(16, 8, 3, 2)\n",
    "\n",
    "        self.conv5_2 = ConvBlock(16, 8, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv1_1(x)\n",
    "        print(x0.shape)\n",
    "        x = self.conv1_2(x0)\n",
    "        x1 = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        out0 = self.conv3_1(x)\n",
    "        x = self.deconv4_1(out0)\n",
    "\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        out1 = self.conv4_2(x)\n",
    "        x = self.deconv5_1(out1)\n",
    "        x = torch.cat([x0, x], dim=1)\n",
    "\n",
    "        out2 = self.conv5_2(x)\n",
    "        return out0, out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_in = torch.rand(8, 3, H, W)\n",
    "print(fake_in.shape)\n",
    "\n",
    "feat_worker = FeatureExtractor()\n",
    "\n",
    "output = feat_worker(fake_in)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from image_utils import Random_Transforms\n",
    "\n",
    "basedir='/mnt/wmy/sport_1_mask'\n",
    "cam_id=0\n",
    "\n",
    "poses = np.loadtxt(os.path.join(basedir,'CamPose.inf'))\n",
    "Ks = read_intrinsics(os.path.join(basedir,'Intrinsic.inf'))\n",
    "\n",
    "img_fp = os.path.join(basedir, 'img', '%d/img_%04d.jpg' % (_FRAME_ID, cam_id))\n",
    "img = imageio.imread(img_fp)\n",
    "mask_fp = os.path.join(basedir, 'img', 'mask', '%d/img_%04d.jpg' % (_FRAME_ID, cam_id))\n",
    "mask = imageio.imread(mask_fp)\n",
    "\n",
    "img, K, img_mask, ROI = transform(img, Ks[cam_id], mask)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "print(K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
